<!DOCTYPE html>
<html lang="en-us">
  <head><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="x-ua-compatible" content="ie=edge"><title>Personal website and blog</title>
    
<link rel="icon" type="image/png" href="/images/favicon.png" />


<link href="https://fonts.googleapis.com/css?family=Kanit&#43;Thin" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Kanit&#43;Thin" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Kanit" rel="stylesheet"><link rel="stylesheet" type="text/css" href="/css/style.min.85163f2dfde91251e3bbdb646c4ab804cabc44722412284c16998bafdd5d2b65.css" integrity="sha256-hRY/Lf3pElHju9tkbEq4BMq8RHIkEihMFpmLr91dK2U=">
<link rel="stylesheet" type="text/css" href="/css/monokai-sublime.9.15.8.min.min.3a5d282f03108101d715e80fd4c07b55502ec4673fc063f2b6e415d4def5b354.css" integrity="sha256-Ol0oLwMQgQHXFegP1MB7VVAuxGc/wGPytuQV1N71s1Q=">
<link rel="stylesheet" type="text/css" href="/css/icons.min.1ba041601114b03c3f14f4b12f5257970a47aad664eae56a4f14076e23402d9b.css" integrity="sha256-G6BBYBEUsDw/FPSxL1JXlwpHqtZk6uVqTxQHbiNALZs=">
<link rel="stylesheet" type="text/css" href="/css/refresh-css.min.8ea63de88d9bf807125eb95c228dcfb89fad6d262b8618279c84a24b81ac6942.css" integrity="sha256-jqY96I2b&#43;AcSXrlcIo3PuJ&#43;tbSYrhhgnnISiS4GsaUI=">
<link rel="stylesheet" type="text/css" href="/css/devicon.min.min.4647c04682f998d685060bd21f398e955819aaebc0cf9ddfe8dc93e9363d44f0.css" integrity="sha256-RkfARoL5mNaFBgvSHzmOlVgZquvAz53f6NyT6TY9RPA=">
  </head>
  <body>
     

    <div id="preloader">
      <div id="status"></div>
    </div><nav class="navbar is-fresh is-transparent no-shadow" role="navigation" aria-label="main navigation">
  <div class="container">
    <div class="navbar-brand">

      
      
      
      <a class="navbar-item">
        <div class="menu-icon-wrapper left-menu-icon-wrapper" style="visibility: visible;">
          <svg width="1000px" height="1000px">
            <path class="path1" d="M 300 400 L 700 400 C 900 400 900 750 600 850 A 400 400 0 0 1 200 200 L 800 800"></path>
            <path class="path2" d="M 300 500 L 700 500"></path>
            <path class="path3" d="M 700 600 L 300 600 C 100 600 100 200 400 150 A 400 380 0 1 1 200 800 L 800 200"></path>
          </svg>
          <button id="menu-icon-trigger" class="menu-icon-trigger"></button>
        </div>
        <div class="navbar-item left-menu-icon-wrapper">Tags</div>
      </a>

      <div class="navbar-item is-expanded"></div>
      <a class="navbar-item is-hidden-desktop">  
        <div data-target="navbar-menu" class="navbar-item right-menu-icon-wrapper is-hidden-desktop">Menu</div>
        <div data-target="navbar-menu" class="menu-icon-wrapper right-menu-icon-wrapper" style="visibility: visible;">
          <svg width="1000px" height="1000px">
            <path class="path1" d="M 300 400 L 700 400 C 900 400 900 750 600 850 A 400 400 0 0 1 200 200 L 800 800"></path>
            <path class="path2" d="M 300 500 L 700 500"></path>
            <path class="path3" d="M 700 600 L 300 600 C 100 600 100 200 400 150 A 400 380 0 1 1 200 800 L 800 200"></path>
          </svg>
          <button id="menu-icon-trigger" class="menu-icon-trigger"></button>
        </div>
      </a>
    </div>

    

    <div id="navbar-menu" class="navbar-menu is-static">
      
      <div class="navbar-end">
        <a href="/about/" class="navbar-item is-secondary">About Me</a><a href="/credits/" class="navbar-item is-secondary">Credits</a>
          <div class="navbar-item has-dropdown is-hoverable">
            <a href="/topic_2/" class="navbar-link">Title of Topic 2</a>
            <div class="navbar-dropdown">
              <a href="/topic_2/subtopic_3/" class="navbar-item">Title of SubTopic 3</a><a href="/topic_2/subtopic_4/" class="navbar-item">Title of Subtopic 4</a></div>
          </div><a href="/topic_1/" class="navbar-item is-secondary">บทความ</a></div>
    </div>




  </div>
</nav><nav id="navbar-clone" class="navbar is-fresh is-transparent" role="navigation" aria-label="main navigation">
  <div class="container">
      <div class="navbar-brand">
  
        
        
        
        <a class="navbar-item">
          <div class="menu-icon-wrapper left-menu-icon-wrapper" style="visibility: visible;">
            <svg width="1000px" height="1000px">
              <path class="path1" d="M 300 400 L 700 400 C 900 400 900 750 600 850 A 400 400 0 0 1 200 200 L 800 800"></path>
              <path class="path2" d="M 300 500 L 700 500"></path>
              <path class="path3" d="M 700 600 L 300 600 C 100 600 100 200 400 150 A 400 380 0 1 1 200 800 L 800 200"></path>
            </svg>
            <button id="menu-icon-trigger" class="menu-icon-trigger"></button>
          </div>
          <div class="navbar-item left-menu-icon-wrapper">Tags</div>
        </a>
  
        <div class="navbar-item is-expanded"></div>
        <a class="navbar-item is-hidden-desktop">  
          <div data-target="cloned-navbar-menu" class="navbar-item right-menu-icon-wrapper is-hidden-desktop">Menu</div>
          <div data-target="cloned-navbar-menu" class="menu-icon-wrapper right-menu-icon-wrapper" style="visibility: visible;">
            <svg width="1000px" height="1000px">
              <path class="path1" d="M 300 400 L 700 400 C 900 400 900 750 600 850 A 400 400 0 0 1 200 200 L 800 800"></path>
              <path class="path2" d="M 300 500 L 700 500"></path>
              <path class="path3" d="M 700 600 L 300 600 C 100 600 100 200 400 150 A 400 380 0 1 1 200 800 L 800 200"></path>
            </svg>
            <button id="menu-icon-trigger" class="menu-icon-trigger"></button>
          </div>
        </a>
      </div>
  
      
  
      <div id="cloned-navbar-menu" class="navbar-menu is-static">
        <div class="navbar-end">
          <a href="/about/" class="navbar-item is-secondary">About Me</a><a href="/credits/" class="navbar-item is-secondary">Credits</a><div class="navbar-item has-dropdown is-hoverable">
              <a href="/topic_2/" class="navbar-link">Title of Topic 2</a>
              <div class="navbar-dropdown">
                <a href="/topic_2/subtopic_3/" class="navbar-item">Title of SubTopic 3</a><a href="/topic_2/subtopic_4/" class="navbar-item">Title of Subtopic 4</a></div>
            </div><a href="/topic_1/" class="navbar-item is-secondary">บทความ</a></div>
      </div>
  
  
  
  
    </div>
  </nav>
<section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">BeautifulSoup</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/web-scraping-request/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup
      </h3>
      <p class="refresh-summary"> After the 2016 election I became much more interested in media bias and the manipulation of individuals through advertising. This series will be a walkthrough of a web scraping project that monitors political news from both left and right wing media outlets and performs an analysis on the rhetoric being used, the ads being displayed, and the sentiment of certain topics.
The first part of the series will we be getting media bias data and focus on only working locally on your computer, but if you wish to learn how to deploy something like this into production, feel free to leave a comment and let me know.
Limit your impact when scraping Every time you load a web page you&amp;rsquo;re making a request to a server, and when you&amp;rsquo;re just a human with a browser there&amp;rsquo;s not a lot of damage you can do. With a Python script that can execute thousands of requests a second if coded incorrectly, you could end up costing the website owner a lot of money and possibly bring down their site (see Denial-of-service attack (DoS)).
With this in mind, we want to be very careful with how we program scrapers to avoid crashing sites and causing damage. Every time we scrape a website we want to attempt to make only one request per page. We don&amp;rsquo;t want to be making a request every time our parsing or other logic doesn&amp;rsquo;t work out, so we need to parse only after we&amp;rsquo;ve saved the page locally.
If I&amp;rsquo;m just doing some quick tests, I&amp;rsquo;ll usually start out in a Jupyter notebook because you can request a web page in one cell and have that web page available to every cell below it without making a new request. Since this article is available as a Jupyter notebook, you will see how it works if you choose that format.
How to save HTML locally After we make a request and retrieve a web page&amp;rsquo;s content, we can store that content locally with Python&amp;rsquo;s open() function. To do so we need to use the argument wb, which stands for &amp;ldquo;write bytes&amp;rdquo;. This let&amp;rsquo;s us avoid any encoding issues when saving.
Below is a function that wraps the open() function to reduce a lot of repetitive coding later on:
def save_html(html, path): with open(path, &#39;wb&#39;) as f: f.write(html) save_html(r.content, &#39;google_com&#39;)  Assume we have captured the HTML from google.com in html, which you&amp;rsquo;ll see later how to do. After running this function we will now have a file in the same directory as this notebook called google_com that contains the HTML.
How to open/read HTML from a local file To retrieve our saved file we&amp;rsquo;ll make another function to wrap reading the HTML back into html. We need to use rb for &amp;ldquo;read bytes&amp;rdquo; in this case.
def open_html(path): with open(path, &#39;rb&#39;) as f: return f.read() html = open_html(&#39;google_com&#39;)  The open function is doing just the opposite: read the HTML from google_com. If our script fails, notebook closes, computer shutsdown, etc., we no longer need to request google.com again, lessening our impact on their servers. While it doesn&amp;rsquo;t matter much with Google since they have a lot of resources, smaller sites with smaller servers will benefit from this.
I save almost every page and parse later when web scraping as a safety precaution.
Follow the rules for scrapers and bots Each site usually has a robots.txt on the root of their domain. This is where the website owner explicitly states what bots are allowed to do on their site. Simply go to example.com/robots.txt and you should find a text file that looks something like this:
User-agent: * Crawl-delay: 10 Allow: /pages/ Disallow: /scripts/ # more stuff  The User-agent field is the name of the bot and the rules that follow are what the bot should follow. Some robots.txt will have many User-agents with different rules. Common bots are googlebot, bingbot, and applebot, all of which you can probably guess the purpose and origin of.
We don&amp;rsquo;t really need to provide a User-agent when scraping, so User-agent: * is what we would follow. A * means that the following rules apply to all bots (that&amp;rsquo;s us).
The Crawl-delay tells us the number of seconds to wait before requests, so in this example we need to wait 10 seconds before making another request.
Allow gives us specific URLs we&amp;rsquo;re allowed to request with bots, and vice versa for Disallow. In this example we&amp;rsquo;re allowed to request anything in the /pages/ subfolder which means anything that starts with example.com/pages/. On the other hand, we are disallowed from scraping anything from the /scripts/ subfolder.
Many times you&amp;rsquo;ll see a * next to Allow or Disallow which means you are either allowed or not allowed to scrape everything on the site.
Sometimes there will be a disallow all pages followed by allowed pages like this:
Disallow: * Allow: /pages/  This means that you&amp;rsquo;re not allowed to scrape anything except the subfolder /pages/. Essentially, you just want to read the rules in order where the next rule overrides the previous rule.
Scraping Project: Getting Media Bias Data This project will primarily be run through a Jupyter notebook, which is done for teaching purposes and is not the usual way scrapers are programmed. After showing you the pieces, we&amp;rsquo;ll put it all together into a Python script that can be run from command line or your IDE of choice.
Making web requests With Python&amp;rsquo;s requests library we&amp;rsquo;re getting a web page by using get() on the URL. The response r contains many things, but using r.content will give us the HTML. Once we have the HTML we can then parse it for the data we&amp;rsquo;re interested in analyzing.
There&amp;rsquo;s an interesting website called AllSides that has a media bias rating table where users can agree or disagree with the rating.
Since there&amp;rsquo;s nothing in their robots.txt that disallows us from scraping this section of the site, I&amp;rsquo;m assuming it&amp;rsquo;s okay to go ahead and extract this data for our project. Let&amp;rsquo;s request the this first page:
!pip install requests  import requests url = &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39; r = requests.get(url) print(r.content[:100])  Since we essentially have a giant string of HTML, we can print a slice of 100 characters to confirm we have the source of the page. Let&amp;rsquo;s start extracting data.
Parsing HTML with BeautifulSoup What does BeautifulSoup do? We used requests to get the page from the AllSides server, but now we need the BeautifulSoup library to parse HTML and XML. When we pass our HTML to the BeautifulSoup constructor we get an object in return that we can then navigate like the original tree structure of the DOM.
This way we can find elements using names of tags, classes, IDs, and through relationships to other elements, like getting the children and siblings of elements.
Creating a new soup object We create a new BeautifulSoup object by passing the constructor our newly acquired HTML content and the type of parser we want to use:
!pip install beautifulsoup4  from bs4 import BeautifulSoup soup = BeautifulSoup(r.content, &#39;html.parser&#39;)  This soup object defines a bunch of methods — many of which can achieve the same result — that we can use to extract data from the HTML. Let&amp;rsquo;s start with finding elements.
Finding elements and data To find elements and data inside our HTML we&amp;rsquo;ll be using select_one, which returns a single element, and select, which returns a list of elements (even if only one item exists). Both of these methods use CSS selectors to find elements, so if you&amp;rsquo;re rusty on how CSS selectors work here&amp;rsquo;s a quick refresher:
A CSS selector refresher 1. To get a tag, such as &amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;, &amp;lt;body&amp;gt;&amp;lt;/body&amp;gt;, use the naked name for the tag. E.g. select_one(&#39;a&#39;) gets an anchor/link element, select_one(&#39;body&#39;) gets the body element 2. .temp gets an element with a class of temp, E.g. to get &amp;lt;a class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp&#39;) 3. #temp gets an element with an id of temp, E.g. to get &amp;lt;a id=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;#temp&#39;) 4. .temp.example gets an element with both classes temp and example, E.g. to get &amp;lt;a class=&amp;quot;temp example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp.example&#39;) 5. .temp a gets an anchor element nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp a&#39;). Note the space between .temp and a. 6. .temp .example gets an element with class example nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a class=&amp;quot;example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp .example&#39;). Again, note the space between .temp and .example. The space tells the selector that the class after the space is a child of the class before the space. 7. ids, such as &amp;lt;a id=one&amp;gt;&amp;lt;/a&amp;gt;, are unique so you can usually use the id selector by itself to get the right element. No need to do nested selectors when using ids.
There&amp;rsquo;s many more selectors for for doing various tasks, like selecting certain child elements, specific links, etc., that you can look up when needed. The selectors above get us pretty close to everything we would need for now.
Tips on figuring out how to select certain elements
Most browsers have a quick way of finding the selector for an element using their developer tools. In Chrome, we can quickly find selectors for elements by 1. Right-click on the the element then select &amp;ldquo;Inspect&amp;rdquo; in the menu. Developer tools opens and and highlights the element we right-clicked 2. Right-click the code element in developer tools, hover over &amp;ldquo;Copy&amp;rdquo; in the menu, then click &amp;ldquo;Copy selector&amp;rdquo;
Sometimes it&amp;rsquo;ll be a little off and we need to scan up a few elements to find the right one. Here&amp;rsquo;s what it looks like to find the selector and Xpath, another type of selector, in Chrome:


Let&amp;rsquo;s start! Getting data out of a table Our data is housed in a table on AllSides, and by inspecting the header element we can find the code that renders the table and rows. What we need to do is select all the rows from the table and then parse out the information from each row.


Simplifying the table&amp;rsquo;s HTML, the structure looks like this (comments &amp;lt;!-- --&amp;gt; added by me):
&amp;lt;table&amp;gt; &amp;lt;thead&amp;gt; &amp;lt;!-- header information --&amp;gt; &amp;lt;/thead&amp;gt; &amp;lt;tbody&amp;gt; &amp;lt;tr class=&amp;quot;odd views-row-first&amp;quot;&amp;gt; &amp;lt;!-- begin table row --&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- outlet name --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- bias data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing-1 what-do-you-think&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree buttons --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;!-- end table row --&amp;gt; &amp;lt;!-- more rows --&amp;gt; &amp;lt;/tbody&amp;gt; &amp;lt;/table&amp;gt;  So to get each row, we just select all &amp;lt;tr&amp;gt; inside &amp;lt;tbody&amp;gt;:
rows = soup.select(&#39;tbody tr&#39;)  tbody tr tells the selector to extract all &amp;lt;tr&amp;gt; (table row) tags that are children of the &amp;lt;tbody&amp;gt; body tag. If there were more than one table on this page we would have to make a more specific selector, but since this is the only table, we&amp;rsquo;re good to go.
Now we have a list of HTML table rows that each contain four cells: - News source name and link - Bias data - Agreement buttons - Community feedback data
Below is a breakdown of how to extract each one.
News source name 

Let&amp;rsquo;s look at the first cell:
&amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/news-source/abc-news-media-bias&amp;quot;&amp;gt;ABC News&amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  The outlet name (ABC News) is the text of an anchor tag that&amp;rsquo;s nested inside a &amp;lt;td&amp;gt; tag, which is a cell — or table data tag.
Getting the outlet name is pretty easy: just get the first row in rows and run a select_one off that object:
row = rows[0] name = row.select_one(&#39;.source-title&#39;).text.strip() print(name)  The only class we needed to use in this case was .source-title since .views-field looks to be just a class each row is given for styling and doesn&amp;rsquo;t provide any uniqueness.
Notice that we didn&amp;rsquo;t need to worry about selecting the anchor tag a that contains the text. When we use .text is gets all text in that element, and since &amp;ldquo;ABC News&amp;rdquo; is the only text, that&amp;rsquo;s all we need to do. Bear in mind that using select or select_one will give you the whole element with the tags included, so we need .text to give us the text between the tags.
.strip() ensures all the whitespace surrounding the name is removed. This is a good thing to always do since many websites use whitespace as a way to visually pad the text inside elements.
You&amp;rsquo;ll notice that we can run BeautifulSoup methods right off one of the rows. That&amp;rsquo;s because the rows become their own BeautifulSoup objects when we make a select from another BeautifulSoup object. On the other hand, our name variable is no longer a BeautifulSoup object because we called .text.
News source page link We also need the link to this news source&amp;rsquo;s page on AllSides. If we look back at the HTML we&amp;rsquo;ll see that in this case we do want to select the anchor in order to get the href that contains the link, so let&amp;rsquo;s do that:
allsides_page = row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] allsides_page = &#39;https://www.allsides.com&#39; &#43; allsides_page print(allsides_page)  It is a relative path in the HTML, so we prepend the site&amp;rsquo;s URL to make it a link we can request later.
Getting the link was a bit different than just selecting an element. We had to access an attribute (href) of the element, which is done using brackets, like how we would access a Python dictionary. This will be the same for other attributes of elements, like src in images and videos.
Bias rating 

We can see that the rating is displayed as an image so how can we get the rating in words? Looking at the HTML notice the link that surrounds the image has the text we need:
&amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/media-bias/left-center&amp;quot;&amp;gt; &amp;lt;img src=&amp;quot;...&amp;quot; width=&amp;quot;144&amp;quot; height=&amp;quot;24&amp;quot; alt=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot; title=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot;&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  We could also pull the alt attribute, but the link looks easier. Let&amp;rsquo;s grab it:
bias = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;] bias = bias.split(&#39;/&#39;)[-1] print(bias)  Here we selected the anchor tag by using the class name and tag together: .views-field-field-bias-image is the class of the &amp;lt;td&amp;gt; and &amp;lt;a&amp;gt; is for the anchor nested inside.
After that we extract the href just like before, but now we only want the last part of the URL for the name of the bias so we split on slashes and get the last element of that split (left-center).
Community feedback data 

The last thing to scrape is the agree/disagree ratio from the community feedback area. The HTML of this cell is pretty convoluted due to the styling, but here&amp;rsquo;s the basic structure:
&amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;getratingval&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;rate-widget-4 rate-widget clear-block rate-average rate-widget-yesno&amp;quot; id=&amp;quot;rate-node-76-4-1&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;item-list&amp;quot;&amp;gt; &amp;lt;ul&amp;gt; &amp;lt;li class=&amp;quot;first&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-3&amp;quot;&amp;gt;agree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;li class=&amp;quot;last&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-4&amp;quot;&amp;gt;disagree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;rate-details&amp;quot;&amp;gt; &amp;lt;span class=&amp;quot;agree&amp;quot;&amp;gt;8241&amp;lt;/span&amp;gt;/&amp;lt;span class=&amp;quot;disagree&amp;quot;&amp;gt;6568&amp;lt;/span&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/td&amp;gt;  The numbers we want are located in two span elements in the last div. Both span elements have classes that are unique in this cell so we can use them to make the selection:
agree = row.select_one(&#39;.agree&#39;).text agree = int(agree) disagree = row.select_one(&#39;.disagree&#39;).text disagree = int(disagree) agree_ratio = agree / disagree print(f&amp;quot;Agree: {agree}, Disagree: {disagree}, Ratio {agree_ratio:.2f}&amp;quot;)  Using .text will return a string, so we need to convert them to integers in order to calculate the ratio.
Side note: If you&amp;rsquo;ve never seen this way of formatting print statements in Python, the f at the front allows us to insert variables right into the string using curly braces. The :.2f is a way to format floats to only show two decimals places.
If you look at the page in your browser you&amp;rsquo;ll notice that they say how much the community is in agreement by using &amp;ldquo;somewhat agree&amp;rdquo;, &amp;ldquo;strongly agree&amp;rdquo;, etc. so how do we get that? If we try to select it:
print(row.select_one(&#39;.community-feedback-rating-page&#39;))  It shows up as None because this element is rendered with Javascript and requests can&amp;rsquo;t pull HTML rendered with Javascript. We&amp;rsquo;ll be looking at how to get data rendered with JS in a later article, but since this is the only piece of information that&amp;rsquo;s rendered this way we can manually recreate the text.
To find the JS files they&amp;rsquo;re using, just CTRL&#43;F for &amp;ldquo;.js&amp;rdquo; in the page source and open the files in a new tab to look for that logic.
It turned out the logic was located in the eleventh JS file and they have a function that calculates the text and color with these parameters:
 Range Agreeance   $ratio  3$ absolutely agrees   $2 strongly agrees   $1.5 agrees   $1 somewhat agrees   $ratio = 1$ neutral   $0.67 somewhat disgrees   $0.5 disgrees   $0.33 strongly disagrees   $ratio \leq 0.33$ absolutely disagrees   Let&amp;rsquo;s make a function that replicates this logic:
def get_agreeance_text(ratio): if ratio &amp;gt; 3: return &amp;quot;absolutely agrees&amp;quot; elif 2 &amp;lt; ratio &amp;lt;= 3: return &amp;quot;strongly agrees&amp;quot; elif 1.5 &amp;lt; ratio &amp;lt;= 2: return &amp;quot;agrees&amp;quot; elif 1 &amp;lt; ratio &amp;lt;= 1.5: return &amp;quot;somewhat agrees&amp;quot; elif ratio == 1: return &amp;quot;neutral&amp;quot; elif 0.67 &amp;lt; ratio &amp;lt; 1: return &amp;quot;somewhat disagrees&amp;quot; elif 0.5 &amp;lt; ratio &amp;lt;= 0.67: return &amp;quot;disagrees&amp;quot; elif 0.33 &amp;lt; ratio &amp;lt;= 0.5: return &amp;quot;strongly disagrees&amp;quot; elif ratio &amp;lt;= 0.33: return &amp;quot;absolutely disagrees&amp;quot; else: return None print(get_agreeance_text(2.5))  Now that we have the general logic for a single row and we can generate the agreeance text, let&amp;rsquo;s create a loop that gets data from every row on the first page:
data= [] for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d)  In the loop we can combine any multi-step extractions into one to create the values in the least number of steps.
Our data list now contains a dictionary containing key information for every row.
print(data[0])  Keep in mind that this is still only the first page. The list on AllSides is three pages long as of this writing, so we need to modify this loop to get the other pages.
Requesting and parsing multiple pages Notice that the URLs for each page follow a pattern. The first page has no parameters on the URL, but the next pages do; specifically they attach a ?page=# to the URL where &amp;lsquo;#&amp;rsquo; is the page number.
Right now, the easiest way to get all pages is just to manually make a list of these three pages and loop over them. If we were working on a project with thousands of pages we might build a more automated way of constructing/finding the next URLs, but for now this works.
pages = [ &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=1&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=2&#39; ]  According to AllSides&amp;rsquo; robots.txt we need to make sure we wait ten seconds before each request.
Our loop will: - request a page - parse the page - wait ten seconds - repeat for next page.
Remember, we&amp;rsquo;ve already tested our parsing above on a page that was cached locally so we know it works. You&amp;rsquo;ll want to make sure to do this before making a loop that performs requests to prevent having to reloop if you forgot to parse something.
By combining all the steps we&amp;rsquo;ve done up to this point and adding a loop over pages, here&amp;rsquo;s how it looks:
from time import sleep data= [] for page in pages: r = requests.get(page) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) rows = soup.select(&#39;tbody tr&#39;) for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d) sleep(10)  Now we have a list of dictionaries for each row on all three pages.
To cap it off, we want to get the real URL to the news source, not just the link to their presence on AllSides. To do this, we will need to get the AllSides page and look for the link.
If we go to ABC News&amp;rsquo; page there&amp;rsquo;s a row of external links to Facebook, Twitter, Wikipedia, and the ABC News website. The HTML for that sections looks like this:
&amp;lt;div class=&amp;quot;row-fluid source-links gray-bg-box&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;container&amp;quot;&amp;gt;	&amp;lt;a href=&amp;quot;https://www.facebook.com/ABCNews/&amp;quot; class=&amp;quot;facebook&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-facebook&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;Facebook&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://twitter.com/ABC&amp;quot; class=&amp;quot;twitter&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-twitter&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Twitter&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/ABC_News&amp;quot; class=&amp;quot;wikipedia&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-wikipedia-w&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Wikipedia&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;http://abcnews.go.com/&amp;quot; class=&amp;quot;www&amp;quot;&amp;gt;&amp;lt;i class=&amp;quot;fa fa-globe&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt; &amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;ABC News&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;/contact&amp;quot; class=&amp;quot;improve-this-page&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-line-chart&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Improve this page&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;  Notice the anchor tag (&amp;lt;a&amp;gt;) that contains the link to ABC News has a class of &amp;ldquo;www&amp;rdquo;. Pretty easy to get with what we&amp;rsquo;ve already learned:
website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;]  So let&amp;rsquo;s make another loop to request the AllSides page and get links for each news source. Unfortunately, some pages don&amp;rsquo;t have a link in this grey bar to the news source, which brings up a good point: always account for elements to randomly not exist.
Up until now we&amp;rsquo;ve assumed elements exist in the tables we scraped, but it&amp;rsquo;s always a good idea to program scrapers in way so they don&amp;rsquo;t break when an element goes missing.
Using select_one or select will always return None or an empty list if nothing is found, so in this loop we&amp;rsquo;ll check if we found the website element or not so it doesn&amp;rsquo;t throw an Exception when trying to access the href attribute.
Finally, since there&amp;rsquo;s 265 news source pages and the wait time between pages is 10 seconds, it&amp;rsquo;s going to take ~44 minutes to do this. Instead of blindly not knowing our progress, let&amp;rsquo;s use the tqdm library to give us a nice progress bar:
!pip install tqdm  from tqdm import tqdm_notebook for d in tqdm_notebook(data): r = requests.get(d[&#39;allsides_page&#39;]) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) try: website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;] d[&#39;website&#39;] = website except TypeError: pass sleep(10)  tqdm is a little weird at first, but essentially tqdm_notebook is just wrapping around our data list to produce a progress bar. We are still able to access each dictionary, d, just as we would normally. Note that tqdm_notebook is only for Jupyter notebooks. In regular editors you&amp;rsquo;ll just import tqdm from tqdm and use tqdm instead.
Saving our data So what do we have now? At this moment, data is a list of dictionaries, each of which contains all the data from the tables as well as the websites from each individual news source&amp;rsquo;s page on AllSides.
The first thing we&amp;rsquo;ll want to do now is save that data to a file so we don&amp;rsquo;t have to make those requests again. We&amp;rsquo;ll be storing the data as JSON since it&amp;rsquo;s already in that form anyway:
import json with open(&#39;allsides.json&#39;, &#39;w&#39;) as f: json.dump(data, f)  To load it back in when you need it:
with open(&#39;allsides.json&#39;, &#39;r&#39;) as f: data = json.load(f)  If you&amp;rsquo;re not familiar with JSON, just quickly open allsides.json in an editor and see what it looks like. It should look almost exactly like what data looks like if we print it in Python: a list of dictionaries.
Brief Data Analysis Before ending this article I think it would be worthwhile to actually see what&amp;rsquo;s interesting about this data we just retrieved. So, let&amp;rsquo;s answer a couple of questions.
Which ratings for outlets does the community absolutely agree on?
To find where the community absolutely agrees we can do a simple list comprehension that checks each dict for the agreeance text we want:
abs_agree = [d for d in data if d[&#39;agreeance_text&#39;] == &#39;absolutely agrees&#39;] print(f&amp;quot;{&#39;Outlet&#39;:&amp;lt;20} {&#39;Bias&#39;:&amp;lt;20}&amp;quot;) print(&amp;quot;-&amp;quot; * 30) for d in abs_agree: print(f&amp;quot;{d[&#39;name&#39;]:&amp;lt;20} {d[&#39;bias&#39;]:&amp;lt;20}&amp;quot;)  Using some string formatting we can make it look somewhat tabular. Interestingly, C-SPAN is the only center bias that the community absolutely agrees on. The others for left and right aren&amp;rsquo;t that surprising.
Making analysis easier with Pandas Which ratings for outlets does the community absolutely disagree on?
To make analysis a little easier, we can also load our JSON data into a Pandas DataFrame as well. This is easy with Pandas since they have a simple function for reading JSON into a DataFrame.
As an aside, if you&amp;rsquo;ve never used Pandas, Matplotlib, or any of the other data science libraries, I would definitely recommend checking out [Jose Portilla&amp;rsquo;s data science course]() for a great intro to these tools and many machine learning concepts.
Now to the DataFrame:
import pandas as pd df = pd.read_json(open(&#39;allsides.json&#39;, &#39;r&#39;)) df.set_index(&#39;name&#39;, inplace=True) df.head()  Now filter the DataFrame by &amp;ldquo;agreeance_text&amp;rdquo;:
df[df[&#39;agreeance_text&#39;] == &#39;strongly disagrees&#39;]  It looks like much of the community disagrees strongly with certain outlets being rated with a &amp;ldquo;center&amp;rdquo; bias.
Let&amp;rsquo;s make a quick visualization of agreeance. Since there&amp;rsquo;s too many news sources to plot so let&amp;rsquo;s pull only those with the most votes. To do that, we can make a new column that counts the total votes and then sort by that value:
df[&#39;total_votes&#39;] = df[&#39;agree&#39;] &#43; df[&#39;disagree&#39;] df.sort_values(&#39;total_votes&#39;, ascending=False, inplace=True) df.head(10)  Visualizing the data To make a bar plot we&amp;rsquo;ll use Matplotlib with Seaborn&amp;rsquo;s dark grid style:
import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-darkgrid&#39;)  As mentioned above, we have too many news outlets to plot comfortably, so just make a copy of the top 25 and place it in a new df2 variable:
df2 = df.head(25).copy() df2.head()  With the top 25 news sources by amount of feedback, let&amp;rsquo;s create a stacked bar chart where the number of agrees are stacked on top of the number of disagrees. This makes the total height of the bar the total amount of feedback.
Below, we first create a figure and axes, plot the agree bars, plot the disagree bars on top of the agrees using bottom, then set various text features:
fig, ax = plt.subplots(figsize=(20, 10)) ax.bar(df2.index, df2[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(df2.index, df2[&#39;disagree&#39;], bottom=df2[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) ax.set_ylabel = &#39;Total feedback&#39; plt.yticks(fontsize=&#39;x-large&#39;) plt.xticks(rotation=60, ha=&#39;right&#39;, fontsize=&#39;x-large&#39;, rotation_mode=&#39;anchor&#39;) plt.legend([&#39;Agree&#39;, &#39;Disagree&#39;], fontsize=&#39;xx-large&#39;) plt.title(&#39;AllSides Bias Rating vs. Community Feedback&#39;, fontsize=&#39;xx-large&#39;) plt.show()  For a slightly more complex version, let&amp;rsquo;s make a subplot for each bias and plot the respective news sources.
This time we&amp;rsquo;ll make a new copy of the original DataFrame beforehand since we can plot more news outlets now.
Instead of making one axes, we&amp;rsquo;ll create a new one for each bias to make six total subplots:
df3 = df.copy() fig = plt.figure(figsize=(15,15)) biases = df3[&#39;bias&#39;].unique() for i, bias in enumerate(biases): # Get top 10 news sources for this bias and sort index alphabetically temp_df = df3[df3[&#39;bias&#39;] == bias].iloc[:10] temp_df.sort_index(inplace=True) # Get max votes, i.e. the y value for tallest bar in this temp dataframe max_votes = temp_df[&#39;total_votes&#39;].max() # Add a new subplot in the correct grid position ax = fig.add_subplot(len(biases) / 2, 2, i &#43; 1) # Create the stacked bars ax.bar(temp_df.index, temp_df[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(temp_df.index, temp_df[&#39;disagree&#39;], bottom=temp_df[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) # Place text for the ratio on top of each bar for x, y, ratio in zip(ax.get_xticks(), temp_df[&#39;total_votes&#39;], temp_df[&#39;agree_ratio&#39;]): ax.text(x, y &#43; (0.02 * max_votes), f&amp;quot;{ratio:.2f}&amp;quot;, ha=&#39;center&#39;) ax.set_ylabel(&#39;Total feedback&#39;) ax.set_title(bias.title()) # Make y limit larger to compensate for text on bars ax.set_ylim(0, max_votes &#43; (0.12 * max_votes)) # Rotate tick labels so they don&#39;t overlap plt.setp(ax.get_xticklabels(), rotation=30, ha=&#39;right&#39;) plt.tight_layout(w_pad=3.0, h_pad=1.0) plt.show()  Hopefully the comments help with how these plots were created. We&amp;rsquo;re just looping through each unique bias and adding a subplot to the figure.
When interpreting these plots keep in mind that the y-axis has different scales for each subplot. Overall it&amp;rsquo;s a nice way to see which outlets have a lot of votes and where the most disagreement is. This is what makes scraping so much fun!
Final words We have the tools to make some fairly complex web scrapers now, but there&amp;rsquo;s still the issue with Javascript rendering. This is something that deserves its own article, but for now we can do quite a lot.
There&amp;rsquo;s also some project organization that needs to occur when making this into a more easily runnable program. We need to pull it out of this notebook and code in command-line arguments if we plan to run it often for updates.
These sorts of things will be addressed later when we build more complex scrapers, but feel free to let me know in the comments of anything in particular you&amp;rsquo;re interested in learning about.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_1/web-scraping-request/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_4/web-scraping-request/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup
      </h3>
      <p class="refresh-summary"> After the 2016 election I became much more interested in media bias and the manipulation of individuals through advertising. This series will be a walkthrough of a web scraping project that monitors political news from both left and right wing media outlets and performs an analysis on the rhetoric being used, the ads being displayed, and the sentiment of certain topics.
The first part of the series will we be getting media bias data and focus on only working locally on your computer, but if you wish to learn how to deploy something like this into production, feel free to leave a comment and let me know.
Limit your impact when scraping Every time you load a web page you&amp;rsquo;re making a request to a server, and when you&amp;rsquo;re just a human with a browser there&amp;rsquo;s not a lot of damage you can do. With a Python script that can execute thousands of requests a second if coded incorrectly, you could end up costing the website owner a lot of money and possibly bring down their site (see Denial-of-service attack (DoS)).
With this in mind, we want to be very careful with how we program scrapers to avoid crashing sites and causing damage. Every time we scrape a website we want to attempt to make only one request per page. We don&amp;rsquo;t want to be making a request every time our parsing or other logic doesn&amp;rsquo;t work out, so we need to parse only after we&amp;rsquo;ve saved the page locally.
If I&amp;rsquo;m just doing some quick tests, I&amp;rsquo;ll usually start out in a Jupyter notebook because you can request a web page in one cell and have that web page available to every cell below it without making a new request. Since this article is available as a Jupyter notebook, you will see how it works if you choose that format.
How to save HTML locally After we make a request and retrieve a web page&amp;rsquo;s content, we can store that content locally with Python&amp;rsquo;s open() function. To do so we need to use the argument wb, which stands for &amp;ldquo;write bytes&amp;rdquo;. This let&amp;rsquo;s us avoid any encoding issues when saving.
Below is a function that wraps the open() function to reduce a lot of repetitive coding later on:
def save_html(html, path): with open(path, &#39;wb&#39;) as f: f.write(html) save_html(r.content, &#39;google_com&#39;)  Assume we have captured the HTML from google.com in html, which you&amp;rsquo;ll see later how to do. After running this function we will now have a file in the same directory as this notebook called google_com that contains the HTML.
How to open/read HTML from a local file To retrieve our saved file we&amp;rsquo;ll make another function to wrap reading the HTML back into html. We need to use rb for &amp;ldquo;read bytes&amp;rdquo; in this case.
def open_html(path): with open(path, &#39;rb&#39;) as f: return f.read() html = open_html(&#39;google_com&#39;)  The open function is doing just the opposite: read the HTML from google_com. If our script fails, notebook closes, computer shutsdown, etc., we no longer need to request google.com again, lessening our impact on their servers. While it doesn&amp;rsquo;t matter much with Google since they have a lot of resources, smaller sites with smaller servers will benefit from this.
I save almost every page and parse later when web scraping as a safety precaution.
Follow the rules for scrapers and bots Each site usually has a robots.txt on the root of their domain. This is where the website owner explicitly states what bots are allowed to do on their site. Simply go to example.com/robots.txt and you should find a text file that looks something like this:
User-agent: * Crawl-delay: 10 Allow: /pages/ Disallow: /scripts/ # more stuff  The User-agent field is the name of the bot and the rules that follow are what the bot should follow. Some robots.txt will have many User-agents with different rules. Common bots are googlebot, bingbot, and applebot, all of which you can probably guess the purpose and origin of.
We don&amp;rsquo;t really need to provide a User-agent when scraping, so User-agent: * is what we would follow. A * means that the following rules apply to all bots (that&amp;rsquo;s us).
The Crawl-delay tells us the number of seconds to wait before requests, so in this example we need to wait 10 seconds before making another request.
Allow gives us specific URLs we&amp;rsquo;re allowed to request with bots, and vice versa for Disallow. In this example we&amp;rsquo;re allowed to request anything in the /pages/ subfolder which means anything that starts with example.com/pages/. On the other hand, we are disallowed from scraping anything from the /scripts/ subfolder.
Many times you&amp;rsquo;ll see a * next to Allow or Disallow which means you are either allowed or not allowed to scrape everything on the site.
Sometimes there will be a disallow all pages followed by allowed pages like this:
Disallow: * Allow: /pages/  This means that you&amp;rsquo;re not allowed to scrape anything except the subfolder /pages/. Essentially, you just want to read the rules in order where the next rule overrides the previous rule.
Scraping Project: Getting Media Bias Data This project will primarily be run through a Jupyter notebook, which is done for teaching purposes and is not the usual way scrapers are programmed. After showing you the pieces, we&amp;rsquo;ll put it all together into a Python script that can be run from command line or your IDE of choice.
Making web requests With Python&amp;rsquo;s requests library we&amp;rsquo;re getting a web page by using get() on the URL. The response r contains many things, but using r.content will give us the HTML. Once we have the HTML we can then parse it for the data we&amp;rsquo;re interested in analyzing.
There&amp;rsquo;s an interesting website called AllSides that has a media bias rating table where users can agree or disagree with the rating.
Since there&amp;rsquo;s nothing in their robots.txt that disallows us from scraping this section of the site, I&amp;rsquo;m assuming it&amp;rsquo;s okay to go ahead and extract this data for our project. Let&amp;rsquo;s request the this first page:
!pip install requests  import requests url = &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39; r = requests.get(url) print(r.content[:100])  Since we essentially have a giant string of HTML, we can print a slice of 100 characters to confirm we have the source of the page. Let&amp;rsquo;s start extracting data.
Parsing HTML with BeautifulSoup What does BeautifulSoup do? We used requests to get the page from the AllSides server, but now we need the BeautifulSoup library to parse HTML and XML. When we pass our HTML to the BeautifulSoup constructor we get an object in return that we can then navigate like the original tree structure of the DOM.
This way we can find elements using names of tags, classes, IDs, and through relationships to other elements, like getting the children and siblings of elements.
Creating a new soup object We create a new BeautifulSoup object by passing the constructor our newly acquired HTML content and the type of parser we want to use:
!pip install beautifulsoup4  from bs4 import BeautifulSoup soup = BeautifulSoup(r.content, &#39;html.parser&#39;)  This soup object defines a bunch of methods — many of which can achieve the same result — that we can use to extract data from the HTML. Let&amp;rsquo;s start with finding elements.
Finding elements and data To find elements and data inside our HTML we&amp;rsquo;ll be using select_one, which returns a single element, and select, which returns a list of elements (even if only one item exists). Both of these methods use CSS selectors to find elements, so if you&amp;rsquo;re rusty on how CSS selectors work here&amp;rsquo;s a quick refresher:
A CSS selector refresher 1. To get a tag, such as &amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;, &amp;lt;body&amp;gt;&amp;lt;/body&amp;gt;, use the naked name for the tag. E.g. select_one(&#39;a&#39;) gets an anchor/link element, select_one(&#39;body&#39;) gets the body element 2. .temp gets an element with a class of temp, E.g. to get &amp;lt;a class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp&#39;) 3. #temp gets an element with an id of temp, E.g. to get &amp;lt;a id=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;#temp&#39;) 4. .temp.example gets an element with both classes temp and example, E.g. to get &amp;lt;a class=&amp;quot;temp example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp.example&#39;) 5. .temp a gets an anchor element nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp a&#39;). Note the space between .temp and a. 6. .temp .example gets an element with class example nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a class=&amp;quot;example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp .example&#39;). Again, note the space between .temp and .example. The space tells the selector that the class after the space is a child of the class before the space. 7. ids, such as &amp;lt;a id=one&amp;gt;&amp;lt;/a&amp;gt;, are unique so you can usually use the id selector by itself to get the right element. No need to do nested selectors when using ids.
There&amp;rsquo;s many more selectors for for doing various tasks, like selecting certain child elements, specific links, etc., that you can look up when needed. The selectors above get us pretty close to everything we would need for now.
Tips on figuring out how to select certain elements
Most browsers have a quick way of finding the selector for an element using their developer tools. In Chrome, we can quickly find selectors for elements by 1. Right-click on the the element then select &amp;ldquo;Inspect&amp;rdquo; in the menu. Developer tools opens and and highlights the element we right-clicked 2. Right-click the code element in developer tools, hover over &amp;ldquo;Copy&amp;rdquo; in the menu, then click &amp;ldquo;Copy selector&amp;rdquo;
Sometimes it&amp;rsquo;ll be a little off and we need to scan up a few elements to find the right one. Here&amp;rsquo;s what it looks like to find the selector and Xpath, another type of selector, in Chrome:


Let&amp;rsquo;s start! Getting data out of a table Our data is housed in a table on AllSides, and by inspecting the header element we can find the code that renders the table and rows. What we need to do is select all the rows from the table and then parse out the information from each row.


Simplifying the table&amp;rsquo;s HTML, the structure looks like this (comments &amp;lt;!-- --&amp;gt; added by me):
&amp;lt;table&amp;gt; &amp;lt;thead&amp;gt; &amp;lt;!-- header information --&amp;gt; &amp;lt;/thead&amp;gt; &amp;lt;tbody&amp;gt; &amp;lt;tr class=&amp;quot;odd views-row-first&amp;quot;&amp;gt; &amp;lt;!-- begin table row --&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- outlet name --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- bias data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing-1 what-do-you-think&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree buttons --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;!-- end table row --&amp;gt; &amp;lt;!-- more rows --&amp;gt; &amp;lt;/tbody&amp;gt; &amp;lt;/table&amp;gt;  So to get each row, we just select all &amp;lt;tr&amp;gt; inside &amp;lt;tbody&amp;gt;:
rows = soup.select(&#39;tbody tr&#39;)  tbody tr tells the selector to extract all &amp;lt;tr&amp;gt; (table row) tags that are children of the &amp;lt;tbody&amp;gt; body tag. If there were more than one table on this page we would have to make a more specific selector, but since this is the only table, we&amp;rsquo;re good to go.
Now we have a list of HTML table rows that each contain four cells: - News source name and link - Bias data - Agreement buttons - Community feedback data
Below is a breakdown of how to extract each one.
News source name 

Let&amp;rsquo;s look at the first cell:
&amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/news-source/abc-news-media-bias&amp;quot;&amp;gt;ABC News&amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  The outlet name (ABC News) is the text of an anchor tag that&amp;rsquo;s nested inside a &amp;lt;td&amp;gt; tag, which is a cell — or table data tag.
Getting the outlet name is pretty easy: just get the first row in rows and run a select_one off that object:
row = rows[0] name = row.select_one(&#39;.source-title&#39;).text.strip() print(name)  The only class we needed to use in this case was .source-title since .views-field looks to be just a class each row is given for styling and doesn&amp;rsquo;t provide any uniqueness.
Notice that we didn&amp;rsquo;t need to worry about selecting the anchor tag a that contains the text. When we use .text is gets all text in that element, and since &amp;ldquo;ABC News&amp;rdquo; is the only text, that&amp;rsquo;s all we need to do. Bear in mind that using select or select_one will give you the whole element with the tags included, so we need .text to give us the text between the tags.
.strip() ensures all the whitespace surrounding the name is removed. This is a good thing to always do since many websites use whitespace as a way to visually pad the text inside elements.
You&amp;rsquo;ll notice that we can run BeautifulSoup methods right off one of the rows. That&amp;rsquo;s because the rows become their own BeautifulSoup objects when we make a select from another BeautifulSoup object. On the other hand, our name variable is no longer a BeautifulSoup object because we called .text.
News source page link We also need the link to this news source&amp;rsquo;s page on AllSides. If we look back at the HTML we&amp;rsquo;ll see that in this case we do want to select the anchor in order to get the href that contains the link, so let&amp;rsquo;s do that:
allsides_page = row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] allsides_page = &#39;https://www.allsides.com&#39; &#43; allsides_page print(allsides_page)  It is a relative path in the HTML, so we prepend the site&amp;rsquo;s URL to make it a link we can request later.
Getting the link was a bit different than just selecting an element. We had to access an attribute (href) of the element, which is done using brackets, like how we would access a Python dictionary. This will be the same for other attributes of elements, like src in images and videos.
Bias rating 

We can see that the rating is displayed as an image so how can we get the rating in words? Looking at the HTML notice the link that surrounds the image has the text we need:
&amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/media-bias/left-center&amp;quot;&amp;gt; &amp;lt;img src=&amp;quot;...&amp;quot; width=&amp;quot;144&amp;quot; height=&amp;quot;24&amp;quot; alt=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot; title=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot;&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  We could also pull the alt attribute, but the link looks easier. Let&amp;rsquo;s grab it:
bias = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;] bias = bias.split(&#39;/&#39;)[-1] print(bias)  Here we selected the anchor tag by using the class name and tag together: .views-field-field-bias-image is the class of the &amp;lt;td&amp;gt; and &amp;lt;a&amp;gt; is for the anchor nested inside.
After that we extract the href just like before, but now we only want the last part of the URL for the name of the bias so we split on slashes and get the last element of that split (left-center).
Community feedback data 

The last thing to scrape is the agree/disagree ratio from the community feedback area. The HTML of this cell is pretty convoluted due to the styling, but here&amp;rsquo;s the basic structure:
&amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;getratingval&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;rate-widget-4 rate-widget clear-block rate-average rate-widget-yesno&amp;quot; id=&amp;quot;rate-node-76-4-1&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;item-list&amp;quot;&amp;gt; &amp;lt;ul&amp;gt; &amp;lt;li class=&amp;quot;first&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-3&amp;quot;&amp;gt;agree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;li class=&amp;quot;last&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-4&amp;quot;&amp;gt;disagree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;rate-details&amp;quot;&amp;gt; &amp;lt;span class=&amp;quot;agree&amp;quot;&amp;gt;8241&amp;lt;/span&amp;gt;/&amp;lt;span class=&amp;quot;disagree&amp;quot;&amp;gt;6568&amp;lt;/span&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/td&amp;gt;  The numbers we want are located in two span elements in the last div. Both span elements have classes that are unique in this cell so we can use them to make the selection:
agree = row.select_one(&#39;.agree&#39;).text agree = int(agree) disagree = row.select_one(&#39;.disagree&#39;).text disagree = int(disagree) agree_ratio = agree / disagree print(f&amp;quot;Agree: {agree}, Disagree: {disagree}, Ratio {agree_ratio:.2f}&amp;quot;)  Using .text will return a string, so we need to convert them to integers in order to calculate the ratio.
Side note: If you&amp;rsquo;ve never seen this way of formatting print statements in Python, the f at the front allows us to insert variables right into the string using curly braces. The :.2f is a way to format floats to only show two decimals places.
If you look at the page in your browser you&amp;rsquo;ll notice that they say how much the community is in agreement by using &amp;ldquo;somewhat agree&amp;rdquo;, &amp;ldquo;strongly agree&amp;rdquo;, etc. so how do we get that? If we try to select it:
print(row.select_one(&#39;.community-feedback-rating-page&#39;))  It shows up as None because this element is rendered with Javascript and requests can&amp;rsquo;t pull HTML rendered with Javascript. We&amp;rsquo;ll be looking at how to get data rendered with JS in a later article, but since this is the only piece of information that&amp;rsquo;s rendered this way we can manually recreate the text.
To find the JS files they&amp;rsquo;re using, just CTRL&#43;F for &amp;ldquo;.js&amp;rdquo; in the page source and open the files in a new tab to look for that logic.
It turned out the logic was located in the eleventh JS file and they have a function that calculates the text and color with these parameters:
 Range Agreeance   $ratio  3$ absolutely agrees   $2 strongly agrees   $1.5 agrees   $1 somewhat agrees   $ratio = 1$ neutral   $0.67 somewhat disgrees   $0.5 disgrees   $0.33 strongly disagrees   $ratio \leq 0.33$ absolutely disagrees   Let&amp;rsquo;s make a function that replicates this logic:
def get_agreeance_text(ratio): if ratio &amp;gt; 3: return &amp;quot;absolutely agrees&amp;quot; elif 2 &amp;lt; ratio &amp;lt;= 3: return &amp;quot;strongly agrees&amp;quot; elif 1.5 &amp;lt; ratio &amp;lt;= 2: return &amp;quot;agrees&amp;quot; elif 1 &amp;lt; ratio &amp;lt;= 1.5: return &amp;quot;somewhat agrees&amp;quot; elif ratio == 1: return &amp;quot;neutral&amp;quot; elif 0.67 &amp;lt; ratio &amp;lt; 1: return &amp;quot;somewhat disagrees&amp;quot; elif 0.5 &amp;lt; ratio &amp;lt;= 0.67: return &amp;quot;disagrees&amp;quot; elif 0.33 &amp;lt; ratio &amp;lt;= 0.5: return &amp;quot;strongly disagrees&amp;quot; elif ratio &amp;lt;= 0.33: return &amp;quot;absolutely disagrees&amp;quot; else: return None print(get_agreeance_text(2.5))  Now that we have the general logic for a single row and we can generate the agreeance text, let&amp;rsquo;s create a loop that gets data from every row on the first page:
data= [] for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d)  In the loop we can combine any multi-step extractions into one to create the values in the least number of steps.
Our data list now contains a dictionary containing key information for every row.
print(data[0])  Keep in mind that this is still only the first page. The list on AllSides is three pages long as of this writing, so we need to modify this loop to get the other pages.
Requesting and parsing multiple pages Notice that the URLs for each page follow a pattern. The first page has no parameters on the URL, but the next pages do; specifically they attach a ?page=# to the URL where &amp;lsquo;#&amp;rsquo; is the page number.
Right now, the easiest way to get all pages is just to manually make a list of these three pages and loop over them. If we were working on a project with thousands of pages we might build a more automated way of constructing/finding the next URLs, but for now this works.
pages = [ &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=1&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=2&#39; ]  According to AllSides&amp;rsquo; robots.txt we need to make sure we wait ten seconds before each request.
Our loop will: - request a page - parse the page - wait ten seconds - repeat for next page.
Remember, we&amp;rsquo;ve already tested our parsing above on a page that was cached locally so we know it works. You&amp;rsquo;ll want to make sure to do this before making a loop that performs requests to prevent having to reloop if you forgot to parse something.
By combining all the steps we&amp;rsquo;ve done up to this point and adding a loop over pages, here&amp;rsquo;s how it looks:
from time import sleep data= [] for page in pages: r = requests.get(page) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) rows = soup.select(&#39;tbody tr&#39;) for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d) sleep(10)  Now we have a list of dictionaries for each row on all three pages.
To cap it off, we want to get the real URL to the news source, not just the link to their presence on AllSides. To do this, we will need to get the AllSides page and look for the link.
If we go to ABC News&amp;rsquo; page there&amp;rsquo;s a row of external links to Facebook, Twitter, Wikipedia, and the ABC News website. The HTML for that sections looks like this:
&amp;lt;div class=&amp;quot;row-fluid source-links gray-bg-box&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;container&amp;quot;&amp;gt;	&amp;lt;a href=&amp;quot;https://www.facebook.com/ABCNews/&amp;quot; class=&amp;quot;facebook&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-facebook&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;Facebook&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://twitter.com/ABC&amp;quot; class=&amp;quot;twitter&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-twitter&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Twitter&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/ABC_News&amp;quot; class=&amp;quot;wikipedia&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-wikipedia-w&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Wikipedia&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;http://abcnews.go.com/&amp;quot; class=&amp;quot;www&amp;quot;&amp;gt;&amp;lt;i class=&amp;quot;fa fa-globe&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt; &amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;ABC News&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;/contact&amp;quot; class=&amp;quot;improve-this-page&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-line-chart&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Improve this page&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;  Notice the anchor tag (&amp;lt;a&amp;gt;) that contains the link to ABC News has a class of &amp;ldquo;www&amp;rdquo;. Pretty easy to get with what we&amp;rsquo;ve already learned:
website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;]  So let&amp;rsquo;s make another loop to request the AllSides page and get links for each news source. Unfortunately, some pages don&amp;rsquo;t have a link in this grey bar to the news source, which brings up a good point: always account for elements to randomly not exist.
Up until now we&amp;rsquo;ve assumed elements exist in the tables we scraped, but it&amp;rsquo;s always a good idea to program scrapers in way so they don&amp;rsquo;t break when an element goes missing.
Using select_one or select will always return None or an empty list if nothing is found, so in this loop we&amp;rsquo;ll check if we found the website element or not so it doesn&amp;rsquo;t throw an Exception when trying to access the href attribute.
Finally, since there&amp;rsquo;s 265 news source pages and the wait time between pages is 10 seconds, it&amp;rsquo;s going to take ~44 minutes to do this. Instead of blindly not knowing our progress, let&amp;rsquo;s use the tqdm library to give us a nice progress bar:
!pip install tqdm  from tqdm import tqdm_notebook for d in tqdm_notebook(data): r = requests.get(d[&#39;allsides_page&#39;]) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) try: website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;] d[&#39;website&#39;] = website except TypeError: pass sleep(10)  tqdm is a little weird at first, but essentially tqdm_notebook is just wrapping around our data list to produce a progress bar. We are still able to access each dictionary, d, just as we would normally. Note that tqdm_notebook is only for Jupyter notebooks. In regular editors you&amp;rsquo;ll just import tqdm from tqdm and use tqdm instead.
Saving our data So what do we have now? At this moment, data is a list of dictionaries, each of which contains all the data from the tables as well as the websites from each individual news source&amp;rsquo;s page on AllSides.
The first thing we&amp;rsquo;ll want to do now is save that data to a file so we don&amp;rsquo;t have to make those requests again. We&amp;rsquo;ll be storing the data as JSON since it&amp;rsquo;s already in that form anyway:
import json with open(&#39;allsides.json&#39;, &#39;w&#39;) as f: json.dump(data, f)  To load it back in when you need it:
with open(&#39;allsides.json&#39;, &#39;r&#39;) as f: data = json.load(f)  If you&amp;rsquo;re not familiar with JSON, just quickly open allsides.json in an editor and see what it looks like. It should look almost exactly like what data looks like if we print it in Python: a list of dictionaries.
Brief Data Analysis Before ending this article I think it would be worthwhile to actually see what&amp;rsquo;s interesting about this data we just retrieved. So, let&amp;rsquo;s answer a couple of questions.
Which ratings for outlets does the community absolutely agree on?
To find where the community absolutely agrees we can do a simple list comprehension that checks each dict for the agreeance text we want:
abs_agree = [d for d in data if d[&#39;agreeance_text&#39;] == &#39;absolutely agrees&#39;] print(f&amp;quot;{&#39;Outlet&#39;:&amp;lt;20} {&#39;Bias&#39;:&amp;lt;20}&amp;quot;) print(&amp;quot;-&amp;quot; * 30) for d in abs_agree: print(f&amp;quot;{d[&#39;name&#39;]:&amp;lt;20} {d[&#39;bias&#39;]:&amp;lt;20}&amp;quot;)  Using some string formatting we can make it look somewhat tabular. Interestingly, C-SPAN is the only center bias that the community absolutely agrees on. The others for left and right aren&amp;rsquo;t that surprising.
Making analysis easier with Pandas Which ratings for outlets does the community absolutely disagree on?
To make analysis a little easier, we can also load our JSON data into a Pandas DataFrame as well. This is easy with Pandas since they have a simple function for reading JSON into a DataFrame.
As an aside, if you&amp;rsquo;ve never used Pandas, Matplotlib, or any of the other data science libraries, I would definitely recommend checking out [Jose Portilla&amp;rsquo;s data science course]() for a great intro to these tools and many machine learning concepts.
Now to the DataFrame:
import pandas as pd df = pd.read_json(open(&#39;allsides.json&#39;, &#39;r&#39;)) df.set_index(&#39;name&#39;, inplace=True) df.head()  Now filter the DataFrame by &amp;ldquo;agreeance_text&amp;rdquo;:
df[df[&#39;agreeance_text&#39;] == &#39;strongly disagrees&#39;]  It looks like much of the community disagrees strongly with certain outlets being rated with a &amp;ldquo;center&amp;rdquo; bias.
Let&amp;rsquo;s make a quick visualization of agreeance. Since there&amp;rsquo;s too many news sources to plot so let&amp;rsquo;s pull only those with the most votes. To do that, we can make a new column that counts the total votes and then sort by that value:
df[&#39;total_votes&#39;] = df[&#39;agree&#39;] &#43; df[&#39;disagree&#39;] df.sort_values(&#39;total_votes&#39;, ascending=False, inplace=True) df.head(10)  Visualizing the data To make a bar plot we&amp;rsquo;ll use Matplotlib with Seaborn&amp;rsquo;s dark grid style:
import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-darkgrid&#39;)  As mentioned above, we have too many news outlets to plot comfortably, so just make a copy of the top 25 and place it in a new df2 variable:
df2 = df.head(25).copy() df2.head()  With the top 25 news sources by amount of feedback, let&amp;rsquo;s create a stacked bar chart where the number of agrees are stacked on top of the number of disagrees. This makes the total height of the bar the total amount of feedback.
Below, we first create a figure and axes, plot the agree bars, plot the disagree bars on top of the agrees using bottom, then set various text features:
fig, ax = plt.subplots(figsize=(20, 10)) ax.bar(df2.index, df2[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(df2.index, df2[&#39;disagree&#39;], bottom=df2[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) ax.set_ylabel = &#39;Total feedback&#39; plt.yticks(fontsize=&#39;x-large&#39;) plt.xticks(rotation=60, ha=&#39;right&#39;, fontsize=&#39;x-large&#39;, rotation_mode=&#39;anchor&#39;) plt.legend([&#39;Agree&#39;, &#39;Disagree&#39;], fontsize=&#39;xx-large&#39;) plt.title(&#39;AllSides Bias Rating vs. Community Feedback&#39;, fontsize=&#39;xx-large&#39;) plt.show()  For a slightly more complex version, let&amp;rsquo;s make a subplot for each bias and plot the respective news sources.
This time we&amp;rsquo;ll make a new copy of the original DataFrame beforehand since we can plot more news outlets now.
Instead of making one axes, we&amp;rsquo;ll create a new one for each bias to make six total subplots:
df3 = df.copy() fig = plt.figure(figsize=(15,15)) biases = df3[&#39;bias&#39;].unique() for i, bias in enumerate(biases): # Get top 10 news sources for this bias and sort index alphabetically temp_df = df3[df3[&#39;bias&#39;] == bias].iloc[:10] temp_df.sort_index(inplace=True) # Get max votes, i.e. the y value for tallest bar in this temp dataframe max_votes = temp_df[&#39;total_votes&#39;].max() # Add a new subplot in the correct grid position ax = fig.add_subplot(len(biases) / 2, 2, i &#43; 1) # Create the stacked bars ax.bar(temp_df.index, temp_df[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(temp_df.index, temp_df[&#39;disagree&#39;], bottom=temp_df[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) # Place text for the ratio on top of each bar for x, y, ratio in zip(ax.get_xticks(), temp_df[&#39;total_votes&#39;], temp_df[&#39;agree_ratio&#39;]): ax.text(x, y &#43; (0.02 * max_votes), f&amp;quot;{ratio:.2f}&amp;quot;, ha=&#39;center&#39;) ax.set_ylabel(&#39;Total feedback&#39;) ax.set_title(bias.title()) # Make y limit larger to compensate for text on bars ax.set_ylim(0, max_votes &#43; (0.12 * max_votes)) # Rotate tick labels so they don&#39;t overlap plt.setp(ax.get_xticklabels(), rotation=30, ha=&#39;right&#39;) plt.tight_layout(w_pad=3.0, h_pad=1.0) plt.show()  Hopefully the comments help with how these plots were created. We&amp;rsquo;re just looping through each unique bias and adding a subplot to the figure.
When interpreting these plots keep in mind that the y-axis has different scales for each subplot. Overall it&amp;rsquo;s a nice way to see which outlets have a lot of votes and where the most disagreement is. This is what makes scraping so much fun!
Final words We have the tools to make some fairly complex web scrapers now, but there&amp;rsquo;s still the issue with Javascript rendering. This is something that deserves its own article, but for now we can do quite a lot.
There&amp;rsquo;s also some project organization that needs to occur when making this into a more easily runnable program. We need to pull it out of this notebook and code in command-line arguments if we plan to run it often for updates.
These sorts of things will be addressed later when we build more complex scrapers, but feel free to let me know in the comments of anything in particular you&amp;rsquo;re interested in learning about.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_4/web-scraping-request/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">code</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/homo_deus/"><img src="/topic_1/homo_deus/images/Homo_Deus_hud949c3cfa973b39c16ba4d2c8cf1d6ae_196421_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        [สรุปหนังสือ] Homo Deus : A Brief History of Tomorrow
      </h3>
      <p class="refresh-summary"> “This is the best reason to learn history: not in order to predict the future, but to free yourself of the past and imagine alternative destinies. Of course this is not total freedom – we cannot avoid being shaped by the past. But some freedom is better than none.”
เมื่อเราเข้าใจ “เรื่องราวที่แท้จริง” ของมนุษย์ เมื่อนั้นเราก็จะเข้าใจ “เป้าหมาย” ของการเดินทางของมนุษยชาติ
Homo Deus คือ หนังสือภาคต่อจาก Sapiens ของ Yuval Noah Harari ศาสตราจารย์ทางประวัติศาสตร์ผู้นำเสนอเรื่องราวของ “อนาคต” ของมนุษยชาติผ่านการศึกษาความเป็นมาทางประวัติศาสตร์ของเผ่าพันธุ์ Homo Sapiens ที่พัฒนาตนเองขึ้นจากการเป็น “สัตว์” อันไร้ซึ่งความสำคัญใดๆมาเป็น “เทพเจ้า” ผู้กำหนดชะตาชีวิตของทุกสรรพสิ่ง แต่เมื่อสิ่งที่ถูกมนุษย์สร้างขึ้นอย่าง algorithm เริ่มมีสติปัญญาที่ชาญฉลาดกว่ามันสมองของมนุษย์ โลกของเราทุกคนจะเปลี่ยนแปลงไปอย่างไร
Chapter 1: The New Human Agenda โศกนาฏกรรม 3 อันดับสำคัญที่มนุษยชาติต้องพบเจอตลอดระยะเวลาหลายพันปีที่ผ่านมานั้นประกอบไปด้วย “ความอดอยาก” “โรคระบาด” และ “สงคราม”
Famine: ในยุคเกษตรกรรม หากภัยธรรมชาติส่งผลกระทบต่อผลิตผลทางการเกษตรของหมู่บ้านแห่งหนึ่ง ความทุกข์ทรมานจากความอดอยากนั้นเป็นสิ่งที่แทบจะหลีกเลี่ยงไม่ได้เลย แต่ทุกวันนี้ ผลิตภาพทางการเกษตรและความเป็นอยู่ที่ดีขึ้นของมนุษย์ได้ทำให้ภาวะอดอยากหายสาบสูญไปจากโลกจนเกือบจะหมดสิ้นและมนุษย์กลับมีโอกาสเสียชีวิตจากภาวะการรับประทานอาหารที่ “มากเกินไป” แทน
Plague: เริ่มต้นในช่วงทศวรรษที่ 1330 ภัยร้ายนามว่า the Black Death ได้คร่าชีวิตมนุษย์กว่า 1 ใน 4 ของประชากรทั้งหมดในแถบทวีปยุโรปและเอเชีย ในยุคเริ่มต้นของการล่าอาณานิคมของชาวสเปนในแถบทวีปอเมริกาได้นำพาเชื้อโรคที่ชาวท้องถิ่นไม่เคยได้สัมผัสข้ามทวีปมาแพร่ระบาดไปทั่วอาณาจักร Aztec และ Maya อย่างรวดเร็วจนทำให้กว่า 90% ของประชากรท้องถิ่นต้องจบชีวิตลงภายในระยะเวลาอันสั้น แต่ในปัจจุบัน ภาวะโรคระบาดนั้นสร้างความเสียหายในอัตราที่น้อยลงเป็นอย่างมากทั้งๆที่สภาวะการอาศัยอยู่รวมกันในเมืองที่แออัดและระบบการคมนาคมอันรวดเร็วนั้นถือเป็นแหล่งแพร่กระจายโรคระบาดได้เป็นอย่างดี ทั้งนี้ก็เพราะว่าวิทยาการทางการแพทย์สมัยใหม่ได้รับการพัฒนาจนสามารถตรวจจับสาเหตุและควบคุมการแพร่ระบาดของโรคติดต่อได้อย่างรวดเร็ว (Ebola ที่เกิดขึ้นในปี 2014 คร่าประชากรไปเพียงแค่หลักหมื่นคนเท่านั้น) ความเสี่ยงของโรคระบาดร้ายแรงในอนาคตน่าจะเกิดขึ้นจากฝีมืของมนุษย์มากกว่าจากธรรมชาติ
War: ปัจจุบันสงครามและการก่ออาชญากรรมนั้นเป็นเพียงแค่ 1% ของสาเหตุการตายของมนุษย์ (ซึ่งน้อยกว่าการฆ่าตัวตายและโรคเบาหวาน) ทั้งนี้ก็เพราะว่าสงครามในยุคปัจจุบันนั้นแทบจะ “ไม่มีประโยชน์” ในความคุ้มค่าทางเศรษฐกิจอีกต่อไป การเกิดขึ้นของอาวุธ “นิวเคลียร์” ส่งผลให้การสู้รบกันระหว่างชาติมหาอำนาจเป็นเหมือนการ “ฆ่าตัวตายหมู่” ที่ไร้เหตุผลสิ้นดี ชัยชนะจากสงครามนั้นก็แทบจะไม่มีมูลค่าอีกต่อไปเพราะทรัพย์สมบัติที่มีค่าที่สุดของมนุษย์ในยุคนี้อยู่ใน “คอมพิวเตอร์” และ “มันสมอง” ของมนุษย์ (สงครามที่เกิดขึ้นในช่วงที่ผ่านมานั้นมักเกิดขึ้นกับประเทศที่มีทรัพยากรมหาศาล อาทิ บ่อน้ำมันหรือเหมืองแร่) ในทางกลับกัน “ความสงบสุขอันยั่งยืน” ก็ได้เกิดขึ้นมาแทนที่ “ความสงบสุบชั่วคราว” ผ่านการทำการค้าและการลงทุนระหว่างประเทศที่ทำให้ความคิดถึงการทำสงครามระหว่างกันนั้น “เป็นไปไม่ได้” อีกต่อไป และนี่คือสาเหตที่ประเทศจีนเลือกทำสัญญาทางธุรกิจกับ Apple และ Microsoft แทนการยกทัพมายึด Silicon Valley
เมื่อ “ความท้าทาย” ในอดีตได้รับการแก้ไขจนเกือบสมบูรณ์แล้ว มนุษย์ผู้มีความทะเยอทะยานสูงก็ได้เริ่มออกตามหา “ความท้าทาย” ครั้งใหม่อันประกอบไปด้วย ความเป็นอมตะ ความสุขและการก้าวข้ามขีดจำกัดของมนุษย์ธรรมดาไปเป็น “พระเจ้า”
Immortality: “ความตาย” ถือเป็นส่วนสำคัญอย่างมากของ “ศาสนา” ที่คอยพร่ำสอนให้มนุษย์ทำความดีเพื่อสร้าง “ชีวิตหลังความตาย” ที่ยอดเยี่ยม (ถ้าไม่มีความตาย แนวคิดของสวรรค์และนรกคงไม่เคยเกิดขึ้น) แต่ปัจจุบัน “ความตาย” ที่เกิดขึ้นกับมนุษย์ผ่าน “โรคร้าย” ต่างๆนั้นถูกตีความใหม่ให้กลายมาเป็นเพียงแค่ “ปัญหาทางเทคนิค” ของร่างกายมนุษย์ที่รอให้เทคโนโลยีทางการแพทย์แก้ไขปัญหาให้หายขาด ปัจจุบัน นักวิทยาศาสตร์และบริษัทเทคโนโลยีชั้นนำเริ่มให้ความสำคัญกับ Life Science หรือศาสตร์แห่งชีวิตที่ศึกษาเรื่องการรักษาความผิดปกติของร่างกายและการยืดอายุไขของเซลส์และอวัยวะต่างๆของมนุษย์ ในเร็วๆนี้ เราอาจจะเริ่มเห็นมนุษย์ที่มีอายุไขเพิ่มขึ้นจากเดิม (ซึ่งนั่นก็ทำให้โครงสร้างทางสังคมเปลี่ยนแปลงไปจากปัจจุบันโดยสิ้นเชิง ลองคิดดูว่ามนุษย์อาจจะเกษียณในอายุ 120 ปี ปูตินอาจจะเป็นนายกของรัสเซียต่อไปอีก 60 ปีก็ได้) ส่วนในระยะยาว มนุษย์อาจเริ่มกลายเป็น “a-mortal” หรือ ผู้ที่มีร่างกายเป็นอมตะ (ยกเว้น ได้รับความเสียหายอย่างรุนแรงจนเสียชีวิต ซึ่งก็อาจจะทำให้บุคคลอมตะเหล่านี้เกรงกลัวต่อกิจกรรมที่มีความเสี่ยงอย่างสุดโต่ง) ซึ่งแน่นอนว่า “มนุษย์” ทุกคนจะต้องพยายามทุกวิถีทางให้ตัวเองกลายเป็นมีชีวิตอยู่ชั่วนิรันดร์และเงินทุนมหาศาลจะเป็นตัวกระตุ้นให้การตามล่าความเป็นอมตะดำเนินต่อไปจนสัมฤทธิ์ผล
Happiness: ตั้งแต่การเริ่มต้นของการสร้างอาณาจักรและประเทศชาติ การพัฒนาโครงสร้างพื้นฐาน การศึกษา สุขอนามัยและการยกระดับความเป็นอยู่อาศัยของประชาชนนั้นมีวัตถุประสงค์หลักคือการสร้าง “ความแข็งแกร่ง” ให้กับประเทศชาติโดยปราศจากการคำนึงถึง “ความสุข” ที่แท้จริงของประชาชน (ประเทศพัฒนาแล้วมากมายมีอัตราการฆ่าตัวตายสูงขึ้นตามการเพิ่มขึ้นของ GDP per capita) การสร้างความสุขที่แท้จริงของมนุษย์นั้นต้องเริ่มต้นจากการแก้ไข “อุปสรรค” สำคัญ 2 ข้อ ได้แก่ 1. อุปสรรคทาง “จิตวิทยา” ที่ว่าด้วยความสุขของมนุษย์นั้นเกิดขึ้นจากการที่ความจริงที่พวกเขาต้องเผชิญนั้นตรงกับความคาดหวังของพวกเขามากน้อยแค่ไหน (ซึ่งปัจจุบัน ความก้าวหน้าทางเทคโนโลยีได้นำพาให้ความคาดหวังของมนุษย์สูงขึ้นๆไปเรื่อยๆ) 2. อุปสรรคทาง “ชีววิทยา” ที่พิสูจน์ไว้แล้วว่าความสุขของมนุษย์ขึ้นอยู่กับ “ฮอร์โมน” ภายในร่างกายเท่านั้น อันเป็นไปตามการวิวัฒนาการของธรรมชาติที่สร้างให้มนุษย์มีความรู้สึกดีหลังจากได้กระทำสิ่งที่เป็นประโยชน์ต่อการดำรงชีวิตได้สำเร็จก่อนที่ความรู้สึกนั้นจะจางหายไปในเวลาต่อมา เทคโนโลยีในอนาคตที่จะเข้ามาเติมเต็มช่องว่างก้อนใหญ่ของมนุษย์ก็คือการสร้าง “ความสุขชั่วนิรันดร์” ผ่านการ “จัดระเบียบ” ระบบการทำงานของสารเคมีในร่างกายมนุษย์ให้สามารถหลั่งฮอร์โมนให้มนุษย์มีความสุขในเชิงบวกได้ตลอดเวลา (ปัจจุบันมีมนุษย์หลายล้านคนกินยาระงับประสาท ยาแก้โรคซึมเศร้าและยาเสพย์ติด ที่ล้วนมีผลต่อการปรับเปลี่ยนสารเคมีที่มีผลต่ออารมณ์ความรู้สึกของมนุษย์ทั้งนั้น)
Divinity: หลังจากที่มนุษย์สามารถออกแบบร่างกายและจิตใจของตัวเองได้แล้ว เป้าหมายที่เหลือของพวกเขาก็คงเป็นการอัพเกรด “พลัง” ให้กับตัวเองให้เทียบชั้นกับ “เทพเจ้า” ตั้งแต่ การเสริมสร้างความสามารถของร่างกายผ่านการปรับเปลี่ยนพันธุกรรม การผสมผสานหุ่นยนต์เข้ากับร่างกายและสมองของมนุษย์ ไปจนถึงการอัพโหลดจิตใจของมนุษย์เข้าไปยังเครื่องจักรที่ไม่มีวันตาย ทั้งหมดนี้อาจจะดูเพ้อฝัน แต่สิ่งหนึ่งที่สามารถมั่นใจได้เลยก็คือ มนุษย์ในปัจจุบันคงไม่สามารถจินตนาการถึง “มนุษย์อนาคต” ที่ก้าวผ่านจากสายพันธุ์ Homo Sapiens (มนุษย์ฉลาด) มาเป็น Homo Deus (มนุษย์เทพเจ้า) ได้อย่างแน่นอน
ประวัติศาสตร์เป็นข้อพิสูจน์ชั้นดีว่าการ “อัพเกรดมนุษย์” นั้นเป็นสิ่งที่ไม่อาจหลีกเลี่ยงได้ในอนาคต เทคโนโลยีในช่วงเริ่มต้นนั้นอาจเกิดขึ้นในรูปแบบของกระบวนการแก้ไขความผิดปกติต่างๆของมนุษย์ (อาทิ การเลือกเอ็มบรีโอของเด็กทารกที่มีสุขภาพสมบูรณ์ หรือ การแก้ไขดีเอ็นเอที่มีปัญหาของเอ็มบรีโอ) แต่หากกระบวนการเหล่านั้นสามารถทำให้มนุษย์ปกติมีคุณสมบัติที่เพิ่มขึ้น สุดท้ายโลกจะไม่อาจควบคุมการแพร่กระจายของเทคโนโลยีนี้ได้ (หากอเมริกาสั่งแบนการตัดต่อพันธุกรรม แต่เกาหลีเหนือสนับสนุนโครงการนี้เต็มที่เพื่อสร้างมนุษย์อัจฉริยะ อเมริกาคงไม่มีทางยอมปล่อยโอกาสนี้ไปแน่ๆ)
แต่ก็อย่าลืมเด็ดขาดว่า “อนาคต” ที่พวกเราพอจะมองเห็นอยู่ลิบๆนั้นเกิดขึ้นจากการร้อยเรียงเรื่องราวและความเชื่อของมนุษย์ที่ถูกสร้างขึ้นโดยมนุษย์ตั้งแต่ในอดีตจนถึงปัจจุบัน นั่นหมายความว่าโลกและมนุษยชาติอาจจะไม่ใช่สิ่งที่พวกเราจินตนาการถึงได้เลยเมื่อ “อนาคต” ที่แท้จริงเดินทางมาถึง
Part I – Homo Sapiens Conquers the World Chapter 2: The Anthropocene การพยากรณ์ความสัมพันธุ์ระหว่าง Homo Sapiens ยุคปัจจุบันกับ Homo Deus ในยุคอนาคตนั้นคงเป็นสิ่งที่ไม่ง่ายนัก แต่เราก็อาจพอจะใช้ประวัติศาสตร์ของความสัมพันธุ์ระหว่าง “มนุษย์” กับ “สัตว์” ชนิดอื่นมาใช้เป็นกรอบความคิดคร่าวๆได้
Antropocene คือ “ยุคแห่งมนุษย์” ที่เริ่มต้นขึ้นเมื่อ 70,000 ปีก่อนหลังจากการปฏิวัติทางความตระหนักรู้ของมนุษย์สายพันธุ์ Homo Sapiens ที่ทำให้ “มนุษย์” กลายเป็นสิ่งมีชีวิตสายพันธุ์เดียวในโลกที่กุมชะตากรรมของสิ่งมีชีวิตทั้งหมด (ปัจจุบัน สัดส่วนน้ำหนักรวมของสิ่งมีชีวิตเกิน 90% ตกเป็นของมนุษย์ สัตว์เลี้ยงและสัตว์อุตสาหกรรม)
ความสัมพันธุ์ของ Homo Sapiens และสิ่งมีชีวิตชนิดอื่นๆในยุคของนักล่าสัตว์และนักหาของป่านั้นอยู่ในรูปของความเชื่อที่ว่าสิ่งมีชีวิตและไม่มีชีวิตทั้งหมดล้วนมีจิตวิญญาณ (Animism) ที่มนุษย์จะต้องทำความเคารพและเกื้อกูลระหว่างกัน (การล่าสัตว์ในยุคโบราณนั้นต้องประกอบด้วยพิธีขอขมาสัตว์ที่ถูกล่านั้นๆ) ก่อนที่การปฏิวัติทางเกษตรกรรมจะเปลี่ยนพฤติกรรมและความเชื่อของมนุษย์ให้กลายเป็นการเคารพต่อ “เทพเจ้า” ผู้มีพลังอำนาจมหาศาลที่มาพร้อมกับการเปลี่ยนความคิดของการนับถือสัตว์กลายเป็นการมองว่าสิ่งมีชีวิตชนิดอื่นๆนั้นมีศักดิ์ที่ต้อยต่ำกว่ามนุษย์อันเป็นเหตุให้ปศุสัตว์ผู้ถูกเลือกอย่าง หมู วัว ไก่และแกะต้องกลายมาเป็นสัตว์ที่ต้องทนทุกข์ทรมานมากที่สุดในโลก (พร้อมกับการแพร่กระจายจำนวนอย่างรวดเร็ว) ซึ่งการปฏิวัติทางวิทยาศาสตร์ก็ยิ่งเข้ามาซ้ำเติมความโหดร้ายของมนุษย์ที่มีต่อสัตว์เหล่านั้นเข้าไปอีก (ยกเว้นในช่วงไม่กี่ทศวรรษที่ผ่านมาที่มนุษย์เริ่มหวนกลับมาให้ความสำคัญกับสิ่งมีชีวิตชนิดอื่น)
ในอนาคตนั้น เราไม่อาจรับรู้ได้เลยว่า “มนุษย์เวอร์ชั่นอัพเกรด” หรือ A.I. ผู้มีสติปัญญาที่หลักแหลมกว่า Homo Sapiens ในปัจจุบันมากจะจัดการกับพวกเราเหมือนในอดีตที่ผ่านมาหรือไม่
Antropocene หรือ “ยุคแห่งมนุษย์” (ขอบคุณภาพจาก Motherboard – Vice)
Chapter 3: The Human Spark “อะไร” คือสิ่งที่ทำให้มนุษย์มีความพิเศษมากกว่าสิ่งมีชีวิตชนิดอื่น
หากคำถามนี้ถูกถามในยุคที่วิทยาศาสตร์ยังไม่ได้รรับการยอมรับอย่างกว้างขวาง คำตอบคงเป็น “วิญญาณ” ที่มีเพียงมนุษย์เท่านั้นที่ได้ครอบครอง อันเป็นเหตุให้มนุษย์มองเห็นสิ่งมีชีวิตอื่นๆเป็นเพียงร่างที่ไร้วิญญาณและสามารถย่ำยีได้ตามชอบ (ปัจจุบันชาวอเมริกันเพียงแค่ 15% เท่านั้นที่เชื่อว่ามนุษย์เกิดขึ้นจากการวิวัฒนาการตามธรรมชาติเท่านั้นโดยไม่ต้องพึ่งพระเจ้า)
ส่วนคำตอบที่น่าจะได้รับการยอมรับในยุคปัจจุบันมากกว่าก็คือ “จิตใจ” (mind) และ “การตระหนักรู้” (consciousness) ซึ่งปัจจุบัน เทคโนโลยีทางวิทยาศาสตร์สามารถตรวจจับกระบวนการทำงานของระบบประสาทภายในสมองของสิ่งมีชีวิตได้อย่างมีประสิทธิภาพในระดับหนึ่งแต่ก็ยังไม่สามารถตรวจจับกระบวนการทำงานของ “จิตใจ” ที่ทำให้มนุษย์รู้สึกรัก โลภ โกรธหรือกลัวได้เลย (มีเพียงตัวของเราเองเท่านั้นที่เชื่อมั่นในการมีอยู่ของจิตใจและความรู้สึกของเรา ไม่แน่เราอาจจะเป็นเพียงผู้เล่นในโลกจำลองของสิ่งมีชีวิตชั้นสูงหรือมนุษย์ในอนาคตอยู่ก็เป็นได้)
นักวิทยาศาสตร์หลายคนพยายามจับเอาคอนเส็ปต์ของระบบคอมพิวเตอร์อย่าง algorithm ที่เป็นเหมือนชุดคำสั่งขนาดใหญ่มาใช้อธิบายถึงพฤติกรรมของสิ่งมีชีวิตต่างๆแทนการใช้จิตใจ อาทิ เมื่อ ลิงเห็นเสือ ประสาทตาของลิงก็จะทำการส่งกระแสไฟฟ้าเข้าไปยังสมองเพื่อประมวลผลผ่าน algorithm ของลิงตัวนั้นและส่งผลลัพธ์ออกมาในรูปของกระแสประสาทเพื่อให้ลิงวิ่งหนีเสือ โดยที่ลิงไม่ได้มีความรู้สึกกลัวหรือตกใจแต่อย่างใดเลย
แต่เอาจริงๆแล้ว การทดลองหลายครั้งในอดีตก็ได้พิสูจน์ว่าสิ่งมีชีวิตชนิดอื่นๆก็มีความตระหนักรู้ได้ไม่แพ้มนุษย์ อาทิ Clever Hans ม้าผู้แสนฉลาดแห่งเยอรมนีในยุคปี 1900s ที่มีความสามารถในการตอบปัญหาบวกลบคูณหารเลขผ่านการเคาะเท้าเป็นจำนวนครั้งตามคำตอบที่ถูกต้องได้อย่างแม่นยำ ซึ่งนักจิตวิทยาได้ค้นพบว่า Clever Hans ไม่ได้มีความสามารถในการคิดเลขเหมือนกับมนุษย์ แต่เจ้าม้าตัวนี้ใช้วิธีการเคาะเท้าพร้อมๆกับการสังเกตสีหน้าของมนุษย์ผู้เป็นคนถามคำถามที่มักจะแสดงอาการอย่างชัดเจนเมื่อ Clever Hans กระแทกเท้าจนใกล้ถึงคำตอบที่ถูกต้องและมันก็จะหยุดกระแทกเท้าไปในที่สุด
แต่จากการศึกษาประวัติศาสตร์ของมนุษยชาติ “ปัจจัย” ที่น่าจะส่งผลให้ Homo Sapiens กลายมาเป็นสิ่งมีชีวิตที่ยิ่งใหญ่ที่สุดในโลกได้ภายในเวลาเพียงแค่เสี้ยวหนึ่งของอายุของดาวดวงนี้ ไม่ใช่ “วิญญาณ” หรือ “จิตใจอันสูงส่ง” แต่กลับกลายเป็น “ความสามารถในการทำงานร่วมกันอย่างซับซ้อนและมีประสิทธิภาพ” อย่างที่สิ่งมีชีวิตชนิดอื่นสามารถทำได้ ซึ่งสาเหตุที่มนุษย์สามารถทำงานร่วมกันเป็นกลุ่มขนาดหลักพันหลักล้านคนได้นั้นเกิดจากการที่มนุษย์สามารถคิดค้น “ความเชื่อที่ถูกสร้างขึ้นโดยฝีมือมนุษย์” (imagined order) อาทิ ศาสนา พระเจ้า หลักมนุษยธรรม ประชาธิปไตยและระบบเงิน
มนุษย์คือสิ่งมีชีวิตเดียวในโลกที่สามารถจินตนาการ “ความหมาย” ของการมีชีวิตอยู่และการทำงานร่วมกันระหว่างมนุษย์ด้วยกันแองได้โดยไม่จำเป็นต้องพึ่ง “ความจริง” ที่เกิดขึ้นตามหลักการทางวิทยาศาสตร์และธรรมชาติ อาทิ นักรบชาวคริสเตียนเชื่อมั่นว่าตัวเองจะได้ขึ้นสวรรค์หากเข้าร่วมสงครามครูเสดเพื่อคร่าชีวิตนักรบชาวอิสลามที่มีความเชื่อคล้ายๆกัน
และเมื่อการเวลาเปลี่ยนไป ความเชื่อและความหมายของชีวิตก็เปลี่ยนแปลงไป ปัจจุบันความเชื่อเกี่ยวกับเทพเจ้าและศาสนากำลังเสื่อมความนิยม ขณะที่ความเชื่อเรื่องประชาธิปไตย ความเท่าเทียมกันและสิทธิมนุษยชนกำลังได้ความนิยมที่เพิ่มขึ้นเรื่อยๆ ส่วนในอนาคต ความเชื่อรูปแบบใหม่ที่มนุษย์ในยุคปัจจุบันอาจจะยังคาดไม่ถึงก็อาจจะเกิดขึ้นได้ในไม่ช้า
ม้าแสนรู้ Clever Hans ที่ไม่ได้ฉลาดเหมือนที่มนุษย์คิด (ขอบคุณภาพจาก Wikipedia)
Part II – Homo Sapiens Gives Meaning to the World Chapter 4: The Storytellers มนุษย์คือสิ่งมีชีวิตชนิดเดียวที่อาศัยอยู่ในโลกที่มีความจริงซ้อนกันอยู่ 3 ชั้น (three-layered reality) อันประกอบไปด้วย ความจริงเชิงวัตถุ (objective reality) อาทิ อากาศและสิ่งแวดล้อม ความจริงเชิงปัจเจกบุคคล (subjective reality) อันได้แก่ อารมณ์และความรู้สึกภายใน และความจริงที่ถูกสร้างขึ้นโดยมนุษย์ด้วยกันเอง (imagined reality)
ประวัติศาสตร์ของมนุษย์นั้นถูกสร้างขึ้นจากการร้อยเรียงกันของเรื่องเล่าและความเชื่อของมนุษย์ ตั้งแต่ 70,000 ปีก่อนที่มนุษย์เริ่มสามารถจินตนาการถึง “สิ่งที่ไม่มีอยู่จริง” อันเป็นจุดเริ่มต้นของการสร้าง “ความจริง” โดยฝีมือของจินตนาการของมนุษย์เพื่อสร้างความสามารถในการทำงานร่วมกันของมนุษย์หมู่มากได้สำเร็จ
โดยจุดเปลี่ยนครั้งสำคัญของมนุษยชาติ คือ การปฏิวัติทางเกษตรกรรมเมื่อ 12,000 ปีก่อนที่ทำให้มนุษย์เริ่มเปลี่ยนพฤติกรรมจากการออกล่าสัตว์และหาของป่าไปเป็นการอยู่รวมกันเป็นหลักแหล่งเป็นหมู่บ้านขนาดย่อม ก่อนที่ “อาณาจักร” ขนาดใหญ่จะเริ่มกำเนิดขึ้นหลังจากที่ชาวสุเมเรียนได้คิดค้น “ภาษาเขียน” อันเป็นพื้นฐานของระบบบัญชีและการปกครองของมวลมนุษย์ขนาดใหญ่ได้สำเร็จในช่วง 5,000 ปีก่อน และภาษาเขียนนี่เองที่เป็นตัวจุดประกายให้เกิดการสร้าง “เรื่องแต่ง” ให้กลายมาเป็น “เรื่องจริง” ที่ทำให้ประชาชนในแต่ละอาณาจักรเชื่อมั่นได้อย่างสนิทใจได้ ซึ่งในช่วงแรกเริ่มของอาณาจักรนั้น เรื่องจริงที่ถูกสร้างขึ้นโดยฝีมือมนุษย์นั้นมีลักษณะคล้ายๆกันก็คือ การนำเอา “เทพเจ้า” หรือ “พระเจ้า” มาเป็นจุดศูนย์กลางของความเชื่อและกฎระเบียบการปกครองของอาณาจักรโดยแต่ละอาณาจักรจะมีผู้นำที่เปรียบเสมือน “ตัวแทนของเทพเจ้า” หรือไม่ก็เป็น “เทพเจ้า” ซะเองเลยอย่างฟาโรห์ของอาณาจักรอียิปต์
ถึงแม้ในยุคปัจจุบัน ความเชื่อเรื่องเทพเจ้าและภูติผีปีศาจจะได้เสื่อมถอยลงไป แต่โลกของเรากลับเต็มไปด้วย “เรื่องแต่ง” โดยฝีมือมนุษย์ในยุคใหม่มากมาย ไม่ว่าจะเป็น ประเทศชาติ หลักการปกครอง เงินและบริษัท ที่ล้วนแล้วแต่จะมีอิทธิพลต่อมนุษย์มากขึ้นเรื่อยๆ ซึ่งเอาจริงๆ เรื่องแต่งเหล่านี้ล้วนมีความสำคัญอย่างมากในการอยู่ร่วมกันของมนุษย์ในยุคปัจจุบัน แต่พวกเราก็ไม่ควรลืมว่าอะไรคือเรื่องแต่งและอะไรคือ “เรื่องจริง”
The Creation of Adam (ขอบคุณภาพจาก Wikipedia)
Chapter 5: The Odd Couple สองหลักการที่ขัดแย้งกันมาตลอดในยุคสมัยใหม่ก็คือ “ศาสนา” กับ “วิทยาศาสตร์” ที่ต่างก็พยายามอธิบายถึง “ความจริง” หนึ่งเดียวของโลกมนุษย์
ศาสนาต่างๆ (รวมถึงลัทธิคอมมิวนิสต์ ทุนนิยมและความเชื่อเรื่องความเท่าเทียมกันของมนุษย์) นั้นล้วนมีพื้นฐานมาจาก “กฎเกณฑ์” ที่มนุษย์สร้างขึ้นโดยอาศัยการเรื่องเล่าที่ว่าอันแท้จริงแล้วกฎเกณฑ์เหล่านี้ถูกกำหนดโดยเทพเจ้าหรือเกิดขึ้นตามกฏของธรรมชาติซึ่งมนุษย์นั้นไม่มีความสามารถที่จะเปลี่ยนแปลงระเบียบเหล่านี้ได้ (ฮิตเลอร์คิดว่าตัวเองต้องจำใจเป็นผู้สังหารชาวยิวให้พ้นโลกจากความเชื่อที่ว่าชาวยิวเป็นกลุ่มมนุษย์ที่มียีนส์ชั้นต่ำและจำเป็นต้องถูกกำจัดเพื่อรักษามนุษยชาติตามกฎแห่งธรรมชาติที่ชาวนาซีเชื่อถือในช่วงเวลานั้น) โดยเป้าหมายสูงสุดของศาสนานั้นคือการสร้าง “ระเบียบ” ให้กับสังคมมนุษย์ ซึ่งผู้ที่นับถือศาสนาหรือเชื่อมั่นในระบบกฎเกณฑ์นั้นจะมีความเชื่อว่าหลักการที่พวกเขาเชื่อมั่นนั้นคือ “สิ่งเดียวที่ถูกต้อง” อันหมายความว่าความเชื่อของผู้ที่นับถือศาสนาอื่นนั้นเป็นเพียงเรื่องเพ้อฝันที่ไร้เหตุผลสิ้นดี
ปัญหาของความขัดแย้งระหว่างศาสนาและวิทยาศาสตร์นั้นจึงเกิดขึ้นจาก “ความขัดแย้งกันของความจริง” ซึ่งวิทยาศาสตร์กำลังมีบทบาทในการปฏิเสธความจริงของศาสนาและความเชื่อต่างๆด้วยหลักฐานที่ชัดเจน แต่อย่างไรก็ตาม สังคมมนุษย์นั้นไม่สามารถพึงพาแต่หลักการทางวิทยาศาสตร์ในการปกครองมนุษย์จำนวนมหาศาลให้อยู่ร่วมกันอย่างสงบสุขได้ สังคมยังต้องการ “หลักการทางจริยธรรม” จากศาสนาที่คอยกำหนดว่าอะไรคือสิ่งที่ถูกต้องและอะไรคือสิ่งที่ผิด (ซึ่งแน่นอนว่าปัญหาที่ตามมาก็คือความขัดแย้งทางความเชื่อระหว่างศาสนาที่ไม่ตรงกัน อาทิ ความเชื่อเรื่องการทำแท้งที่ศาสนาคริสต์มองว่าเป็นเรื่องที่ผิดแต่ชาวเสรีนิยมกลับมองว่าเป็นสิ่งที่มนุษย์สามารถกระทำได้ ทั้งนี้หลักการทางวิทยาศาสตร์นั้นสามารถตอบได้เพียงว่าทารกเริ่มได้รับความรู้สึกเจ็บปวดเมื่ออายุเท่าไหร่ แต่ไม่สามารถบอกได้ว่าการคร่าชีวิตทารกในครรภ์นั้นคือสิ่งที่ถูกต้องหรือไม่)
ศาสนาและวิทยาศาสตร์ในอดีตจึงมีความสัมพันธุ์ที่ต้องพึ่งพาอาศัยกันมาโดยตลอด
Chapter 6: The Modern Covenant “ความทันสมัย (modernist)” นั้นเกิดขึ้นจากการที่มนุษย์ยอมละทิ้ง “ความหมายของชีวิต” จากความเชื่อทางศาสนาที่คอยสร้างกรอบให้มนุษย์ทำตามคำสั่งของพระเจ้าหรือกฎของธรรมชาติไปเป็นการออกตามหา “พลังอำนาจ” อันไร้ซึ่งขอบเขตซึ่งได้รับการสนับสนุนโดยความก้าวหน้าทาง “วิทยาศาสตร์” และอัตราการ “เติบโต” ของระบบเศรษฐกิจสมัยใหม่อย่างก้าวกระโดด
การปฏิวัติทางวิทยาศาสตร์นั้นมีจุดกำเนิดจากการเปลี่ยนแปลงความคิดของมนุษย์ที่แต่เดิมเชื่อมั่นว่าตัวเองค้นพบทรัพยากรทั้งหมดของโลกแล้วซึ่งหมายความว่าการที่มนุษย์จะมีฐานะที่ดีขึ้นได้นั้นจะต้องแลกเปลี่ยนด้วยการถดถอยลงของมนุษย์อีกคน (zero-sum game) กลายมาเป็นความเชื่อที่ว่ามนุษย์สามารถสร้างอัตราการเติบโตของทรัพยากรและพลังงานที่มีอยู่อย่างจำกัดบนโลกได้ด้วยการใช้ “ความรู้” ในการพัฒนาวิทยาศาสตร์และเทคโนโลยีที่สามารถเพิ่มประสิทธิภาพของมนุษยชาติได้อย่างที่ไม่เคยเกิดขึ้นมาก่อน
มนุษย์ในปัจจุบันจึงกลายเป็นสิ่งมีชีวิตที่เสพติด “อัตราการเติบโต” ของเศรษฐกิจที่คอยช่วยให้ความเป็นอยู่ของพวกเราดีขึ้นไปเรื่อยๆ ซึ่งแน่นอนว่าปัญหาที่ตามมาอย่าง “มลพิษ” และ “ภาวะโลกร้อน” นั้นก็จะส่งผลที่รุนแรงมากขึ้นเรื่อยๆ (หลักฐานที่แสดงให้เห็นว่ามนุษย์สนใจการเติบโตมากกว่าสิ่งแวดล้อมที่ชัดเจนที่สุดคืออัตราการปล่อยก๊าซคาร์บอนไดออกไซด์ที่มีแต่จะเพิ่มขึ้นเรื่อยๆถึงแม้ว่าจะมีการทำสัญญาระหว่างประเทศกันหลายรอบแล้วก็ตาม)
Chapter 7: The Humanist Revolution เหตุใดสังคมของมนุษย์สมัยใหม่ในยุคที่ปราศจากความเชื่อทางศาสนาถึงยังคงอยู่ร่วมกันได้อย่างสงบสุข คำตอบก็คือ มนุษย์ได้คิดค้นศาสนาชนิดใหม่ที่มีชื่อว่า “มนุษยนิยม (humanism)” ขึ้นมาในช่วงไม่กี่ทศวรรษที่ผ่านมา
มนุษยนิยมเชื่อมั่นในพลังของ “มนุษย์” และยอมรับให้มนุษย์ทำหน้าที่แทนพระเจ้าหรือกฏแห่งธรรมชาติในการสร้าง “ความหมาย” ให้กับโลกและจักรวาล อันเป็นเหตุให้การตัดสินใจทั้งหมดของมนุษย์นั้นเกิดขึ้นจากการถาม “ความรู้สึก” ของตัวเองว่าสิ่งนั้นเป็นสิ่งที่ควรกระทำหรือไม่โดยไม่ต้องยึดถือคัมภีร์ไบเบิ้ลหรือกฎข้อบังคับของศาสนาอื่นๆอีกต่อไป (มนุษย์เพียงแค่ถามตัวเองลึกๆว่าการกระทำนั้นทำให้เราและผู้อื่นรู้สึกดีหรือไม่ ธุรกิจก็แค่สร้างสินค้าหรือบริการที่เป็นที่ต้องการของลูกค้าเท่านั้นเพราะหลักการทางเศรษฐกิจของมนุษยนิยมก็คือ “ลูกค้าหรือมนุษย์นั้นถูกต้องเสมอ” อันเป็นเหตุให้ธุรกิจสีเทาหรือธุรกิจที่ส่งผลกระทบต่อสิ่งแวดล้อมยังคงดำเนินกิจการได้ในปัจจุบันเพราะมนุษย์ผู้เป็นลูกค้าไม่ได้มองว่าธุรกิจเหล่านั้นเป็นสิ่งที่ผิด !!)
สมการที่อธิบายกลไกของมนุษยนิยมนั้นได้แก่ Knowledge = Experience x Sensitivity ซึ่งมีความหมายว่า “กระบวนการทางความคิด” ของมนุษย์นั้นได้รับอิทธิพลจากการทำงานร่วมกันของ “ประสบการณ์” ที่ประกอบไปด้วย สัมผัส อารมณ์และความคิดที่ถูกสั่งสมมาในมนุษย์แต่ละคนและ “ความสามารถในการประมวลผล” ของประสบการณ์เหล่านั้น ซึ่งหมายความว่ามนุษย์มีการ “พัฒนาการทางความคิด” อยู่ตลอดเวลา ยกตัวอย่างเช่น ผู้เชี่ยวชาญการดื่มชาที่ผ่านประสบการณ์การชิมชามาแล้วทั่วโลกจะมีความสามารถในการรับรู้คุณค่าของชาชั้นดีได้มากกว่าผู้ที่ไร้ประสบการณ์ (สมการการสร้างความรู้ตามหลักการทางวิทยาศาสตร์ Knowledge = Empirical Data x Mathematics นั้นไม่สามารถตอบปัญหาทางจริยธรรมได้ ส่วนสมการของศาสนา Knowledge = Scriptures x Logic ก็ถูกจำกัดด้วยหลักคำสอนของศาสนา)
มนุษยนิยมนั้นแบ่งออกได้เป็น 3 สายหลักๆ ได้แก่
 เสรีนิยม (Liberalism) ผู้เชื่อมั่นในความสามารถของมนุษย์ “แต่ละคน” ในการดำเนินชีวิตของตัวเอง ดังนั้น เสรีภาพในการตัดสินใจของมนุษย์จึงเป็นสิ่งสำคัญ (ข้อเสียของระบบเสรีนิยมนั้นก็คือ “ความไร้ประสิทธิภาพ” ที่เกิดขึ้นจากการใช้เสียงส่วนใหญ่ตามระบอบประชาธิปไตยเป็นตัวตัดสินใจแทนสมาชิกของสังคมซึ่งบางส่วนอาจไม่พอใจกับการตัดสินใจนั้นๆ) สังคมนิยม (Socialism) ผู้เชื่อมั่นในความสามารถของ “สังคมมนุษย์” ในภาพรวมโดยมีกลุ่มแกนนำเป็นผู้ดูแลการตัดสินใจแทนสมาชิกทุกคน โดยอ้างถึงความชอบธรรมในการแก้ปัญหาความไม่เท่าเทียมกันของระบบเสรีนิยม (ซึ่งต่อมาประเทศเสรีนิยมก็ได้นำเอาหลักการบางส่วนของสังคมนิยมมาปรับใช้ อาทิ การสนับสนุนทางการศึกษาและสาธารณสุขของประชาชน) มนุษยนิยมเชิงวิวัฒนาการ (Evolutionary humanism) ผู้เชื่อมั่นในความสามารถของชาติพันธุ์ของตัวเองว่ามีคุณสมบัติที่สูงส่งกว่าชาติพันธุ์อื่นๆ อันเป็นเหตุให้เกิดการ “กำจัด” มนุษย์สายพันธุ์ที่ด้อยกว่า อาทิ Nazism ที่เชื่อมั่นในความสามารถของชาติพันธ์ุอารยันที่สูงส่งกว่าชาวยิว  หลังจากการสิ้นสุดลงของสงครามเย็น มนุษยนิยมสายเสรีนิยมคือ “ศาสนา” ที่ประสบความสำเร็จสูงสุดในสังคมของมนุษย์ในปัจจุบัน อันเป็นผลมาจากความสามารถในการปรับตัวของหลักการให้เข้ากับการเปลี่ยนแปลงทางเทคโนโลยีในยุคปัจจุบัน (กลุ่มอิสลามหัวรุนแรงกำลังจะไม่มีจุดยืนในเร็วๆนี้เนื่องจากการไม่ยอมปรับตัวเข้ากับเทคโนโลยีสมัยใหม่) ซึ่งหมายความว่าเมื่อเทคโนโลยีของมนุษย์มีการพัฒนาการไปเรื่อยๆ ศาสนาแห่งใหม่ที่สามารถตอบสนองความคิดและอารมณ์ของมนุษย์ในยุคแห่งอนาคตก็อาจจะเข้ามาแทนที่มนุษยนิยมก็เป็นได้
Part III – Homo Sapiens Loses Control “Organisms are algorithms and life is data processing”
Chapter 8: The Time Bomb in the Laboratory ในยุคปัจจุบันที่วิทยาศาสตร์เข้ามามีบทบาทในสังคมมากขึ้นเรื่อยๆ “ข้อเท็จจริง” ตามหลักการของเสรีนิยมกำลังที่ว่าด้วยการให้ความสำคัญของ “อิสรภาพทางความคิดของมนุษย์” กำลังได้รับการทดสอบครั้งใหญ่
วิทยาศาสตร์พิสูจน์ให้เห็นแล้วว่ากระบวนการตัดสินใจของมนุษย์นั้นเกิดขึ้นจาก “ปฏิกิริยาเคมี” ภายในร่างกาย ซึ่งถึงแม้ว่ามนุษย์จะอ้างว่าพวกเรามีอิสรภาพในการตัดสินใจด้วยตัวเอง แต่พวกเราก็ได้ถูกกระบวนการทางเคมีตั้ง “กรอบ” ให้กับคำถามและความคิดของพวกเราไม่ต่างกับกระรอกที่สามารถตัดสินใจด้วยตัวเองได้ว่ามันจะเลือกกินวอลนัทที่หล่นอยู่ใต้ต้นไม้หรือไม่โดยไม่เคยต้องถามตัวเองเลยว่าทำไมต้องคิดถึง “วอลนัท” (มนุษย์สามารถตัดสินใจคำตอบของคำถามในหัวของตัวเองได้ แต่อะไรหละคือตัวกำหนดให้พวกเรา “ถาม” คำถามเหล่านั้น) เราไม่ได้เลือกความต้องการของตัวเอง สิ่งที่เกิดขึ้นจริงในระบบประสาทก็คือ เรา “รับรู้” ถึงความต้องการเหล่านั้นที่ไหลผ่านมาในสมองเรา ณ จังหวะเวลานั้นพอดีต่างหาก (เราสามารถทดสอบทฤษฎีนี้ง่ายๆด้วยการตั้งคำถามตัวเองหลังจากที่ “ความคิด” บางอย่างผุดขึ้นมาในใจว่าความคิดเหล่านั้นมันเกิดขึ้นมาได้ยังไงและเราสามารถสั่งสมองให้ “หยุดคิด” ได้หรือไม่)
สิ่งที่ตามมาก็คือ กระบวนการ “ปรับแต่งความรู้สึก” ของมนุษย์ผ่านเทคโนโลยีสมัยใหม่ ที่ปัจจุบันนักวิทยาศาสตร์สามารถสั่งการ “หนูทดลอง” ให้ปฏิบัติภารกิจตามคำสั่งผ่านการฝังเครื่องมือที่คอยกระตุ้นสมองส่วนที่สร้างความสุขให้กับหนูควบคู่กับการฝึกระยะสั้นโดยหนูทดลองที่ร่วมโครงการนั้นจะมีความรู้สึกดีทุกครั้งหลังจากได้กระทำสิ่งที่พวกมันถูกสั่งให้ทำผ่านการกระตุ้นสมอง ซึ่งเทคโนโลยีสำหรับมนุษย์ก็เริ่มได้รับการพัฒนาให้มีประสิทธิภาพมากขึ้นแล้ว อาทิ ไมโครชิปส์สำหรับแก้โรคซึมเศร้าและหมวกสัญญาณแม่เหล็กไฟฟ้าสำหรับกระตุ้นสมองของทหารสหรัฐเพื่อเพิ่มสมรรถภาพในสนามรบ
หลักการของเสรีนิยมที่พูดถึง “ตัวตน” ที่มีเพียงหนึ่งเดียวของมนุษย์แต่ละคน (individual) นั้นก็ได้รับการโต้แย้งอย่างรุนแรงจากทฤษฎีของศาสตร์เศรษฐศาสตร์เชิงพฤติกรรม (behavioral economics) ที่พิสูจน์ว่ามนุษย์หนึ่งคนนั้นมีระบบการตัดสินใจอยู่ 2 ระบบ อันได้แก่ ตัวตนเชิงประสบการณ์ (experience self) ผู้รับรู้ถึงสิ่งที่มนุษย์ได้ประสบพบเจอและตัวตนเชิงบอกเล่า (narrative self) ผู้ทำหน้าที่บอกเล่าเรื่องราวเหล่านั้น โดยการทดลองได้ค้นพบว่าตัวตนเชิงบอกเล่านั้นมักสร้างเรื่องราวที่ไม่ได้สอดรับกับตัวตนเชิงประสบการณ์และตัวตนเชิงบอกเล่านี้เองคือตัวตนหลักที่ทำหน้าที่ชี้นำการตัดสินใจของมนุษย์ (ตัวอย่างการทดลอง: การทดลองจุ่มมือในน้ำเย็นจัดที่ผู้ทดลองจะต้องทำการจุ่มมือใน 2 รูปแบบ รูปแบบแรกผู้ทดลองจะต้องจุ่มมือในน้ำเย็นจัดเป็นระยะเวลาหนึ่ง รูปแบบที่สองนั้นเหมือนกับรูปแบบแรกแต่ผู้ทดลองจะต้องจุ่มมือนานขึ้นอีกในขณะที่น้ำจะอุ่นขึ้นเล็กน้อย ผลของการทดลองพบว่าผู้ทดลองที่ได้รับอิทธิพลจากตัวตนเชิงบอกเล่าชอบการจุ่มมือรูปแบบที่สองมากกว่าทั้งๆที่ตัวตนเชิงประสบการณ์ของพวกเขาต้องทนทุกข์เป็นระยะเวลานานกว่า ผลลัพธ์ของการทดลองนี้นำไปสู่กฎ peak-end rule ที่กล่าวว่ามนุษย์มักจะทำการเฉลี่ยความทรมานหรือความสุขของประสบการณ์ที่พวกเขาพบเจอมากกว่าการรวมผลลัพธ์จากเหตุการณ์เหล่านั้น)
Chapter 9: The Great Decoupling ในศตวรรษที่ 21 มนุษยชาติกำลังจะต้องประสบกับการคุกคามของ “เทคโนโลยี” ที่กำลังจะทำให้มนุษย์จำนวนมากต้องสูญเสีย “คุณค่า” ทางเศรษฐกิจและการทหารไปอย่างหมดสิ้นอันเป็นเสมือน “ชนวนระเบิด” ที่พร้อมจะทำลายความเชื่อของหลักการทางเสรีนิยมที่ให้คุณค่าแก่มนุษย์ทุกๆคนอย่างเท่าเทียมกัน ตัวอย่างเช่น IBM’s Watson ในอนาคตที่สามารถทำหน้าที่แทนคุณหมอทั่วโลกในการวินิจฉัยโรคร้ายของผู้ป่วยพร้อมๆกันทั้งโลกได้อย่างแม่นยำตลอด 24 ชั่วโมง การจู่โจมทางโลกไซเบอร์โดยกลุ่มแฮกเกอร์เพียงหยิบมือสามารถส่งผลร้ายแรงต่อเป้าหมายได้อย่างรวดเร็วและมีประสิทธิภาพกว่าการใช้ทหารนับล้านนายในการสู้รบ
ในอดีตมนุษย์มีความเชื่อว่าสิ่งมีชีวิตที่มีความสามารถในการตระหนักรู้ถึงอารมณ์และความรู้สึกของตัวเองเท่านั้นที่จะสามารถสร้างภูมิปัญญาที่สูงส่งได้ แต่แล้ว เทคโนโลยีทางคอมพิวเตอร์และหุ่นยนต์ในปัจจุบันนั้นได้แสดงให้เห็นถึง “การแยกออกจากกัน” ของ “ปัญญา (Intelligence)” และ “การตระหนักรู้ (consciousness)” ที่ในอนาคต algorithm ที่มีระดับสติปัญญาขั้นสูงจะเข้ามาทำหน้าที่แทนมนุษย์ผู้มีความตระหนักรู้แต่มีระบบประมวลผลที่ด้อยประสิทธิภาพกว่าโดยที่ algorithm เหล่านั้นไม่จำเป็นต้องมีความรู้สึกนึกคิดแต่อย่างใด ไม่แตกต่างจากเหตุการณ์ในอดีตที่ “ม้า” ซึ่งมีความรู้สึกและความผูกพันธุ์กับเจ้าของถูกแทนที่ด้วย “รถยนต์” อันไร้ความรู้สึกที่มีประสิทธิภาพในการพามนุษย์เดินทางจากจุดหนึ่งไปยังอีกจุดหนึ่งที่ดีกว่าได้ภายในระยะเวลาอันรวดเร็ว
คำถามสำคัญที่ตามมาก็คือ “แล้วที่ยืนของมนุษย์นั้นอยู่ตรงไหน” คำตอบของนักวิชาการส่วนใหญ่ในยุคปัจจุบันก็คือ งานที่ต้องใช้ความสามารถในการตระหนักรู้ถึงอารมณ์และความคิดสร้างสรรค์ของมนุษย์ แต่เอาเข้าจริงๆแล้ว สิ่งมีชีวิตทุกชนิดรวมถึงมนุษย์นั้นล้วนมี algorithm สำหรับการตัดสินใจทั้งนั้น งานที่ต้องใช้ความคิดสร้างสรรค์อย่างการแต่งเพลงนั้นตามหลักการของ Life science นั้นถือเป็นกระบวนการในการประมวลผลรูปแบบของเสียงตามหลักการทางคณิตศาสตร์ของมนุษย์เท่านั้น ซึ่งนักประดิษฐ์คนหนึ่งสามารถพัฒนา A.I. นามว่า EMI ที่สามารถประมวลผลเพลงของศิลปินชื่อดังอย่าง Bach มาใช้ในการแต่งเพลงใหม่ของตัวเองกว่า 5,000 เพลงภายใน 1 วันโดยที่ผู้ฟังนั้นเชื่อว่าเพลงเหล่านี้ถูกแต่งขึ้นมาโดยศิลปินเอกได้อย่างสนิทใจ โลกในอนาคตกำลังจะสร้างประชากรกลุ่มใหม่ที่ไร้ซึ่งคุณค่าใดๆต่อสังคม (useless class) ส่วนกลุ่มงานที่อาจได้รับผลกระทบน้อยที่สุดนั้นคือกลุ่มงานที่ต้องใช้ความสามารถเฉพาะตัวขั้นสูงแต่มีผลตอบแทนที่ต่ำจนไม่มีใครอยากลงทุนพัฒนาโปรแกรมมาทำหน้าที่ทดแทน อาทิ นักโบราณคดี
แนวโน้มถัดมาของศตวรรษที่ 21 ก็คือ การเกิดขึ้นของระบบการตัดสินใจที่หลอมรวมมนุษย์เข้าด้วยกันเป็นกลุ่ม ศาสตร์ Life science ได้พิสูจน์ว่าทุกกระบวนการตัดสินใจของมนุษย์นั้นเกิดขึ้นจาก algorithm ภายในของมนุษย์แต่ละคนที่เต็มไปด้วยข้อบกพร่องนานับประการ และเมื่อถึงจุดหนึ่งที่เทคโนโลยีสามารถทำความเข้าใจหลักการของ algorithm เหล่านั้นมากพอที่จะสร้าง external algorithm ที่มีประสิทธิภาพมากกว่าได้สำเร็จ ความสำคัญของปัจเจกบุคคลก็จะสูญสลายไป มนุษย์จะถูกหลอมรวมเป็นกลุ่มก้อนและถูกถ่ายเทพลังอำนาจไปยัง algorithm ที่สามารถตัดสินใจสิ่งต่างๆได้ดีกว่า ตัวอย่างที่เห็นชัดๆในปัจจุบันก็คือ เทคโนโลยีทางการแพทย์อย่าง wearable sensor ที่สามารถตรวจจับกระบวนการทำงานของร่างกายและให้คำแนะนำแก่ผู้ใช้งานอยู่ตลอดเวลา หรือ เหตุการณ์ที่ Angelina Jolie ตัดสินใจผ่าตัดเต้านมหลังรับทราบข้อมูลทางสถิติจากการตรวจสอบทางพันธุกรรมว่าตัวของเธอมีโอกาส 87% ที่จะเป็นมะเร็งเต้านม
ในอนาคต “มนุษย์” อาจจะต้องพึ่งพิง “โปรแกรม” อย่างแยกจากกันไม่ได้ตลอด 24 ชั่วโมง ลองจินตนาการโลกในอนาคตที่มนุษย์ทุกคนยอมให้ข้อมูลทั้งหมดแก่ Google ไม่ว่าจะเป็นข้อมูลทางชีววิทยา (อาทิ ข้อมูลการทำงานของร่างกายแบบ real time ผ่านการฝังไมโครเซนเซอร์ DNA และประวัติทางพันธุกรรม) ข้อมูลการใช้จ่าย อีเมล์และอีกมากมาย เมื่อข้อมูลมีมากพอถึงจุดหนึ่ง Google จะกลายมาเป็น algorithm ที่รู้จักมนุษย์มากกว่าตัวของพวกเขาเองและสามารถให้คำแนะนำในการตัดสินใจได้ดีกว่ามนุษย์ที่เต็มไปด้วยความลำเอียง (bias) และความไร้ประสิทธิภาพในการประมวลผลข้อมูลของตัวเอง
และเมื่อมนุษย์สามารถอัพเกรดร่างกายและสติปัญญาของตัวเองได้สำเร็จ โลกก็จะเริ่มเข้าสู่ยุคที่ความไม่เท่าเทียมกันระหว่างชนชั้นสูงกับชนชั้นล่างนั้นรุนแรงอย่างที่ไม่เคยเกิดขึ้นมาก่อน ความแตกต่างนั้นไม่ได้เกิดขึ้นเฉพาะฐานะทางเศรษฐกิจเท่านั้น แต่กลุ่มชนชั้นสูงนั้นจะกลายมาเป็นสิ่งมีชีวิตที่มีสมรรถภาพทางชีววิทยาที่เหนือกว่าและเมื่อเทคโนโลยีได้ทำให้ “ประโยชน์” ของกลุ่มชนชั้นล่างหมดไป ประชากรเกือบทั้งหมดของโลกมนุษย์อาจจะกลายมาเป็นเพียงขยะทางชีววิทยาที่ไร้คุณค่าใดๆในสายตาของกลุ่มมนุษย์ที่สามารถแปลงกายเป็นเทพเจ้าได้สำเร็จ
Lee Sedol vs. AlphaGo (ขอบคุณภาพจาก BGR India)
Chapter 10: The Ocean of Consciousness ศาสนาใหม่ของมนุษยชาติกำลังถือกำเนิดขึ้นในห้องทดลองทางวิทยาศาสตร์ที่มี Silicon Valley เป็นจุดศูนย์กลาง สิ่งที่สร้างความแตกต่างให้กับศาสนาแห่งใหม่นี้ก็คือความสามารถในการสร้างความสุข ความสงบหรือแม้กระทั่งชีวิตอันเป็นนิรันดร์ให้กับมนุษย์บนโลกใบนี้ (ไม่ใช่โลกหลังความตายเหมือนศาสนาอื่นๆ)
Techno-humanist คือ ศาสนาที่ต่อยอดจากมนุษยนิยมที่ยังคงมีความเชื่อมั่นในความสามารถของมนุษย์อยู่ แต่มนุษย์ตามความหมายของศาสนาใหม่นี้คือมนุษย์สายพันธุ์ Homo Deus ที่ได้รับการอัพเกรดจากเทคโนโลยีทางชีววิทยา นาโนและการเชื่อมต่อคอมพิวเตอร์เข้ากับระบบประสาท อันเป็นเหตุให้ Homo Deus มีความสามารถที่ยังคงสร้างคุณค่าในโลกที่เต็มไปด้วย algorithm ได้ (นักวิทยาศาสตร์เชื่อว่ามนุษย์สามารถขยายประสิทธิภาพของการใช้งานของสมองและจิตใจได้อีกมหาศาลโดยยกตัวอย่างศักยภาพที่มนุษย์ในปัจจุบันไม่สามารถทำได้ อาทิ ประสิทธิภาพในการดมกลิ่นของมนุษย์ในยุคล่าสัตว์ การอ่านคลื่นสะท้อนของค้างคาวและการตรวจจับเสียงที่อยู่ห่างออกไปหลายร้อยกิโลเมตรของปลาวาฬ)
แต่หลักการของ Techno-humanist ก็มีข้อจำกัด เมื่อมนุษย์สามารถอัพเกรดระบบการตัดสินใจของตัวเองจนสามารถกำจัดตัวตนที่ไร้เหตุผลหรือตัวตนที่ก่อให้เกิดความลำบากใจภายในจิตใจของพวกเขาได้ มนุษย์เวอร์ชั่นอัพเกรดเหล่านั้นก็จะมีระบบการตัดสินใจที่ไม่แตกต่างจาก algorithm อันเป็นจุดกำเนิดของศาสนาชนิดใหม่ที่มี “ข้อมูล” เป็นศูนย์กลาง
Chapter 11: The Data Religion Dataism (ข้อมูลนิยม) คือ ศาสนาที่เชื่อมั่นในประสิทธิภาพของ “ข้อมูล” อันเกิดขึ้นจากการผสมผสานความก้าวหน้าของศาสตร์ทางชีววิทยาและคอมพิวเตอร์ที่เชื่อมั่นว่า algorithm ของสิ่งมีชีวิตและคอมพิวเตอร์นั้นสามารถผสมผสานรวมกันได้
ประวัติศาสตร์ของศตวรรษที่ 20 แสดงให้เห็นอย่างชัดเจนว่าระบบการปกครองที่สามารถ “ประมวลผลข้อมูล (data processing)” ได้อย่างครอบคลุมมากกว่านั้นคือระบบการปกครองที่มีประสิทธิภาพและพลังอำนาจที่สูงที่สุด (ระบอบคอมมิวนิสต์ที่มีรัฐบาลกลางทำหน้าที่ประมวลผลข้อมูลแต่เพียงผู้เดียวไม่สามารถสู้กับกำลังของหน่วยประมวลผลข้อมูลจำนวนมหาศาลของระบบกระจายอำนาจตามหลักเสรีนิยมได้) แต่ในยุคปัจจุบันที่การเปลี่ยนแปลงทางเทคโนโลยีนั้นเกิดขึ้นอย่างรวดเร็วพร้อมๆกับการเพิ่มขึ้นของข้อมูลจำนวนมหาศาล ระบบการปกครองทุกรูปแบบในปัจจุบันไม่สามารถติดตามการเปลี่ยนแปลงได้อีกต่อไปอันเป็นเหตุให้โลกต้องการระบบและผู้ปกครองรูปแบบใหม่ที่มีประสิทธิภาพที่ดีกว่า
ประวัติศาสตร์ยังแสดงให้เห็นถึงวิวัฒนาการของกระบวนการประมวลผลข้อมูลของมนุษยชาติ ที่เริ่มตั้งแต่ การขยายจำนวนประชากร (หน่วยประมวลผล) อันก่อให้เกิดความหลากหลายของหน่วยประมวลผลเหล่านั้นที่กระจัดกระจายอยู่ทั่วโลก จนกระทั่งโลกในยุคเกษตรกรรมและอุตสาหกรรมที่หน่วยประมวลผลที่มีความหลากหลายได้กลับมารวมตัวกันเป็นกลุ่มที่ใหญ่ขึ้นและเชื่อมต่อกันได้อย่างมีประสิทธิภาพมากขึ้นเรื่อยๆ ซึ่งแสดงให้เห็นถึงแนวโน้มของการเกิดขึ้นของการเชื่อมต่อทางข้อมูลอย่างสมบูรณ์ของมนุษยชาติและทุกสรรพสิ่ง (Internet-of-all-thing)
อิสรภาพของข้อมูล (freedom of information) คือ “หัวใจ” สำคัญของ Dataism ที่เชื่อมั่นว่า “เมื่อมนุษย์ยินยอมเปิดเผยข้อมูลทั้งหมดให้กับระบบ algorithm หนึ่งเดียวของโลก ทุกกระบวนการตัดสินใจของมนุษย์จะถูกประมวลผลและตัดสินใจผ่านระบบประมวลผลแห่งนั้นอันนำมาซึ่งผลลัพธ์ที่ดีที่สุดให้กับมนุษย์และทุกสรรพสิ่ง” ตัวอย่างของการใช้ระบบข้อมูลมหาศาลในการเพิ่มประสิทธิภาพให้กับโลกมนุษย์นั้นได้แก่ ระบบแบ่งปันรถยนต์ไร้คนขับ (driverless carpooling) ที่เข้ามาขจัดปัญหาของความสิ้นเปลืองของการใช้ทรัพยากร “รถยนต์” ที่ใช้เวลามากกว่า 90% ในการจอดอยู่กับที่เฉยๆอย่างไร้ประโยชน์ หากมนุษย์ทุกคนยอมเปิดเผยข้อมูลตำแหน่งที่อยู่อาศัย ที่ทำงาน จุดหมายปลายทางและเวลาให้กับระบบ algorithm อย่างสมบูรณ์ ระบบประมวลผลนี้ก็จะสามารถจัดสรรการใช้ทรัพยากรรถยนต์แบบแบ่งปันให้กับผู้ที่ต้องการเดินทางด้วยรถยนต์ได้อย่างมีประสิทธิภาพและยังสามารถลดปริมาณรถยนต์ส่วนบุคคลในท้องถนนได้ถึง 20 เท่าพร้อมกับการลดลงของพื้นที่จอดรถอีกจำนวนมหาศาล
Dataism (ขอบคุณภาพจาก Financial Times)
แต่ถึงกระนั้น ทฤษฎีที่กล่าวมาทั้งหมดในหนังสือเล่มนี้เป็นเพียงแค่การพยากรณ์ที่อาศัยการศึกษาทางประวัติศาสตร์ของมนุษยชาติและการทำความเข้าใจเทคโนโลยีในยุคปัจจุบันเท่านั้น มนุษย์ในทุกวันนี้คงไม่มีทางมองเห็นและเข้าใจมนุษย์ในอีก 50 ปีข้างหน้าได้อย่างสมบูรณ์ แต่ 3 แนวโน้มที่กำลังเกิดขึ้นจริงที่ทุกคนควรจะต้องคำนึงถึงอยู่เสมอในการวางแผนอนาคตนั้นก็คือ
 สิ่งมีชีวิตทั้งหมดนั้นคือระบบประมวลผลที่ถูกผลักดันโดย algorithm “สติปัญญา” กับ “การตระหนักรู้” นั้นกำลังถูกแยกออกจากกัน มนุษย์ผู้มีอารมณ์ความรู้สึกกำลังจะถูกแทนที่ด้วย algorithm ที่มีสติปัญญาที่สูงกว่า algorithm กำลังจะมีความสามารถในการเข้าใจมนุษย์มากกว่าตัวของพวกเราเอง   Source :.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_1/homo_deus/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_3/example_1_subtopic_3/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Subtopic 3 is very cool!
      </h3>
      <p class="refresh-summary">This is the real text of the article.
def sum_function(a, b): c = a &#43; b return c  </p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_3/example_1_subtopic_3/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/example_1_topic_1/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Topic 1 is very cool (again)!
      </h3>
      <p class="refresh-summary">This summary is multiline</p> 
      <div class="action has-text-right">
        <a href="/topic_1/example_1_topic_1/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/example_2_topic_2/"><img src="/topic_2/example_2_topic_2/summary_2_hu8dbdcdbe8d39d486b4229abea415a569_6024271_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Topic 2 is very cool!
      </h3>
      <p class="refresh-summary">The summary image should be a custom one</p> 
      <div class="action has-text-right">
        <a href="/topic_2/example_2_topic_2/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">custom_image</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/example_3_topic_1/"><img src="/topic_1/example_3_topic_1/summary_hu61d4afe1662869bae3e950774424d001_1030734_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Topic 1 is very cool!
      </h3>
      <p class="refresh-summary">The summary image should be a custom one</p> 
      <div class="action has-text-right">
        <a href="/topic_1/example_3_topic_1/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/example_2_topic_2/"><img src="/topic_2/example_2_topic_2/summary_2_hu8dbdcdbe8d39d486b4229abea415a569_6024271_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Topic 2 is very cool!
      </h3>
      <p class="refresh-summary">The summary image should be a custom one</p> 
      <div class="action has-text-right">
        <a href="/topic_2/example_2_topic_2/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">custom_summary</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/homo_deus/"><img src="/topic_1/homo_deus/images/Homo_Deus_hud949c3cfa973b39c16ba4d2c8cf1d6ae_196421_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        [สรุปหนังสือ] Homo Deus : A Brief History of Tomorrow
      </h3>
      <p class="refresh-summary"> “This is the best reason to learn history: not in order to predict the future, but to free yourself of the past and imagine alternative destinies. Of course this is not total freedom – we cannot avoid being shaped by the past. But some freedom is better than none.”
เมื่อเราเข้าใจ “เรื่องราวที่แท้จริง” ของมนุษย์ เมื่อนั้นเราก็จะเข้าใจ “เป้าหมาย” ของการเดินทางของมนุษยชาติ
Homo Deus คือ หนังสือภาคต่อจาก Sapiens ของ Yuval Noah Harari ศาสตราจารย์ทางประวัติศาสตร์ผู้นำเสนอเรื่องราวของ “อนาคต” ของมนุษยชาติผ่านการศึกษาความเป็นมาทางประวัติศาสตร์ของเผ่าพันธุ์ Homo Sapiens ที่พัฒนาตนเองขึ้นจากการเป็น “สัตว์” อันไร้ซึ่งความสำคัญใดๆมาเป็น “เทพเจ้า” ผู้กำหนดชะตาชีวิตของทุกสรรพสิ่ง แต่เมื่อสิ่งที่ถูกมนุษย์สร้างขึ้นอย่าง algorithm เริ่มมีสติปัญญาที่ชาญฉลาดกว่ามันสมองของมนุษย์ โลกของเราทุกคนจะเปลี่ยนแปลงไปอย่างไร
Chapter 1: The New Human Agenda โศกนาฏกรรม 3 อันดับสำคัญที่มนุษยชาติต้องพบเจอตลอดระยะเวลาหลายพันปีที่ผ่านมานั้นประกอบไปด้วย “ความอดอยาก” “โรคระบาด” และ “สงคราม”
Famine: ในยุคเกษตรกรรม หากภัยธรรมชาติส่งผลกระทบต่อผลิตผลทางการเกษตรของหมู่บ้านแห่งหนึ่ง ความทุกข์ทรมานจากความอดอยากนั้นเป็นสิ่งที่แทบจะหลีกเลี่ยงไม่ได้เลย แต่ทุกวันนี้ ผลิตภาพทางการเกษตรและความเป็นอยู่ที่ดีขึ้นของมนุษย์ได้ทำให้ภาวะอดอยากหายสาบสูญไปจากโลกจนเกือบจะหมดสิ้นและมนุษย์กลับมีโอกาสเสียชีวิตจากภาวะการรับประทานอาหารที่ “มากเกินไป” แทน
Plague: เริ่มต้นในช่วงทศวรรษที่ 1330 ภัยร้ายนามว่า the Black Death ได้คร่าชีวิตมนุษย์กว่า 1 ใน 4 ของประชากรทั้งหมดในแถบทวีปยุโรปและเอเชีย ในยุคเริ่มต้นของการล่าอาณานิคมของชาวสเปนในแถบทวีปอเมริกาได้นำพาเชื้อโรคที่ชาวท้องถิ่นไม่เคยได้สัมผัสข้ามทวีปมาแพร่ระบาดไปทั่วอาณาจักร Aztec และ Maya อย่างรวดเร็วจนทำให้กว่า 90% ของประชากรท้องถิ่นต้องจบชีวิตลงภายในระยะเวลาอันสั้น แต่ในปัจจุบัน ภาวะโรคระบาดนั้นสร้างความเสียหายในอัตราที่น้อยลงเป็นอย่างมากทั้งๆที่สภาวะการอาศัยอยู่รวมกันในเมืองที่แออัดและระบบการคมนาคมอันรวดเร็วนั้นถือเป็นแหล่งแพร่กระจายโรคระบาดได้เป็นอย่างดี ทั้งนี้ก็เพราะว่าวิทยาการทางการแพทย์สมัยใหม่ได้รับการพัฒนาจนสามารถตรวจจับสาเหตุและควบคุมการแพร่ระบาดของโรคติดต่อได้อย่างรวดเร็ว (Ebola ที่เกิดขึ้นในปี 2014 คร่าประชากรไปเพียงแค่หลักหมื่นคนเท่านั้น) ความเสี่ยงของโรคระบาดร้ายแรงในอนาคตน่าจะเกิดขึ้นจากฝีมืของมนุษย์มากกว่าจากธรรมชาติ
War: ปัจจุบันสงครามและการก่ออาชญากรรมนั้นเป็นเพียงแค่ 1% ของสาเหตุการตายของมนุษย์ (ซึ่งน้อยกว่าการฆ่าตัวตายและโรคเบาหวาน) ทั้งนี้ก็เพราะว่าสงครามในยุคปัจจุบันนั้นแทบจะ “ไม่มีประโยชน์” ในความคุ้มค่าทางเศรษฐกิจอีกต่อไป การเกิดขึ้นของอาวุธ “นิวเคลียร์” ส่งผลให้การสู้รบกันระหว่างชาติมหาอำนาจเป็นเหมือนการ “ฆ่าตัวตายหมู่” ที่ไร้เหตุผลสิ้นดี ชัยชนะจากสงครามนั้นก็แทบจะไม่มีมูลค่าอีกต่อไปเพราะทรัพย์สมบัติที่มีค่าที่สุดของมนุษย์ในยุคนี้อยู่ใน “คอมพิวเตอร์” และ “มันสมอง” ของมนุษย์ (สงครามที่เกิดขึ้นในช่วงที่ผ่านมานั้นมักเกิดขึ้นกับประเทศที่มีทรัพยากรมหาศาล อาทิ บ่อน้ำมันหรือเหมืองแร่) ในทางกลับกัน “ความสงบสุขอันยั่งยืน” ก็ได้เกิดขึ้นมาแทนที่ “ความสงบสุบชั่วคราว” ผ่านการทำการค้าและการลงทุนระหว่างประเทศที่ทำให้ความคิดถึงการทำสงครามระหว่างกันนั้น “เป็นไปไม่ได้” อีกต่อไป และนี่คือสาเหตที่ประเทศจีนเลือกทำสัญญาทางธุรกิจกับ Apple และ Microsoft แทนการยกทัพมายึด Silicon Valley
เมื่อ “ความท้าทาย” ในอดีตได้รับการแก้ไขจนเกือบสมบูรณ์แล้ว มนุษย์ผู้มีความทะเยอทะยานสูงก็ได้เริ่มออกตามหา “ความท้าทาย” ครั้งใหม่อันประกอบไปด้วย ความเป็นอมตะ ความสุขและการก้าวข้ามขีดจำกัดของมนุษย์ธรรมดาไปเป็น “พระเจ้า”
Immortality: “ความตาย” ถือเป็นส่วนสำคัญอย่างมากของ “ศาสนา” ที่คอยพร่ำสอนให้มนุษย์ทำความดีเพื่อสร้าง “ชีวิตหลังความตาย” ที่ยอดเยี่ยม (ถ้าไม่มีความตาย แนวคิดของสวรรค์และนรกคงไม่เคยเกิดขึ้น) แต่ปัจจุบัน “ความตาย” ที่เกิดขึ้นกับมนุษย์ผ่าน “โรคร้าย” ต่างๆนั้นถูกตีความใหม่ให้กลายมาเป็นเพียงแค่ “ปัญหาทางเทคนิค” ของร่างกายมนุษย์ที่รอให้เทคโนโลยีทางการแพทย์แก้ไขปัญหาให้หายขาด ปัจจุบัน นักวิทยาศาสตร์และบริษัทเทคโนโลยีชั้นนำเริ่มให้ความสำคัญกับ Life Science หรือศาสตร์แห่งชีวิตที่ศึกษาเรื่องการรักษาความผิดปกติของร่างกายและการยืดอายุไขของเซลส์และอวัยวะต่างๆของมนุษย์ ในเร็วๆนี้ เราอาจจะเริ่มเห็นมนุษย์ที่มีอายุไขเพิ่มขึ้นจากเดิม (ซึ่งนั่นก็ทำให้โครงสร้างทางสังคมเปลี่ยนแปลงไปจากปัจจุบันโดยสิ้นเชิง ลองคิดดูว่ามนุษย์อาจจะเกษียณในอายุ 120 ปี ปูตินอาจจะเป็นนายกของรัสเซียต่อไปอีก 60 ปีก็ได้) ส่วนในระยะยาว มนุษย์อาจเริ่มกลายเป็น “a-mortal” หรือ ผู้ที่มีร่างกายเป็นอมตะ (ยกเว้น ได้รับความเสียหายอย่างรุนแรงจนเสียชีวิต ซึ่งก็อาจจะทำให้บุคคลอมตะเหล่านี้เกรงกลัวต่อกิจกรรมที่มีความเสี่ยงอย่างสุดโต่ง) ซึ่งแน่นอนว่า “มนุษย์” ทุกคนจะต้องพยายามทุกวิถีทางให้ตัวเองกลายเป็นมีชีวิตอยู่ชั่วนิรันดร์และเงินทุนมหาศาลจะเป็นตัวกระตุ้นให้การตามล่าความเป็นอมตะดำเนินต่อไปจนสัมฤทธิ์ผล
Happiness: ตั้งแต่การเริ่มต้นของการสร้างอาณาจักรและประเทศชาติ การพัฒนาโครงสร้างพื้นฐาน การศึกษา สุขอนามัยและการยกระดับความเป็นอยู่อาศัยของประชาชนนั้นมีวัตถุประสงค์หลักคือการสร้าง “ความแข็งแกร่ง” ให้กับประเทศชาติโดยปราศจากการคำนึงถึง “ความสุข” ที่แท้จริงของประชาชน (ประเทศพัฒนาแล้วมากมายมีอัตราการฆ่าตัวตายสูงขึ้นตามการเพิ่มขึ้นของ GDP per capita) การสร้างความสุขที่แท้จริงของมนุษย์นั้นต้องเริ่มต้นจากการแก้ไข “อุปสรรค” สำคัญ 2 ข้อ ได้แก่ 1. อุปสรรคทาง “จิตวิทยา” ที่ว่าด้วยความสุขของมนุษย์นั้นเกิดขึ้นจากการที่ความจริงที่พวกเขาต้องเผชิญนั้นตรงกับความคาดหวังของพวกเขามากน้อยแค่ไหน (ซึ่งปัจจุบัน ความก้าวหน้าทางเทคโนโลยีได้นำพาให้ความคาดหวังของมนุษย์สูงขึ้นๆไปเรื่อยๆ) 2. อุปสรรคทาง “ชีววิทยา” ที่พิสูจน์ไว้แล้วว่าความสุขของมนุษย์ขึ้นอยู่กับ “ฮอร์โมน” ภายในร่างกายเท่านั้น อันเป็นไปตามการวิวัฒนาการของธรรมชาติที่สร้างให้มนุษย์มีความรู้สึกดีหลังจากได้กระทำสิ่งที่เป็นประโยชน์ต่อการดำรงชีวิตได้สำเร็จก่อนที่ความรู้สึกนั้นจะจางหายไปในเวลาต่อมา เทคโนโลยีในอนาคตที่จะเข้ามาเติมเต็มช่องว่างก้อนใหญ่ของมนุษย์ก็คือการสร้าง “ความสุขชั่วนิรันดร์” ผ่านการ “จัดระเบียบ” ระบบการทำงานของสารเคมีในร่างกายมนุษย์ให้สามารถหลั่งฮอร์โมนให้มนุษย์มีความสุขในเชิงบวกได้ตลอดเวลา (ปัจจุบันมีมนุษย์หลายล้านคนกินยาระงับประสาท ยาแก้โรคซึมเศร้าและยาเสพย์ติด ที่ล้วนมีผลต่อการปรับเปลี่ยนสารเคมีที่มีผลต่ออารมณ์ความรู้สึกของมนุษย์ทั้งนั้น)
Divinity: หลังจากที่มนุษย์สามารถออกแบบร่างกายและจิตใจของตัวเองได้แล้ว เป้าหมายที่เหลือของพวกเขาก็คงเป็นการอัพเกรด “พลัง” ให้กับตัวเองให้เทียบชั้นกับ “เทพเจ้า” ตั้งแต่ การเสริมสร้างความสามารถของร่างกายผ่านการปรับเปลี่ยนพันธุกรรม การผสมผสานหุ่นยนต์เข้ากับร่างกายและสมองของมนุษย์ ไปจนถึงการอัพโหลดจิตใจของมนุษย์เข้าไปยังเครื่องจักรที่ไม่มีวันตาย ทั้งหมดนี้อาจจะดูเพ้อฝัน แต่สิ่งหนึ่งที่สามารถมั่นใจได้เลยก็คือ มนุษย์ในปัจจุบันคงไม่สามารถจินตนาการถึง “มนุษย์อนาคต” ที่ก้าวผ่านจากสายพันธุ์ Homo Sapiens (มนุษย์ฉลาด) มาเป็น Homo Deus (มนุษย์เทพเจ้า) ได้อย่างแน่นอน
ประวัติศาสตร์เป็นข้อพิสูจน์ชั้นดีว่าการ “อัพเกรดมนุษย์” นั้นเป็นสิ่งที่ไม่อาจหลีกเลี่ยงได้ในอนาคต เทคโนโลยีในช่วงเริ่มต้นนั้นอาจเกิดขึ้นในรูปแบบของกระบวนการแก้ไขความผิดปกติต่างๆของมนุษย์ (อาทิ การเลือกเอ็มบรีโอของเด็กทารกที่มีสุขภาพสมบูรณ์ หรือ การแก้ไขดีเอ็นเอที่มีปัญหาของเอ็มบรีโอ) แต่หากกระบวนการเหล่านั้นสามารถทำให้มนุษย์ปกติมีคุณสมบัติที่เพิ่มขึ้น สุดท้ายโลกจะไม่อาจควบคุมการแพร่กระจายของเทคโนโลยีนี้ได้ (หากอเมริกาสั่งแบนการตัดต่อพันธุกรรม แต่เกาหลีเหนือสนับสนุนโครงการนี้เต็มที่เพื่อสร้างมนุษย์อัจฉริยะ อเมริกาคงไม่มีทางยอมปล่อยโอกาสนี้ไปแน่ๆ)
แต่ก็อย่าลืมเด็ดขาดว่า “อนาคต” ที่พวกเราพอจะมองเห็นอยู่ลิบๆนั้นเกิดขึ้นจากการร้อยเรียงเรื่องราวและความเชื่อของมนุษย์ที่ถูกสร้างขึ้นโดยมนุษย์ตั้งแต่ในอดีตจนถึงปัจจุบัน นั่นหมายความว่าโลกและมนุษยชาติอาจจะไม่ใช่สิ่งที่พวกเราจินตนาการถึงได้เลยเมื่อ “อนาคต” ที่แท้จริงเดินทางมาถึง
Part I – Homo Sapiens Conquers the World Chapter 2: The Anthropocene การพยากรณ์ความสัมพันธุ์ระหว่าง Homo Sapiens ยุคปัจจุบันกับ Homo Deus ในยุคอนาคตนั้นคงเป็นสิ่งที่ไม่ง่ายนัก แต่เราก็อาจพอจะใช้ประวัติศาสตร์ของความสัมพันธุ์ระหว่าง “มนุษย์” กับ “สัตว์” ชนิดอื่นมาใช้เป็นกรอบความคิดคร่าวๆได้
Antropocene คือ “ยุคแห่งมนุษย์” ที่เริ่มต้นขึ้นเมื่อ 70,000 ปีก่อนหลังจากการปฏิวัติทางความตระหนักรู้ของมนุษย์สายพันธุ์ Homo Sapiens ที่ทำให้ “มนุษย์” กลายเป็นสิ่งมีชีวิตสายพันธุ์เดียวในโลกที่กุมชะตากรรมของสิ่งมีชีวิตทั้งหมด (ปัจจุบัน สัดส่วนน้ำหนักรวมของสิ่งมีชีวิตเกิน 90% ตกเป็นของมนุษย์ สัตว์เลี้ยงและสัตว์อุตสาหกรรม)
ความสัมพันธุ์ของ Homo Sapiens และสิ่งมีชีวิตชนิดอื่นๆในยุคของนักล่าสัตว์และนักหาของป่านั้นอยู่ในรูปของความเชื่อที่ว่าสิ่งมีชีวิตและไม่มีชีวิตทั้งหมดล้วนมีจิตวิญญาณ (Animism) ที่มนุษย์จะต้องทำความเคารพและเกื้อกูลระหว่างกัน (การล่าสัตว์ในยุคโบราณนั้นต้องประกอบด้วยพิธีขอขมาสัตว์ที่ถูกล่านั้นๆ) ก่อนที่การปฏิวัติทางเกษตรกรรมจะเปลี่ยนพฤติกรรมและความเชื่อของมนุษย์ให้กลายเป็นการเคารพต่อ “เทพเจ้า” ผู้มีพลังอำนาจมหาศาลที่มาพร้อมกับการเปลี่ยนความคิดของการนับถือสัตว์กลายเป็นการมองว่าสิ่งมีชีวิตชนิดอื่นๆนั้นมีศักดิ์ที่ต้อยต่ำกว่ามนุษย์อันเป็นเหตุให้ปศุสัตว์ผู้ถูกเลือกอย่าง หมู วัว ไก่และแกะต้องกลายมาเป็นสัตว์ที่ต้องทนทุกข์ทรมานมากที่สุดในโลก (พร้อมกับการแพร่กระจายจำนวนอย่างรวดเร็ว) ซึ่งการปฏิวัติทางวิทยาศาสตร์ก็ยิ่งเข้ามาซ้ำเติมความโหดร้ายของมนุษย์ที่มีต่อสัตว์เหล่านั้นเข้าไปอีก (ยกเว้นในช่วงไม่กี่ทศวรรษที่ผ่านมาที่มนุษย์เริ่มหวนกลับมาให้ความสำคัญกับสิ่งมีชีวิตชนิดอื่น)
ในอนาคตนั้น เราไม่อาจรับรู้ได้เลยว่า “มนุษย์เวอร์ชั่นอัพเกรด” หรือ A.I. ผู้มีสติปัญญาที่หลักแหลมกว่า Homo Sapiens ในปัจจุบันมากจะจัดการกับพวกเราเหมือนในอดีตที่ผ่านมาหรือไม่
Antropocene หรือ “ยุคแห่งมนุษย์” (ขอบคุณภาพจาก Motherboard – Vice)
Chapter 3: The Human Spark “อะไร” คือสิ่งที่ทำให้มนุษย์มีความพิเศษมากกว่าสิ่งมีชีวิตชนิดอื่น
หากคำถามนี้ถูกถามในยุคที่วิทยาศาสตร์ยังไม่ได้รรับการยอมรับอย่างกว้างขวาง คำตอบคงเป็น “วิญญาณ” ที่มีเพียงมนุษย์เท่านั้นที่ได้ครอบครอง อันเป็นเหตุให้มนุษย์มองเห็นสิ่งมีชีวิตอื่นๆเป็นเพียงร่างที่ไร้วิญญาณและสามารถย่ำยีได้ตามชอบ (ปัจจุบันชาวอเมริกันเพียงแค่ 15% เท่านั้นที่เชื่อว่ามนุษย์เกิดขึ้นจากการวิวัฒนาการตามธรรมชาติเท่านั้นโดยไม่ต้องพึ่งพระเจ้า)
ส่วนคำตอบที่น่าจะได้รับการยอมรับในยุคปัจจุบันมากกว่าก็คือ “จิตใจ” (mind) และ “การตระหนักรู้” (consciousness) ซึ่งปัจจุบัน เทคโนโลยีทางวิทยาศาสตร์สามารถตรวจจับกระบวนการทำงานของระบบประสาทภายในสมองของสิ่งมีชีวิตได้อย่างมีประสิทธิภาพในระดับหนึ่งแต่ก็ยังไม่สามารถตรวจจับกระบวนการทำงานของ “จิตใจ” ที่ทำให้มนุษย์รู้สึกรัก โลภ โกรธหรือกลัวได้เลย (มีเพียงตัวของเราเองเท่านั้นที่เชื่อมั่นในการมีอยู่ของจิตใจและความรู้สึกของเรา ไม่แน่เราอาจจะเป็นเพียงผู้เล่นในโลกจำลองของสิ่งมีชีวิตชั้นสูงหรือมนุษย์ในอนาคตอยู่ก็เป็นได้)
นักวิทยาศาสตร์หลายคนพยายามจับเอาคอนเส็ปต์ของระบบคอมพิวเตอร์อย่าง algorithm ที่เป็นเหมือนชุดคำสั่งขนาดใหญ่มาใช้อธิบายถึงพฤติกรรมของสิ่งมีชีวิตต่างๆแทนการใช้จิตใจ อาทิ เมื่อ ลิงเห็นเสือ ประสาทตาของลิงก็จะทำการส่งกระแสไฟฟ้าเข้าไปยังสมองเพื่อประมวลผลผ่าน algorithm ของลิงตัวนั้นและส่งผลลัพธ์ออกมาในรูปของกระแสประสาทเพื่อให้ลิงวิ่งหนีเสือ โดยที่ลิงไม่ได้มีความรู้สึกกลัวหรือตกใจแต่อย่างใดเลย
แต่เอาจริงๆแล้ว การทดลองหลายครั้งในอดีตก็ได้พิสูจน์ว่าสิ่งมีชีวิตชนิดอื่นๆก็มีความตระหนักรู้ได้ไม่แพ้มนุษย์ อาทิ Clever Hans ม้าผู้แสนฉลาดแห่งเยอรมนีในยุคปี 1900s ที่มีความสามารถในการตอบปัญหาบวกลบคูณหารเลขผ่านการเคาะเท้าเป็นจำนวนครั้งตามคำตอบที่ถูกต้องได้อย่างแม่นยำ ซึ่งนักจิตวิทยาได้ค้นพบว่า Clever Hans ไม่ได้มีความสามารถในการคิดเลขเหมือนกับมนุษย์ แต่เจ้าม้าตัวนี้ใช้วิธีการเคาะเท้าพร้อมๆกับการสังเกตสีหน้าของมนุษย์ผู้เป็นคนถามคำถามที่มักจะแสดงอาการอย่างชัดเจนเมื่อ Clever Hans กระแทกเท้าจนใกล้ถึงคำตอบที่ถูกต้องและมันก็จะหยุดกระแทกเท้าไปในที่สุด
แต่จากการศึกษาประวัติศาสตร์ของมนุษยชาติ “ปัจจัย” ที่น่าจะส่งผลให้ Homo Sapiens กลายมาเป็นสิ่งมีชีวิตที่ยิ่งใหญ่ที่สุดในโลกได้ภายในเวลาเพียงแค่เสี้ยวหนึ่งของอายุของดาวดวงนี้ ไม่ใช่ “วิญญาณ” หรือ “จิตใจอันสูงส่ง” แต่กลับกลายเป็น “ความสามารถในการทำงานร่วมกันอย่างซับซ้อนและมีประสิทธิภาพ” อย่างที่สิ่งมีชีวิตชนิดอื่นสามารถทำได้ ซึ่งสาเหตุที่มนุษย์สามารถทำงานร่วมกันเป็นกลุ่มขนาดหลักพันหลักล้านคนได้นั้นเกิดจากการที่มนุษย์สามารถคิดค้น “ความเชื่อที่ถูกสร้างขึ้นโดยฝีมือมนุษย์” (imagined order) อาทิ ศาสนา พระเจ้า หลักมนุษยธรรม ประชาธิปไตยและระบบเงิน
มนุษย์คือสิ่งมีชีวิตเดียวในโลกที่สามารถจินตนาการ “ความหมาย” ของการมีชีวิตอยู่และการทำงานร่วมกันระหว่างมนุษย์ด้วยกันแองได้โดยไม่จำเป็นต้องพึ่ง “ความจริง” ที่เกิดขึ้นตามหลักการทางวิทยาศาสตร์และธรรมชาติ อาทิ นักรบชาวคริสเตียนเชื่อมั่นว่าตัวเองจะได้ขึ้นสวรรค์หากเข้าร่วมสงครามครูเสดเพื่อคร่าชีวิตนักรบชาวอิสลามที่มีความเชื่อคล้ายๆกัน
และเมื่อการเวลาเปลี่ยนไป ความเชื่อและความหมายของชีวิตก็เปลี่ยนแปลงไป ปัจจุบันความเชื่อเกี่ยวกับเทพเจ้าและศาสนากำลังเสื่อมความนิยม ขณะที่ความเชื่อเรื่องประชาธิปไตย ความเท่าเทียมกันและสิทธิมนุษยชนกำลังได้ความนิยมที่เพิ่มขึ้นเรื่อยๆ ส่วนในอนาคต ความเชื่อรูปแบบใหม่ที่มนุษย์ในยุคปัจจุบันอาจจะยังคาดไม่ถึงก็อาจจะเกิดขึ้นได้ในไม่ช้า
ม้าแสนรู้ Clever Hans ที่ไม่ได้ฉลาดเหมือนที่มนุษย์คิด (ขอบคุณภาพจาก Wikipedia)
Part II – Homo Sapiens Gives Meaning to the World Chapter 4: The Storytellers มนุษย์คือสิ่งมีชีวิตชนิดเดียวที่อาศัยอยู่ในโลกที่มีความจริงซ้อนกันอยู่ 3 ชั้น (three-layered reality) อันประกอบไปด้วย ความจริงเชิงวัตถุ (objective reality) อาทิ อากาศและสิ่งแวดล้อม ความจริงเชิงปัจเจกบุคคล (subjective reality) อันได้แก่ อารมณ์และความรู้สึกภายใน และความจริงที่ถูกสร้างขึ้นโดยมนุษย์ด้วยกันเอง (imagined reality)
ประวัติศาสตร์ของมนุษย์นั้นถูกสร้างขึ้นจากการร้อยเรียงกันของเรื่องเล่าและความเชื่อของมนุษย์ ตั้งแต่ 70,000 ปีก่อนที่มนุษย์เริ่มสามารถจินตนาการถึง “สิ่งที่ไม่มีอยู่จริง” อันเป็นจุดเริ่มต้นของการสร้าง “ความจริง” โดยฝีมือของจินตนาการของมนุษย์เพื่อสร้างความสามารถในการทำงานร่วมกันของมนุษย์หมู่มากได้สำเร็จ
โดยจุดเปลี่ยนครั้งสำคัญของมนุษยชาติ คือ การปฏิวัติทางเกษตรกรรมเมื่อ 12,000 ปีก่อนที่ทำให้มนุษย์เริ่มเปลี่ยนพฤติกรรมจากการออกล่าสัตว์และหาของป่าไปเป็นการอยู่รวมกันเป็นหลักแหล่งเป็นหมู่บ้านขนาดย่อม ก่อนที่ “อาณาจักร” ขนาดใหญ่จะเริ่มกำเนิดขึ้นหลังจากที่ชาวสุเมเรียนได้คิดค้น “ภาษาเขียน” อันเป็นพื้นฐานของระบบบัญชีและการปกครองของมวลมนุษย์ขนาดใหญ่ได้สำเร็จในช่วง 5,000 ปีก่อน และภาษาเขียนนี่เองที่เป็นตัวจุดประกายให้เกิดการสร้าง “เรื่องแต่ง” ให้กลายมาเป็น “เรื่องจริง” ที่ทำให้ประชาชนในแต่ละอาณาจักรเชื่อมั่นได้อย่างสนิทใจได้ ซึ่งในช่วงแรกเริ่มของอาณาจักรนั้น เรื่องจริงที่ถูกสร้างขึ้นโดยฝีมือมนุษย์นั้นมีลักษณะคล้ายๆกันก็คือ การนำเอา “เทพเจ้า” หรือ “พระเจ้า” มาเป็นจุดศูนย์กลางของความเชื่อและกฎระเบียบการปกครองของอาณาจักรโดยแต่ละอาณาจักรจะมีผู้นำที่เปรียบเสมือน “ตัวแทนของเทพเจ้า” หรือไม่ก็เป็น “เทพเจ้า” ซะเองเลยอย่างฟาโรห์ของอาณาจักรอียิปต์
ถึงแม้ในยุคปัจจุบัน ความเชื่อเรื่องเทพเจ้าและภูติผีปีศาจจะได้เสื่อมถอยลงไป แต่โลกของเรากลับเต็มไปด้วย “เรื่องแต่ง” โดยฝีมือมนุษย์ในยุคใหม่มากมาย ไม่ว่าจะเป็น ประเทศชาติ หลักการปกครอง เงินและบริษัท ที่ล้วนแล้วแต่จะมีอิทธิพลต่อมนุษย์มากขึ้นเรื่อยๆ ซึ่งเอาจริงๆ เรื่องแต่งเหล่านี้ล้วนมีความสำคัญอย่างมากในการอยู่ร่วมกันของมนุษย์ในยุคปัจจุบัน แต่พวกเราก็ไม่ควรลืมว่าอะไรคือเรื่องแต่งและอะไรคือ “เรื่องจริง”
The Creation of Adam (ขอบคุณภาพจาก Wikipedia)
Chapter 5: The Odd Couple สองหลักการที่ขัดแย้งกันมาตลอดในยุคสมัยใหม่ก็คือ “ศาสนา” กับ “วิทยาศาสตร์” ที่ต่างก็พยายามอธิบายถึง “ความจริง” หนึ่งเดียวของโลกมนุษย์
ศาสนาต่างๆ (รวมถึงลัทธิคอมมิวนิสต์ ทุนนิยมและความเชื่อเรื่องความเท่าเทียมกันของมนุษย์) นั้นล้วนมีพื้นฐานมาจาก “กฎเกณฑ์” ที่มนุษย์สร้างขึ้นโดยอาศัยการเรื่องเล่าที่ว่าอันแท้จริงแล้วกฎเกณฑ์เหล่านี้ถูกกำหนดโดยเทพเจ้าหรือเกิดขึ้นตามกฏของธรรมชาติซึ่งมนุษย์นั้นไม่มีความสามารถที่จะเปลี่ยนแปลงระเบียบเหล่านี้ได้ (ฮิตเลอร์คิดว่าตัวเองต้องจำใจเป็นผู้สังหารชาวยิวให้พ้นโลกจากความเชื่อที่ว่าชาวยิวเป็นกลุ่มมนุษย์ที่มียีนส์ชั้นต่ำและจำเป็นต้องถูกกำจัดเพื่อรักษามนุษยชาติตามกฎแห่งธรรมชาติที่ชาวนาซีเชื่อถือในช่วงเวลานั้น) โดยเป้าหมายสูงสุดของศาสนานั้นคือการสร้าง “ระเบียบ” ให้กับสังคมมนุษย์ ซึ่งผู้ที่นับถือศาสนาหรือเชื่อมั่นในระบบกฎเกณฑ์นั้นจะมีความเชื่อว่าหลักการที่พวกเขาเชื่อมั่นนั้นคือ “สิ่งเดียวที่ถูกต้อง” อันหมายความว่าความเชื่อของผู้ที่นับถือศาสนาอื่นนั้นเป็นเพียงเรื่องเพ้อฝันที่ไร้เหตุผลสิ้นดี
ปัญหาของความขัดแย้งระหว่างศาสนาและวิทยาศาสตร์นั้นจึงเกิดขึ้นจาก “ความขัดแย้งกันของความจริง” ซึ่งวิทยาศาสตร์กำลังมีบทบาทในการปฏิเสธความจริงของศาสนาและความเชื่อต่างๆด้วยหลักฐานที่ชัดเจน แต่อย่างไรก็ตาม สังคมมนุษย์นั้นไม่สามารถพึงพาแต่หลักการทางวิทยาศาสตร์ในการปกครองมนุษย์จำนวนมหาศาลให้อยู่ร่วมกันอย่างสงบสุขได้ สังคมยังต้องการ “หลักการทางจริยธรรม” จากศาสนาที่คอยกำหนดว่าอะไรคือสิ่งที่ถูกต้องและอะไรคือสิ่งที่ผิด (ซึ่งแน่นอนว่าปัญหาที่ตามมาก็คือความขัดแย้งทางความเชื่อระหว่างศาสนาที่ไม่ตรงกัน อาทิ ความเชื่อเรื่องการทำแท้งที่ศาสนาคริสต์มองว่าเป็นเรื่องที่ผิดแต่ชาวเสรีนิยมกลับมองว่าเป็นสิ่งที่มนุษย์สามารถกระทำได้ ทั้งนี้หลักการทางวิทยาศาสตร์นั้นสามารถตอบได้เพียงว่าทารกเริ่มได้รับความรู้สึกเจ็บปวดเมื่ออายุเท่าไหร่ แต่ไม่สามารถบอกได้ว่าการคร่าชีวิตทารกในครรภ์นั้นคือสิ่งที่ถูกต้องหรือไม่)
ศาสนาและวิทยาศาสตร์ในอดีตจึงมีความสัมพันธุ์ที่ต้องพึ่งพาอาศัยกันมาโดยตลอด
Chapter 6: The Modern Covenant “ความทันสมัย (modernist)” นั้นเกิดขึ้นจากการที่มนุษย์ยอมละทิ้ง “ความหมายของชีวิต” จากความเชื่อทางศาสนาที่คอยสร้างกรอบให้มนุษย์ทำตามคำสั่งของพระเจ้าหรือกฎของธรรมชาติไปเป็นการออกตามหา “พลังอำนาจ” อันไร้ซึ่งขอบเขตซึ่งได้รับการสนับสนุนโดยความก้าวหน้าทาง “วิทยาศาสตร์” และอัตราการ “เติบโต” ของระบบเศรษฐกิจสมัยใหม่อย่างก้าวกระโดด
การปฏิวัติทางวิทยาศาสตร์นั้นมีจุดกำเนิดจากการเปลี่ยนแปลงความคิดของมนุษย์ที่แต่เดิมเชื่อมั่นว่าตัวเองค้นพบทรัพยากรทั้งหมดของโลกแล้วซึ่งหมายความว่าการที่มนุษย์จะมีฐานะที่ดีขึ้นได้นั้นจะต้องแลกเปลี่ยนด้วยการถดถอยลงของมนุษย์อีกคน (zero-sum game) กลายมาเป็นความเชื่อที่ว่ามนุษย์สามารถสร้างอัตราการเติบโตของทรัพยากรและพลังงานที่มีอยู่อย่างจำกัดบนโลกได้ด้วยการใช้ “ความรู้” ในการพัฒนาวิทยาศาสตร์และเทคโนโลยีที่สามารถเพิ่มประสิทธิภาพของมนุษยชาติได้อย่างที่ไม่เคยเกิดขึ้นมาก่อน
มนุษย์ในปัจจุบันจึงกลายเป็นสิ่งมีชีวิตที่เสพติด “อัตราการเติบโต” ของเศรษฐกิจที่คอยช่วยให้ความเป็นอยู่ของพวกเราดีขึ้นไปเรื่อยๆ ซึ่งแน่นอนว่าปัญหาที่ตามมาอย่าง “มลพิษ” และ “ภาวะโลกร้อน” นั้นก็จะส่งผลที่รุนแรงมากขึ้นเรื่อยๆ (หลักฐานที่แสดงให้เห็นว่ามนุษย์สนใจการเติบโตมากกว่าสิ่งแวดล้อมที่ชัดเจนที่สุดคืออัตราการปล่อยก๊าซคาร์บอนไดออกไซด์ที่มีแต่จะเพิ่มขึ้นเรื่อยๆถึงแม้ว่าจะมีการทำสัญญาระหว่างประเทศกันหลายรอบแล้วก็ตาม)
Chapter 7: The Humanist Revolution เหตุใดสังคมของมนุษย์สมัยใหม่ในยุคที่ปราศจากความเชื่อทางศาสนาถึงยังคงอยู่ร่วมกันได้อย่างสงบสุข คำตอบก็คือ มนุษย์ได้คิดค้นศาสนาชนิดใหม่ที่มีชื่อว่า “มนุษยนิยม (humanism)” ขึ้นมาในช่วงไม่กี่ทศวรรษที่ผ่านมา
มนุษยนิยมเชื่อมั่นในพลังของ “มนุษย์” และยอมรับให้มนุษย์ทำหน้าที่แทนพระเจ้าหรือกฏแห่งธรรมชาติในการสร้าง “ความหมาย” ให้กับโลกและจักรวาล อันเป็นเหตุให้การตัดสินใจทั้งหมดของมนุษย์นั้นเกิดขึ้นจากการถาม “ความรู้สึก” ของตัวเองว่าสิ่งนั้นเป็นสิ่งที่ควรกระทำหรือไม่โดยไม่ต้องยึดถือคัมภีร์ไบเบิ้ลหรือกฎข้อบังคับของศาสนาอื่นๆอีกต่อไป (มนุษย์เพียงแค่ถามตัวเองลึกๆว่าการกระทำนั้นทำให้เราและผู้อื่นรู้สึกดีหรือไม่ ธุรกิจก็แค่สร้างสินค้าหรือบริการที่เป็นที่ต้องการของลูกค้าเท่านั้นเพราะหลักการทางเศรษฐกิจของมนุษยนิยมก็คือ “ลูกค้าหรือมนุษย์นั้นถูกต้องเสมอ” อันเป็นเหตุให้ธุรกิจสีเทาหรือธุรกิจที่ส่งผลกระทบต่อสิ่งแวดล้อมยังคงดำเนินกิจการได้ในปัจจุบันเพราะมนุษย์ผู้เป็นลูกค้าไม่ได้มองว่าธุรกิจเหล่านั้นเป็นสิ่งที่ผิด !!)
สมการที่อธิบายกลไกของมนุษยนิยมนั้นได้แก่ Knowledge = Experience x Sensitivity ซึ่งมีความหมายว่า “กระบวนการทางความคิด” ของมนุษย์นั้นได้รับอิทธิพลจากการทำงานร่วมกันของ “ประสบการณ์” ที่ประกอบไปด้วย สัมผัส อารมณ์และความคิดที่ถูกสั่งสมมาในมนุษย์แต่ละคนและ “ความสามารถในการประมวลผล” ของประสบการณ์เหล่านั้น ซึ่งหมายความว่ามนุษย์มีการ “พัฒนาการทางความคิด” อยู่ตลอดเวลา ยกตัวอย่างเช่น ผู้เชี่ยวชาญการดื่มชาที่ผ่านประสบการณ์การชิมชามาแล้วทั่วโลกจะมีความสามารถในการรับรู้คุณค่าของชาชั้นดีได้มากกว่าผู้ที่ไร้ประสบการณ์ (สมการการสร้างความรู้ตามหลักการทางวิทยาศาสตร์ Knowledge = Empirical Data x Mathematics นั้นไม่สามารถตอบปัญหาทางจริยธรรมได้ ส่วนสมการของศาสนา Knowledge = Scriptures x Logic ก็ถูกจำกัดด้วยหลักคำสอนของศาสนา)
มนุษยนิยมนั้นแบ่งออกได้เป็น 3 สายหลักๆ ได้แก่
 เสรีนิยม (Liberalism) ผู้เชื่อมั่นในความสามารถของมนุษย์ “แต่ละคน” ในการดำเนินชีวิตของตัวเอง ดังนั้น เสรีภาพในการตัดสินใจของมนุษย์จึงเป็นสิ่งสำคัญ (ข้อเสียของระบบเสรีนิยมนั้นก็คือ “ความไร้ประสิทธิภาพ” ที่เกิดขึ้นจากการใช้เสียงส่วนใหญ่ตามระบอบประชาธิปไตยเป็นตัวตัดสินใจแทนสมาชิกของสังคมซึ่งบางส่วนอาจไม่พอใจกับการตัดสินใจนั้นๆ) สังคมนิยม (Socialism) ผู้เชื่อมั่นในความสามารถของ “สังคมมนุษย์” ในภาพรวมโดยมีกลุ่มแกนนำเป็นผู้ดูแลการตัดสินใจแทนสมาชิกทุกคน โดยอ้างถึงความชอบธรรมในการแก้ปัญหาความไม่เท่าเทียมกันของระบบเสรีนิยม (ซึ่งต่อมาประเทศเสรีนิยมก็ได้นำเอาหลักการบางส่วนของสังคมนิยมมาปรับใช้ อาทิ การสนับสนุนทางการศึกษาและสาธารณสุขของประชาชน) มนุษยนิยมเชิงวิวัฒนาการ (Evolutionary humanism) ผู้เชื่อมั่นในความสามารถของชาติพันธุ์ของตัวเองว่ามีคุณสมบัติที่สูงส่งกว่าชาติพันธุ์อื่นๆ อันเป็นเหตุให้เกิดการ “กำจัด” มนุษย์สายพันธุ์ที่ด้อยกว่า อาทิ Nazism ที่เชื่อมั่นในความสามารถของชาติพันธ์ุอารยันที่สูงส่งกว่าชาวยิว  หลังจากการสิ้นสุดลงของสงครามเย็น มนุษยนิยมสายเสรีนิยมคือ “ศาสนา” ที่ประสบความสำเร็จสูงสุดในสังคมของมนุษย์ในปัจจุบัน อันเป็นผลมาจากความสามารถในการปรับตัวของหลักการให้เข้ากับการเปลี่ยนแปลงทางเทคโนโลยีในยุคปัจจุบัน (กลุ่มอิสลามหัวรุนแรงกำลังจะไม่มีจุดยืนในเร็วๆนี้เนื่องจากการไม่ยอมปรับตัวเข้ากับเทคโนโลยีสมัยใหม่) ซึ่งหมายความว่าเมื่อเทคโนโลยีของมนุษย์มีการพัฒนาการไปเรื่อยๆ ศาสนาแห่งใหม่ที่สามารถตอบสนองความคิดและอารมณ์ของมนุษย์ในยุคแห่งอนาคตก็อาจจะเข้ามาแทนที่มนุษยนิยมก็เป็นได้
Part III – Homo Sapiens Loses Control “Organisms are algorithms and life is data processing”
Chapter 8: The Time Bomb in the Laboratory ในยุคปัจจุบันที่วิทยาศาสตร์เข้ามามีบทบาทในสังคมมากขึ้นเรื่อยๆ “ข้อเท็จจริง” ตามหลักการของเสรีนิยมกำลังที่ว่าด้วยการให้ความสำคัญของ “อิสรภาพทางความคิดของมนุษย์” กำลังได้รับการทดสอบครั้งใหญ่
วิทยาศาสตร์พิสูจน์ให้เห็นแล้วว่ากระบวนการตัดสินใจของมนุษย์นั้นเกิดขึ้นจาก “ปฏิกิริยาเคมี” ภายในร่างกาย ซึ่งถึงแม้ว่ามนุษย์จะอ้างว่าพวกเรามีอิสรภาพในการตัดสินใจด้วยตัวเอง แต่พวกเราก็ได้ถูกกระบวนการทางเคมีตั้ง “กรอบ” ให้กับคำถามและความคิดของพวกเราไม่ต่างกับกระรอกที่สามารถตัดสินใจด้วยตัวเองได้ว่ามันจะเลือกกินวอลนัทที่หล่นอยู่ใต้ต้นไม้หรือไม่โดยไม่เคยต้องถามตัวเองเลยว่าทำไมต้องคิดถึง “วอลนัท” (มนุษย์สามารถตัดสินใจคำตอบของคำถามในหัวของตัวเองได้ แต่อะไรหละคือตัวกำหนดให้พวกเรา “ถาม” คำถามเหล่านั้น) เราไม่ได้เลือกความต้องการของตัวเอง สิ่งที่เกิดขึ้นจริงในระบบประสาทก็คือ เรา “รับรู้” ถึงความต้องการเหล่านั้นที่ไหลผ่านมาในสมองเรา ณ จังหวะเวลานั้นพอดีต่างหาก (เราสามารถทดสอบทฤษฎีนี้ง่ายๆด้วยการตั้งคำถามตัวเองหลังจากที่ “ความคิด” บางอย่างผุดขึ้นมาในใจว่าความคิดเหล่านั้นมันเกิดขึ้นมาได้ยังไงและเราสามารถสั่งสมองให้ “หยุดคิด” ได้หรือไม่)
สิ่งที่ตามมาก็คือ กระบวนการ “ปรับแต่งความรู้สึก” ของมนุษย์ผ่านเทคโนโลยีสมัยใหม่ ที่ปัจจุบันนักวิทยาศาสตร์สามารถสั่งการ “หนูทดลอง” ให้ปฏิบัติภารกิจตามคำสั่งผ่านการฝังเครื่องมือที่คอยกระตุ้นสมองส่วนที่สร้างความสุขให้กับหนูควบคู่กับการฝึกระยะสั้นโดยหนูทดลองที่ร่วมโครงการนั้นจะมีความรู้สึกดีทุกครั้งหลังจากได้กระทำสิ่งที่พวกมันถูกสั่งให้ทำผ่านการกระตุ้นสมอง ซึ่งเทคโนโลยีสำหรับมนุษย์ก็เริ่มได้รับการพัฒนาให้มีประสิทธิภาพมากขึ้นแล้ว อาทิ ไมโครชิปส์สำหรับแก้โรคซึมเศร้าและหมวกสัญญาณแม่เหล็กไฟฟ้าสำหรับกระตุ้นสมองของทหารสหรัฐเพื่อเพิ่มสมรรถภาพในสนามรบ
หลักการของเสรีนิยมที่พูดถึง “ตัวตน” ที่มีเพียงหนึ่งเดียวของมนุษย์แต่ละคน (individual) นั้นก็ได้รับการโต้แย้งอย่างรุนแรงจากทฤษฎีของศาสตร์เศรษฐศาสตร์เชิงพฤติกรรม (behavioral economics) ที่พิสูจน์ว่ามนุษย์หนึ่งคนนั้นมีระบบการตัดสินใจอยู่ 2 ระบบ อันได้แก่ ตัวตนเชิงประสบการณ์ (experience self) ผู้รับรู้ถึงสิ่งที่มนุษย์ได้ประสบพบเจอและตัวตนเชิงบอกเล่า (narrative self) ผู้ทำหน้าที่บอกเล่าเรื่องราวเหล่านั้น โดยการทดลองได้ค้นพบว่าตัวตนเชิงบอกเล่านั้นมักสร้างเรื่องราวที่ไม่ได้สอดรับกับตัวตนเชิงประสบการณ์และตัวตนเชิงบอกเล่านี้เองคือตัวตนหลักที่ทำหน้าที่ชี้นำการตัดสินใจของมนุษย์ (ตัวอย่างการทดลอง: การทดลองจุ่มมือในน้ำเย็นจัดที่ผู้ทดลองจะต้องทำการจุ่มมือใน 2 รูปแบบ รูปแบบแรกผู้ทดลองจะต้องจุ่มมือในน้ำเย็นจัดเป็นระยะเวลาหนึ่ง รูปแบบที่สองนั้นเหมือนกับรูปแบบแรกแต่ผู้ทดลองจะต้องจุ่มมือนานขึ้นอีกในขณะที่น้ำจะอุ่นขึ้นเล็กน้อย ผลของการทดลองพบว่าผู้ทดลองที่ได้รับอิทธิพลจากตัวตนเชิงบอกเล่าชอบการจุ่มมือรูปแบบที่สองมากกว่าทั้งๆที่ตัวตนเชิงประสบการณ์ของพวกเขาต้องทนทุกข์เป็นระยะเวลานานกว่า ผลลัพธ์ของการทดลองนี้นำไปสู่กฎ peak-end rule ที่กล่าวว่ามนุษย์มักจะทำการเฉลี่ยความทรมานหรือความสุขของประสบการณ์ที่พวกเขาพบเจอมากกว่าการรวมผลลัพธ์จากเหตุการณ์เหล่านั้น)
Chapter 9: The Great Decoupling ในศตวรรษที่ 21 มนุษยชาติกำลังจะต้องประสบกับการคุกคามของ “เทคโนโลยี” ที่กำลังจะทำให้มนุษย์จำนวนมากต้องสูญเสีย “คุณค่า” ทางเศรษฐกิจและการทหารไปอย่างหมดสิ้นอันเป็นเสมือน “ชนวนระเบิด” ที่พร้อมจะทำลายความเชื่อของหลักการทางเสรีนิยมที่ให้คุณค่าแก่มนุษย์ทุกๆคนอย่างเท่าเทียมกัน ตัวอย่างเช่น IBM’s Watson ในอนาคตที่สามารถทำหน้าที่แทนคุณหมอทั่วโลกในการวินิจฉัยโรคร้ายของผู้ป่วยพร้อมๆกันทั้งโลกได้อย่างแม่นยำตลอด 24 ชั่วโมง การจู่โจมทางโลกไซเบอร์โดยกลุ่มแฮกเกอร์เพียงหยิบมือสามารถส่งผลร้ายแรงต่อเป้าหมายได้อย่างรวดเร็วและมีประสิทธิภาพกว่าการใช้ทหารนับล้านนายในการสู้รบ
ในอดีตมนุษย์มีความเชื่อว่าสิ่งมีชีวิตที่มีความสามารถในการตระหนักรู้ถึงอารมณ์และความรู้สึกของตัวเองเท่านั้นที่จะสามารถสร้างภูมิปัญญาที่สูงส่งได้ แต่แล้ว เทคโนโลยีทางคอมพิวเตอร์และหุ่นยนต์ในปัจจุบันนั้นได้แสดงให้เห็นถึง “การแยกออกจากกัน” ของ “ปัญญา (Intelligence)” และ “การตระหนักรู้ (consciousness)” ที่ในอนาคต algorithm ที่มีระดับสติปัญญาขั้นสูงจะเข้ามาทำหน้าที่แทนมนุษย์ผู้มีความตระหนักรู้แต่มีระบบประมวลผลที่ด้อยประสิทธิภาพกว่าโดยที่ algorithm เหล่านั้นไม่จำเป็นต้องมีความรู้สึกนึกคิดแต่อย่างใด ไม่แตกต่างจากเหตุการณ์ในอดีตที่ “ม้า” ซึ่งมีความรู้สึกและความผูกพันธุ์กับเจ้าของถูกแทนที่ด้วย “รถยนต์” อันไร้ความรู้สึกที่มีประสิทธิภาพในการพามนุษย์เดินทางจากจุดหนึ่งไปยังอีกจุดหนึ่งที่ดีกว่าได้ภายในระยะเวลาอันรวดเร็ว
คำถามสำคัญที่ตามมาก็คือ “แล้วที่ยืนของมนุษย์นั้นอยู่ตรงไหน” คำตอบของนักวิชาการส่วนใหญ่ในยุคปัจจุบันก็คือ งานที่ต้องใช้ความสามารถในการตระหนักรู้ถึงอารมณ์และความคิดสร้างสรรค์ของมนุษย์ แต่เอาเข้าจริงๆแล้ว สิ่งมีชีวิตทุกชนิดรวมถึงมนุษย์นั้นล้วนมี algorithm สำหรับการตัดสินใจทั้งนั้น งานที่ต้องใช้ความคิดสร้างสรรค์อย่างการแต่งเพลงนั้นตามหลักการของ Life science นั้นถือเป็นกระบวนการในการประมวลผลรูปแบบของเสียงตามหลักการทางคณิตศาสตร์ของมนุษย์เท่านั้น ซึ่งนักประดิษฐ์คนหนึ่งสามารถพัฒนา A.I. นามว่า EMI ที่สามารถประมวลผลเพลงของศิลปินชื่อดังอย่าง Bach มาใช้ในการแต่งเพลงใหม่ของตัวเองกว่า 5,000 เพลงภายใน 1 วันโดยที่ผู้ฟังนั้นเชื่อว่าเพลงเหล่านี้ถูกแต่งขึ้นมาโดยศิลปินเอกได้อย่างสนิทใจ โลกในอนาคตกำลังจะสร้างประชากรกลุ่มใหม่ที่ไร้ซึ่งคุณค่าใดๆต่อสังคม (useless class) ส่วนกลุ่มงานที่อาจได้รับผลกระทบน้อยที่สุดนั้นคือกลุ่มงานที่ต้องใช้ความสามารถเฉพาะตัวขั้นสูงแต่มีผลตอบแทนที่ต่ำจนไม่มีใครอยากลงทุนพัฒนาโปรแกรมมาทำหน้าที่ทดแทน อาทิ นักโบราณคดี
แนวโน้มถัดมาของศตวรรษที่ 21 ก็คือ การเกิดขึ้นของระบบการตัดสินใจที่หลอมรวมมนุษย์เข้าด้วยกันเป็นกลุ่ม ศาสตร์ Life science ได้พิสูจน์ว่าทุกกระบวนการตัดสินใจของมนุษย์นั้นเกิดขึ้นจาก algorithm ภายในของมนุษย์แต่ละคนที่เต็มไปด้วยข้อบกพร่องนานับประการ และเมื่อถึงจุดหนึ่งที่เทคโนโลยีสามารถทำความเข้าใจหลักการของ algorithm เหล่านั้นมากพอที่จะสร้าง external algorithm ที่มีประสิทธิภาพมากกว่าได้สำเร็จ ความสำคัญของปัจเจกบุคคลก็จะสูญสลายไป มนุษย์จะถูกหลอมรวมเป็นกลุ่มก้อนและถูกถ่ายเทพลังอำนาจไปยัง algorithm ที่สามารถตัดสินใจสิ่งต่างๆได้ดีกว่า ตัวอย่างที่เห็นชัดๆในปัจจุบันก็คือ เทคโนโลยีทางการแพทย์อย่าง wearable sensor ที่สามารถตรวจจับกระบวนการทำงานของร่างกายและให้คำแนะนำแก่ผู้ใช้งานอยู่ตลอดเวลา หรือ เหตุการณ์ที่ Angelina Jolie ตัดสินใจผ่าตัดเต้านมหลังรับทราบข้อมูลทางสถิติจากการตรวจสอบทางพันธุกรรมว่าตัวของเธอมีโอกาส 87% ที่จะเป็นมะเร็งเต้านม
ในอนาคต “มนุษย์” อาจจะต้องพึ่งพิง “โปรแกรม” อย่างแยกจากกันไม่ได้ตลอด 24 ชั่วโมง ลองจินตนาการโลกในอนาคตที่มนุษย์ทุกคนยอมให้ข้อมูลทั้งหมดแก่ Google ไม่ว่าจะเป็นข้อมูลทางชีววิทยา (อาทิ ข้อมูลการทำงานของร่างกายแบบ real time ผ่านการฝังไมโครเซนเซอร์ DNA และประวัติทางพันธุกรรม) ข้อมูลการใช้จ่าย อีเมล์และอีกมากมาย เมื่อข้อมูลมีมากพอถึงจุดหนึ่ง Google จะกลายมาเป็น algorithm ที่รู้จักมนุษย์มากกว่าตัวของพวกเขาเองและสามารถให้คำแนะนำในการตัดสินใจได้ดีกว่ามนุษย์ที่เต็มไปด้วยความลำเอียง (bias) และความไร้ประสิทธิภาพในการประมวลผลข้อมูลของตัวเอง
และเมื่อมนุษย์สามารถอัพเกรดร่างกายและสติปัญญาของตัวเองได้สำเร็จ โลกก็จะเริ่มเข้าสู่ยุคที่ความไม่เท่าเทียมกันระหว่างชนชั้นสูงกับชนชั้นล่างนั้นรุนแรงอย่างที่ไม่เคยเกิดขึ้นมาก่อน ความแตกต่างนั้นไม่ได้เกิดขึ้นเฉพาะฐานะทางเศรษฐกิจเท่านั้น แต่กลุ่มชนชั้นสูงนั้นจะกลายมาเป็นสิ่งมีชีวิตที่มีสมรรถภาพทางชีววิทยาที่เหนือกว่าและเมื่อเทคโนโลยีได้ทำให้ “ประโยชน์” ของกลุ่มชนชั้นล่างหมดไป ประชากรเกือบทั้งหมดของโลกมนุษย์อาจจะกลายมาเป็นเพียงขยะทางชีววิทยาที่ไร้คุณค่าใดๆในสายตาของกลุ่มมนุษย์ที่สามารถแปลงกายเป็นเทพเจ้าได้สำเร็จ
Lee Sedol vs. AlphaGo (ขอบคุณภาพจาก BGR India)
Chapter 10: The Ocean of Consciousness ศาสนาใหม่ของมนุษยชาติกำลังถือกำเนิดขึ้นในห้องทดลองทางวิทยาศาสตร์ที่มี Silicon Valley เป็นจุดศูนย์กลาง สิ่งที่สร้างความแตกต่างให้กับศาสนาแห่งใหม่นี้ก็คือความสามารถในการสร้างความสุข ความสงบหรือแม้กระทั่งชีวิตอันเป็นนิรันดร์ให้กับมนุษย์บนโลกใบนี้ (ไม่ใช่โลกหลังความตายเหมือนศาสนาอื่นๆ)
Techno-humanist คือ ศาสนาที่ต่อยอดจากมนุษยนิยมที่ยังคงมีความเชื่อมั่นในความสามารถของมนุษย์อยู่ แต่มนุษย์ตามความหมายของศาสนาใหม่นี้คือมนุษย์สายพันธุ์ Homo Deus ที่ได้รับการอัพเกรดจากเทคโนโลยีทางชีววิทยา นาโนและการเชื่อมต่อคอมพิวเตอร์เข้ากับระบบประสาท อันเป็นเหตุให้ Homo Deus มีความสามารถที่ยังคงสร้างคุณค่าในโลกที่เต็มไปด้วย algorithm ได้ (นักวิทยาศาสตร์เชื่อว่ามนุษย์สามารถขยายประสิทธิภาพของการใช้งานของสมองและจิตใจได้อีกมหาศาลโดยยกตัวอย่างศักยภาพที่มนุษย์ในปัจจุบันไม่สามารถทำได้ อาทิ ประสิทธิภาพในการดมกลิ่นของมนุษย์ในยุคล่าสัตว์ การอ่านคลื่นสะท้อนของค้างคาวและการตรวจจับเสียงที่อยู่ห่างออกไปหลายร้อยกิโลเมตรของปลาวาฬ)
แต่หลักการของ Techno-humanist ก็มีข้อจำกัด เมื่อมนุษย์สามารถอัพเกรดระบบการตัดสินใจของตัวเองจนสามารถกำจัดตัวตนที่ไร้เหตุผลหรือตัวตนที่ก่อให้เกิดความลำบากใจภายในจิตใจของพวกเขาได้ มนุษย์เวอร์ชั่นอัพเกรดเหล่านั้นก็จะมีระบบการตัดสินใจที่ไม่แตกต่างจาก algorithm อันเป็นจุดกำเนิดของศาสนาชนิดใหม่ที่มี “ข้อมูล” เป็นศูนย์กลาง
Chapter 11: The Data Religion Dataism (ข้อมูลนิยม) คือ ศาสนาที่เชื่อมั่นในประสิทธิภาพของ “ข้อมูล” อันเกิดขึ้นจากการผสมผสานความก้าวหน้าของศาสตร์ทางชีววิทยาและคอมพิวเตอร์ที่เชื่อมั่นว่า algorithm ของสิ่งมีชีวิตและคอมพิวเตอร์นั้นสามารถผสมผสานรวมกันได้
ประวัติศาสตร์ของศตวรรษที่ 20 แสดงให้เห็นอย่างชัดเจนว่าระบบการปกครองที่สามารถ “ประมวลผลข้อมูล (data processing)” ได้อย่างครอบคลุมมากกว่านั้นคือระบบการปกครองที่มีประสิทธิภาพและพลังอำนาจที่สูงที่สุด (ระบอบคอมมิวนิสต์ที่มีรัฐบาลกลางทำหน้าที่ประมวลผลข้อมูลแต่เพียงผู้เดียวไม่สามารถสู้กับกำลังของหน่วยประมวลผลข้อมูลจำนวนมหาศาลของระบบกระจายอำนาจตามหลักเสรีนิยมได้) แต่ในยุคปัจจุบันที่การเปลี่ยนแปลงทางเทคโนโลยีนั้นเกิดขึ้นอย่างรวดเร็วพร้อมๆกับการเพิ่มขึ้นของข้อมูลจำนวนมหาศาล ระบบการปกครองทุกรูปแบบในปัจจุบันไม่สามารถติดตามการเปลี่ยนแปลงได้อีกต่อไปอันเป็นเหตุให้โลกต้องการระบบและผู้ปกครองรูปแบบใหม่ที่มีประสิทธิภาพที่ดีกว่า
ประวัติศาสตร์ยังแสดงให้เห็นถึงวิวัฒนาการของกระบวนการประมวลผลข้อมูลของมนุษยชาติ ที่เริ่มตั้งแต่ การขยายจำนวนประชากร (หน่วยประมวลผล) อันก่อให้เกิดความหลากหลายของหน่วยประมวลผลเหล่านั้นที่กระจัดกระจายอยู่ทั่วโลก จนกระทั่งโลกในยุคเกษตรกรรมและอุตสาหกรรมที่หน่วยประมวลผลที่มีความหลากหลายได้กลับมารวมตัวกันเป็นกลุ่มที่ใหญ่ขึ้นและเชื่อมต่อกันได้อย่างมีประสิทธิภาพมากขึ้นเรื่อยๆ ซึ่งแสดงให้เห็นถึงแนวโน้มของการเกิดขึ้นของการเชื่อมต่อทางข้อมูลอย่างสมบูรณ์ของมนุษยชาติและทุกสรรพสิ่ง (Internet-of-all-thing)
อิสรภาพของข้อมูล (freedom of information) คือ “หัวใจ” สำคัญของ Dataism ที่เชื่อมั่นว่า “เมื่อมนุษย์ยินยอมเปิดเผยข้อมูลทั้งหมดให้กับระบบ algorithm หนึ่งเดียวของโลก ทุกกระบวนการตัดสินใจของมนุษย์จะถูกประมวลผลและตัดสินใจผ่านระบบประมวลผลแห่งนั้นอันนำมาซึ่งผลลัพธ์ที่ดีที่สุดให้กับมนุษย์และทุกสรรพสิ่ง” ตัวอย่างของการใช้ระบบข้อมูลมหาศาลในการเพิ่มประสิทธิภาพให้กับโลกมนุษย์นั้นได้แก่ ระบบแบ่งปันรถยนต์ไร้คนขับ (driverless carpooling) ที่เข้ามาขจัดปัญหาของความสิ้นเปลืองของการใช้ทรัพยากร “รถยนต์” ที่ใช้เวลามากกว่า 90% ในการจอดอยู่กับที่เฉยๆอย่างไร้ประโยชน์ หากมนุษย์ทุกคนยอมเปิดเผยข้อมูลตำแหน่งที่อยู่อาศัย ที่ทำงาน จุดหมายปลายทางและเวลาให้กับระบบ algorithm อย่างสมบูรณ์ ระบบประมวลผลนี้ก็จะสามารถจัดสรรการใช้ทรัพยากรรถยนต์แบบแบ่งปันให้กับผู้ที่ต้องการเดินทางด้วยรถยนต์ได้อย่างมีประสิทธิภาพและยังสามารถลดปริมาณรถยนต์ส่วนบุคคลในท้องถนนได้ถึง 20 เท่าพร้อมกับการลดลงของพื้นที่จอดรถอีกจำนวนมหาศาล
Dataism (ขอบคุณภาพจาก Financial Times)
แต่ถึงกระนั้น ทฤษฎีที่กล่าวมาทั้งหมดในหนังสือเล่มนี้เป็นเพียงแค่การพยากรณ์ที่อาศัยการศึกษาทางประวัติศาสตร์ของมนุษยชาติและการทำความเข้าใจเทคโนโลยีในยุคปัจจุบันเท่านั้น มนุษย์ในทุกวันนี้คงไม่มีทางมองเห็นและเข้าใจมนุษย์ในอีก 50 ปีข้างหน้าได้อย่างสมบูรณ์ แต่ 3 แนวโน้มที่กำลังเกิดขึ้นจริงที่ทุกคนควรจะต้องคำนึงถึงอยู่เสมอในการวางแผนอนาคตนั้นก็คือ
 สิ่งมีชีวิตทั้งหมดนั้นคือระบบประมวลผลที่ถูกผลักดันโดย algorithm “สติปัญญา” กับ “การตระหนักรู้” นั้นกำลังถูกแยกออกจากกัน มนุษย์ผู้มีอารมณ์ความรู้สึกกำลังจะถูกแทนที่ด้วย algorithm ที่มีสติปัญญาที่สูงกว่า algorithm กำลังจะมีความสามารถในการเข้าใจมนุษย์มากกว่าตัวของพวกเราเอง   Source :.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_1/homo_deus/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_3/example_2_subtopic_3/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Subtopic 3 is very cool!
      </h3>
      <p class="refresh-summary">The summary is a custom custom one</p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_3/example_2_subtopic_3/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/example_1_topic_1/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Topic 1 is very cool (again)!
      </h3>
      <p class="refresh-summary">This summary is multiline</p> 
      <div class="action has-text-right">
        <a href="/topic_1/example_1_topic_1/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/example_3_topic_1/"><img src="/topic_1/example_3_topic_1/summary_hu61d4afe1662869bae3e950774424d001_1030734_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Topic 1 is very cool!
      </h3>
      <p class="refresh-summary">The summary image should be a custom one</p> 
      <div class="action has-text-right">
        <a href="/topic_1/example_3_topic_1/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/example_2_topic_2/"><img src="/topic_2/example_2_topic_2/summary_2_hu8dbdcdbe8d39d486b4229abea415a569_6024271_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Topic 2 is very cool!
      </h3>
      <p class="refresh-summary">The summary image should be a custom one</p> 
      <div class="action has-text-right">
        <a href="/topic_2/example_2_topic_2/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">default_image</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/example_1_topic_2/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Example 1 of Topic 2
      </h3>
      <p class="refresh-summary">This summary is the real content of the article.
</p> 
      <div class="action has-text-right">
        <a href="/topic_2/example_1_topic_2/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_4/example_2_subtopic_4/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Example 2 of subtopic 4
      </h3>
      <p class="refresh-summary">This summary is the real content of the article.
</p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_4/example_2_subtopic_4/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_4/example_1_subtopic_4/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Subtopic 4 is very cool!
      </h3>
      <p class="refresh-summary">The summary image should be the default</p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_4/example_1_subtopic_4/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">homo_deus</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/homo_deus/"><img src="/topic_1/homo_deus/images/Homo_Deus_hud949c3cfa973b39c16ba4d2c8cf1d6ae_196421_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        [สรุปหนังสือ] Homo Deus : A Brief History of Tomorrow
      </h3>
      <p class="refresh-summary"> “This is the best reason to learn history: not in order to predict the future, but to free yourself of the past and imagine alternative destinies. Of course this is not total freedom – we cannot avoid being shaped by the past. But some freedom is better than none.”
เมื่อเราเข้าใจ “เรื่องราวที่แท้จริง” ของมนุษย์ เมื่อนั้นเราก็จะเข้าใจ “เป้าหมาย” ของการเดินทางของมนุษยชาติ
Homo Deus คือ หนังสือภาคต่อจาก Sapiens ของ Yuval Noah Harari ศาสตราจารย์ทางประวัติศาสตร์ผู้นำเสนอเรื่องราวของ “อนาคต” ของมนุษยชาติผ่านการศึกษาความเป็นมาทางประวัติศาสตร์ของเผ่าพันธุ์ Homo Sapiens ที่พัฒนาตนเองขึ้นจากการเป็น “สัตว์” อันไร้ซึ่งความสำคัญใดๆมาเป็น “เทพเจ้า” ผู้กำหนดชะตาชีวิตของทุกสรรพสิ่ง แต่เมื่อสิ่งที่ถูกมนุษย์สร้างขึ้นอย่าง algorithm เริ่มมีสติปัญญาที่ชาญฉลาดกว่ามันสมองของมนุษย์ โลกของเราทุกคนจะเปลี่ยนแปลงไปอย่างไร
Chapter 1: The New Human Agenda โศกนาฏกรรม 3 อันดับสำคัญที่มนุษยชาติต้องพบเจอตลอดระยะเวลาหลายพันปีที่ผ่านมานั้นประกอบไปด้วย “ความอดอยาก” “โรคระบาด” และ “สงคราม”
Famine: ในยุคเกษตรกรรม หากภัยธรรมชาติส่งผลกระทบต่อผลิตผลทางการเกษตรของหมู่บ้านแห่งหนึ่ง ความทุกข์ทรมานจากความอดอยากนั้นเป็นสิ่งที่แทบจะหลีกเลี่ยงไม่ได้เลย แต่ทุกวันนี้ ผลิตภาพทางการเกษตรและความเป็นอยู่ที่ดีขึ้นของมนุษย์ได้ทำให้ภาวะอดอยากหายสาบสูญไปจากโลกจนเกือบจะหมดสิ้นและมนุษย์กลับมีโอกาสเสียชีวิตจากภาวะการรับประทานอาหารที่ “มากเกินไป” แทน
Plague: เริ่มต้นในช่วงทศวรรษที่ 1330 ภัยร้ายนามว่า the Black Death ได้คร่าชีวิตมนุษย์กว่า 1 ใน 4 ของประชากรทั้งหมดในแถบทวีปยุโรปและเอเชีย ในยุคเริ่มต้นของการล่าอาณานิคมของชาวสเปนในแถบทวีปอเมริกาได้นำพาเชื้อโรคที่ชาวท้องถิ่นไม่เคยได้สัมผัสข้ามทวีปมาแพร่ระบาดไปทั่วอาณาจักร Aztec และ Maya อย่างรวดเร็วจนทำให้กว่า 90% ของประชากรท้องถิ่นต้องจบชีวิตลงภายในระยะเวลาอันสั้น แต่ในปัจจุบัน ภาวะโรคระบาดนั้นสร้างความเสียหายในอัตราที่น้อยลงเป็นอย่างมากทั้งๆที่สภาวะการอาศัยอยู่รวมกันในเมืองที่แออัดและระบบการคมนาคมอันรวดเร็วนั้นถือเป็นแหล่งแพร่กระจายโรคระบาดได้เป็นอย่างดี ทั้งนี้ก็เพราะว่าวิทยาการทางการแพทย์สมัยใหม่ได้รับการพัฒนาจนสามารถตรวจจับสาเหตุและควบคุมการแพร่ระบาดของโรคติดต่อได้อย่างรวดเร็ว (Ebola ที่เกิดขึ้นในปี 2014 คร่าประชากรไปเพียงแค่หลักหมื่นคนเท่านั้น) ความเสี่ยงของโรคระบาดร้ายแรงในอนาคตน่าจะเกิดขึ้นจากฝีมืของมนุษย์มากกว่าจากธรรมชาติ
War: ปัจจุบันสงครามและการก่ออาชญากรรมนั้นเป็นเพียงแค่ 1% ของสาเหตุการตายของมนุษย์ (ซึ่งน้อยกว่าการฆ่าตัวตายและโรคเบาหวาน) ทั้งนี้ก็เพราะว่าสงครามในยุคปัจจุบันนั้นแทบจะ “ไม่มีประโยชน์” ในความคุ้มค่าทางเศรษฐกิจอีกต่อไป การเกิดขึ้นของอาวุธ “นิวเคลียร์” ส่งผลให้การสู้รบกันระหว่างชาติมหาอำนาจเป็นเหมือนการ “ฆ่าตัวตายหมู่” ที่ไร้เหตุผลสิ้นดี ชัยชนะจากสงครามนั้นก็แทบจะไม่มีมูลค่าอีกต่อไปเพราะทรัพย์สมบัติที่มีค่าที่สุดของมนุษย์ในยุคนี้อยู่ใน “คอมพิวเตอร์” และ “มันสมอง” ของมนุษย์ (สงครามที่เกิดขึ้นในช่วงที่ผ่านมานั้นมักเกิดขึ้นกับประเทศที่มีทรัพยากรมหาศาล อาทิ บ่อน้ำมันหรือเหมืองแร่) ในทางกลับกัน “ความสงบสุขอันยั่งยืน” ก็ได้เกิดขึ้นมาแทนที่ “ความสงบสุบชั่วคราว” ผ่านการทำการค้าและการลงทุนระหว่างประเทศที่ทำให้ความคิดถึงการทำสงครามระหว่างกันนั้น “เป็นไปไม่ได้” อีกต่อไป และนี่คือสาเหตที่ประเทศจีนเลือกทำสัญญาทางธุรกิจกับ Apple และ Microsoft แทนการยกทัพมายึด Silicon Valley
เมื่อ “ความท้าทาย” ในอดีตได้รับการแก้ไขจนเกือบสมบูรณ์แล้ว มนุษย์ผู้มีความทะเยอทะยานสูงก็ได้เริ่มออกตามหา “ความท้าทาย” ครั้งใหม่อันประกอบไปด้วย ความเป็นอมตะ ความสุขและการก้าวข้ามขีดจำกัดของมนุษย์ธรรมดาไปเป็น “พระเจ้า”
Immortality: “ความตาย” ถือเป็นส่วนสำคัญอย่างมากของ “ศาสนา” ที่คอยพร่ำสอนให้มนุษย์ทำความดีเพื่อสร้าง “ชีวิตหลังความตาย” ที่ยอดเยี่ยม (ถ้าไม่มีความตาย แนวคิดของสวรรค์และนรกคงไม่เคยเกิดขึ้น) แต่ปัจจุบัน “ความตาย” ที่เกิดขึ้นกับมนุษย์ผ่าน “โรคร้าย” ต่างๆนั้นถูกตีความใหม่ให้กลายมาเป็นเพียงแค่ “ปัญหาทางเทคนิค” ของร่างกายมนุษย์ที่รอให้เทคโนโลยีทางการแพทย์แก้ไขปัญหาให้หายขาด ปัจจุบัน นักวิทยาศาสตร์และบริษัทเทคโนโลยีชั้นนำเริ่มให้ความสำคัญกับ Life Science หรือศาสตร์แห่งชีวิตที่ศึกษาเรื่องการรักษาความผิดปกติของร่างกายและการยืดอายุไขของเซลส์และอวัยวะต่างๆของมนุษย์ ในเร็วๆนี้ เราอาจจะเริ่มเห็นมนุษย์ที่มีอายุไขเพิ่มขึ้นจากเดิม (ซึ่งนั่นก็ทำให้โครงสร้างทางสังคมเปลี่ยนแปลงไปจากปัจจุบันโดยสิ้นเชิง ลองคิดดูว่ามนุษย์อาจจะเกษียณในอายุ 120 ปี ปูตินอาจจะเป็นนายกของรัสเซียต่อไปอีก 60 ปีก็ได้) ส่วนในระยะยาว มนุษย์อาจเริ่มกลายเป็น “a-mortal” หรือ ผู้ที่มีร่างกายเป็นอมตะ (ยกเว้น ได้รับความเสียหายอย่างรุนแรงจนเสียชีวิต ซึ่งก็อาจจะทำให้บุคคลอมตะเหล่านี้เกรงกลัวต่อกิจกรรมที่มีความเสี่ยงอย่างสุดโต่ง) ซึ่งแน่นอนว่า “มนุษย์” ทุกคนจะต้องพยายามทุกวิถีทางให้ตัวเองกลายเป็นมีชีวิตอยู่ชั่วนิรันดร์และเงินทุนมหาศาลจะเป็นตัวกระตุ้นให้การตามล่าความเป็นอมตะดำเนินต่อไปจนสัมฤทธิ์ผล
Happiness: ตั้งแต่การเริ่มต้นของการสร้างอาณาจักรและประเทศชาติ การพัฒนาโครงสร้างพื้นฐาน การศึกษา สุขอนามัยและการยกระดับความเป็นอยู่อาศัยของประชาชนนั้นมีวัตถุประสงค์หลักคือการสร้าง “ความแข็งแกร่ง” ให้กับประเทศชาติโดยปราศจากการคำนึงถึง “ความสุข” ที่แท้จริงของประชาชน (ประเทศพัฒนาแล้วมากมายมีอัตราการฆ่าตัวตายสูงขึ้นตามการเพิ่มขึ้นของ GDP per capita) การสร้างความสุขที่แท้จริงของมนุษย์นั้นต้องเริ่มต้นจากการแก้ไข “อุปสรรค” สำคัญ 2 ข้อ ได้แก่ 1. อุปสรรคทาง “จิตวิทยา” ที่ว่าด้วยความสุขของมนุษย์นั้นเกิดขึ้นจากการที่ความจริงที่พวกเขาต้องเผชิญนั้นตรงกับความคาดหวังของพวกเขามากน้อยแค่ไหน (ซึ่งปัจจุบัน ความก้าวหน้าทางเทคโนโลยีได้นำพาให้ความคาดหวังของมนุษย์สูงขึ้นๆไปเรื่อยๆ) 2. อุปสรรคทาง “ชีววิทยา” ที่พิสูจน์ไว้แล้วว่าความสุขของมนุษย์ขึ้นอยู่กับ “ฮอร์โมน” ภายในร่างกายเท่านั้น อันเป็นไปตามการวิวัฒนาการของธรรมชาติที่สร้างให้มนุษย์มีความรู้สึกดีหลังจากได้กระทำสิ่งที่เป็นประโยชน์ต่อการดำรงชีวิตได้สำเร็จก่อนที่ความรู้สึกนั้นจะจางหายไปในเวลาต่อมา เทคโนโลยีในอนาคตที่จะเข้ามาเติมเต็มช่องว่างก้อนใหญ่ของมนุษย์ก็คือการสร้าง “ความสุขชั่วนิรันดร์” ผ่านการ “จัดระเบียบ” ระบบการทำงานของสารเคมีในร่างกายมนุษย์ให้สามารถหลั่งฮอร์โมนให้มนุษย์มีความสุขในเชิงบวกได้ตลอดเวลา (ปัจจุบันมีมนุษย์หลายล้านคนกินยาระงับประสาท ยาแก้โรคซึมเศร้าและยาเสพย์ติด ที่ล้วนมีผลต่อการปรับเปลี่ยนสารเคมีที่มีผลต่ออารมณ์ความรู้สึกของมนุษย์ทั้งนั้น)
Divinity: หลังจากที่มนุษย์สามารถออกแบบร่างกายและจิตใจของตัวเองได้แล้ว เป้าหมายที่เหลือของพวกเขาก็คงเป็นการอัพเกรด “พลัง” ให้กับตัวเองให้เทียบชั้นกับ “เทพเจ้า” ตั้งแต่ การเสริมสร้างความสามารถของร่างกายผ่านการปรับเปลี่ยนพันธุกรรม การผสมผสานหุ่นยนต์เข้ากับร่างกายและสมองของมนุษย์ ไปจนถึงการอัพโหลดจิตใจของมนุษย์เข้าไปยังเครื่องจักรที่ไม่มีวันตาย ทั้งหมดนี้อาจจะดูเพ้อฝัน แต่สิ่งหนึ่งที่สามารถมั่นใจได้เลยก็คือ มนุษย์ในปัจจุบันคงไม่สามารถจินตนาการถึง “มนุษย์อนาคต” ที่ก้าวผ่านจากสายพันธุ์ Homo Sapiens (มนุษย์ฉลาด) มาเป็น Homo Deus (มนุษย์เทพเจ้า) ได้อย่างแน่นอน
ประวัติศาสตร์เป็นข้อพิสูจน์ชั้นดีว่าการ “อัพเกรดมนุษย์” นั้นเป็นสิ่งที่ไม่อาจหลีกเลี่ยงได้ในอนาคต เทคโนโลยีในช่วงเริ่มต้นนั้นอาจเกิดขึ้นในรูปแบบของกระบวนการแก้ไขความผิดปกติต่างๆของมนุษย์ (อาทิ การเลือกเอ็มบรีโอของเด็กทารกที่มีสุขภาพสมบูรณ์ หรือ การแก้ไขดีเอ็นเอที่มีปัญหาของเอ็มบรีโอ) แต่หากกระบวนการเหล่านั้นสามารถทำให้มนุษย์ปกติมีคุณสมบัติที่เพิ่มขึ้น สุดท้ายโลกจะไม่อาจควบคุมการแพร่กระจายของเทคโนโลยีนี้ได้ (หากอเมริกาสั่งแบนการตัดต่อพันธุกรรม แต่เกาหลีเหนือสนับสนุนโครงการนี้เต็มที่เพื่อสร้างมนุษย์อัจฉริยะ อเมริกาคงไม่มีทางยอมปล่อยโอกาสนี้ไปแน่ๆ)
แต่ก็อย่าลืมเด็ดขาดว่า “อนาคต” ที่พวกเราพอจะมองเห็นอยู่ลิบๆนั้นเกิดขึ้นจากการร้อยเรียงเรื่องราวและความเชื่อของมนุษย์ที่ถูกสร้างขึ้นโดยมนุษย์ตั้งแต่ในอดีตจนถึงปัจจุบัน นั่นหมายความว่าโลกและมนุษยชาติอาจจะไม่ใช่สิ่งที่พวกเราจินตนาการถึงได้เลยเมื่อ “อนาคต” ที่แท้จริงเดินทางมาถึง
Part I – Homo Sapiens Conquers the World Chapter 2: The Anthropocene การพยากรณ์ความสัมพันธุ์ระหว่าง Homo Sapiens ยุคปัจจุบันกับ Homo Deus ในยุคอนาคตนั้นคงเป็นสิ่งที่ไม่ง่ายนัก แต่เราก็อาจพอจะใช้ประวัติศาสตร์ของความสัมพันธุ์ระหว่าง “มนุษย์” กับ “สัตว์” ชนิดอื่นมาใช้เป็นกรอบความคิดคร่าวๆได้
Antropocene คือ “ยุคแห่งมนุษย์” ที่เริ่มต้นขึ้นเมื่อ 70,000 ปีก่อนหลังจากการปฏิวัติทางความตระหนักรู้ของมนุษย์สายพันธุ์ Homo Sapiens ที่ทำให้ “มนุษย์” กลายเป็นสิ่งมีชีวิตสายพันธุ์เดียวในโลกที่กุมชะตากรรมของสิ่งมีชีวิตทั้งหมด (ปัจจุบัน สัดส่วนน้ำหนักรวมของสิ่งมีชีวิตเกิน 90% ตกเป็นของมนุษย์ สัตว์เลี้ยงและสัตว์อุตสาหกรรม)
ความสัมพันธุ์ของ Homo Sapiens และสิ่งมีชีวิตชนิดอื่นๆในยุคของนักล่าสัตว์และนักหาของป่านั้นอยู่ในรูปของความเชื่อที่ว่าสิ่งมีชีวิตและไม่มีชีวิตทั้งหมดล้วนมีจิตวิญญาณ (Animism) ที่มนุษย์จะต้องทำความเคารพและเกื้อกูลระหว่างกัน (การล่าสัตว์ในยุคโบราณนั้นต้องประกอบด้วยพิธีขอขมาสัตว์ที่ถูกล่านั้นๆ) ก่อนที่การปฏิวัติทางเกษตรกรรมจะเปลี่ยนพฤติกรรมและความเชื่อของมนุษย์ให้กลายเป็นการเคารพต่อ “เทพเจ้า” ผู้มีพลังอำนาจมหาศาลที่มาพร้อมกับการเปลี่ยนความคิดของการนับถือสัตว์กลายเป็นการมองว่าสิ่งมีชีวิตชนิดอื่นๆนั้นมีศักดิ์ที่ต้อยต่ำกว่ามนุษย์อันเป็นเหตุให้ปศุสัตว์ผู้ถูกเลือกอย่าง หมู วัว ไก่และแกะต้องกลายมาเป็นสัตว์ที่ต้องทนทุกข์ทรมานมากที่สุดในโลก (พร้อมกับการแพร่กระจายจำนวนอย่างรวดเร็ว) ซึ่งการปฏิวัติทางวิทยาศาสตร์ก็ยิ่งเข้ามาซ้ำเติมความโหดร้ายของมนุษย์ที่มีต่อสัตว์เหล่านั้นเข้าไปอีก (ยกเว้นในช่วงไม่กี่ทศวรรษที่ผ่านมาที่มนุษย์เริ่มหวนกลับมาให้ความสำคัญกับสิ่งมีชีวิตชนิดอื่น)
ในอนาคตนั้น เราไม่อาจรับรู้ได้เลยว่า “มนุษย์เวอร์ชั่นอัพเกรด” หรือ A.I. ผู้มีสติปัญญาที่หลักแหลมกว่า Homo Sapiens ในปัจจุบันมากจะจัดการกับพวกเราเหมือนในอดีตที่ผ่านมาหรือไม่
Antropocene หรือ “ยุคแห่งมนุษย์” (ขอบคุณภาพจาก Motherboard – Vice)
Chapter 3: The Human Spark “อะไร” คือสิ่งที่ทำให้มนุษย์มีความพิเศษมากกว่าสิ่งมีชีวิตชนิดอื่น
หากคำถามนี้ถูกถามในยุคที่วิทยาศาสตร์ยังไม่ได้รรับการยอมรับอย่างกว้างขวาง คำตอบคงเป็น “วิญญาณ” ที่มีเพียงมนุษย์เท่านั้นที่ได้ครอบครอง อันเป็นเหตุให้มนุษย์มองเห็นสิ่งมีชีวิตอื่นๆเป็นเพียงร่างที่ไร้วิญญาณและสามารถย่ำยีได้ตามชอบ (ปัจจุบันชาวอเมริกันเพียงแค่ 15% เท่านั้นที่เชื่อว่ามนุษย์เกิดขึ้นจากการวิวัฒนาการตามธรรมชาติเท่านั้นโดยไม่ต้องพึ่งพระเจ้า)
ส่วนคำตอบที่น่าจะได้รับการยอมรับในยุคปัจจุบันมากกว่าก็คือ “จิตใจ” (mind) และ “การตระหนักรู้” (consciousness) ซึ่งปัจจุบัน เทคโนโลยีทางวิทยาศาสตร์สามารถตรวจจับกระบวนการทำงานของระบบประสาทภายในสมองของสิ่งมีชีวิตได้อย่างมีประสิทธิภาพในระดับหนึ่งแต่ก็ยังไม่สามารถตรวจจับกระบวนการทำงานของ “จิตใจ” ที่ทำให้มนุษย์รู้สึกรัก โลภ โกรธหรือกลัวได้เลย (มีเพียงตัวของเราเองเท่านั้นที่เชื่อมั่นในการมีอยู่ของจิตใจและความรู้สึกของเรา ไม่แน่เราอาจจะเป็นเพียงผู้เล่นในโลกจำลองของสิ่งมีชีวิตชั้นสูงหรือมนุษย์ในอนาคตอยู่ก็เป็นได้)
นักวิทยาศาสตร์หลายคนพยายามจับเอาคอนเส็ปต์ของระบบคอมพิวเตอร์อย่าง algorithm ที่เป็นเหมือนชุดคำสั่งขนาดใหญ่มาใช้อธิบายถึงพฤติกรรมของสิ่งมีชีวิตต่างๆแทนการใช้จิตใจ อาทิ เมื่อ ลิงเห็นเสือ ประสาทตาของลิงก็จะทำการส่งกระแสไฟฟ้าเข้าไปยังสมองเพื่อประมวลผลผ่าน algorithm ของลิงตัวนั้นและส่งผลลัพธ์ออกมาในรูปของกระแสประสาทเพื่อให้ลิงวิ่งหนีเสือ โดยที่ลิงไม่ได้มีความรู้สึกกลัวหรือตกใจแต่อย่างใดเลย
แต่เอาจริงๆแล้ว การทดลองหลายครั้งในอดีตก็ได้พิสูจน์ว่าสิ่งมีชีวิตชนิดอื่นๆก็มีความตระหนักรู้ได้ไม่แพ้มนุษย์ อาทิ Clever Hans ม้าผู้แสนฉลาดแห่งเยอรมนีในยุคปี 1900s ที่มีความสามารถในการตอบปัญหาบวกลบคูณหารเลขผ่านการเคาะเท้าเป็นจำนวนครั้งตามคำตอบที่ถูกต้องได้อย่างแม่นยำ ซึ่งนักจิตวิทยาได้ค้นพบว่า Clever Hans ไม่ได้มีความสามารถในการคิดเลขเหมือนกับมนุษย์ แต่เจ้าม้าตัวนี้ใช้วิธีการเคาะเท้าพร้อมๆกับการสังเกตสีหน้าของมนุษย์ผู้เป็นคนถามคำถามที่มักจะแสดงอาการอย่างชัดเจนเมื่อ Clever Hans กระแทกเท้าจนใกล้ถึงคำตอบที่ถูกต้องและมันก็จะหยุดกระแทกเท้าไปในที่สุด
แต่จากการศึกษาประวัติศาสตร์ของมนุษยชาติ “ปัจจัย” ที่น่าจะส่งผลให้ Homo Sapiens กลายมาเป็นสิ่งมีชีวิตที่ยิ่งใหญ่ที่สุดในโลกได้ภายในเวลาเพียงแค่เสี้ยวหนึ่งของอายุของดาวดวงนี้ ไม่ใช่ “วิญญาณ” หรือ “จิตใจอันสูงส่ง” แต่กลับกลายเป็น “ความสามารถในการทำงานร่วมกันอย่างซับซ้อนและมีประสิทธิภาพ” อย่างที่สิ่งมีชีวิตชนิดอื่นสามารถทำได้ ซึ่งสาเหตุที่มนุษย์สามารถทำงานร่วมกันเป็นกลุ่มขนาดหลักพันหลักล้านคนได้นั้นเกิดจากการที่มนุษย์สามารถคิดค้น “ความเชื่อที่ถูกสร้างขึ้นโดยฝีมือมนุษย์” (imagined order) อาทิ ศาสนา พระเจ้า หลักมนุษยธรรม ประชาธิปไตยและระบบเงิน
มนุษย์คือสิ่งมีชีวิตเดียวในโลกที่สามารถจินตนาการ “ความหมาย” ของการมีชีวิตอยู่และการทำงานร่วมกันระหว่างมนุษย์ด้วยกันแองได้โดยไม่จำเป็นต้องพึ่ง “ความจริง” ที่เกิดขึ้นตามหลักการทางวิทยาศาสตร์และธรรมชาติ อาทิ นักรบชาวคริสเตียนเชื่อมั่นว่าตัวเองจะได้ขึ้นสวรรค์หากเข้าร่วมสงครามครูเสดเพื่อคร่าชีวิตนักรบชาวอิสลามที่มีความเชื่อคล้ายๆกัน
และเมื่อการเวลาเปลี่ยนไป ความเชื่อและความหมายของชีวิตก็เปลี่ยนแปลงไป ปัจจุบันความเชื่อเกี่ยวกับเทพเจ้าและศาสนากำลังเสื่อมความนิยม ขณะที่ความเชื่อเรื่องประชาธิปไตย ความเท่าเทียมกันและสิทธิมนุษยชนกำลังได้ความนิยมที่เพิ่มขึ้นเรื่อยๆ ส่วนในอนาคต ความเชื่อรูปแบบใหม่ที่มนุษย์ในยุคปัจจุบันอาจจะยังคาดไม่ถึงก็อาจจะเกิดขึ้นได้ในไม่ช้า
ม้าแสนรู้ Clever Hans ที่ไม่ได้ฉลาดเหมือนที่มนุษย์คิด (ขอบคุณภาพจาก Wikipedia)
Part II – Homo Sapiens Gives Meaning to the World Chapter 4: The Storytellers มนุษย์คือสิ่งมีชีวิตชนิดเดียวที่อาศัยอยู่ในโลกที่มีความจริงซ้อนกันอยู่ 3 ชั้น (three-layered reality) อันประกอบไปด้วย ความจริงเชิงวัตถุ (objective reality) อาทิ อากาศและสิ่งแวดล้อม ความจริงเชิงปัจเจกบุคคล (subjective reality) อันได้แก่ อารมณ์และความรู้สึกภายใน และความจริงที่ถูกสร้างขึ้นโดยมนุษย์ด้วยกันเอง (imagined reality)
ประวัติศาสตร์ของมนุษย์นั้นถูกสร้างขึ้นจากการร้อยเรียงกันของเรื่องเล่าและความเชื่อของมนุษย์ ตั้งแต่ 70,000 ปีก่อนที่มนุษย์เริ่มสามารถจินตนาการถึง “สิ่งที่ไม่มีอยู่จริง” อันเป็นจุดเริ่มต้นของการสร้าง “ความจริง” โดยฝีมือของจินตนาการของมนุษย์เพื่อสร้างความสามารถในการทำงานร่วมกันของมนุษย์หมู่มากได้สำเร็จ
โดยจุดเปลี่ยนครั้งสำคัญของมนุษยชาติ คือ การปฏิวัติทางเกษตรกรรมเมื่อ 12,000 ปีก่อนที่ทำให้มนุษย์เริ่มเปลี่ยนพฤติกรรมจากการออกล่าสัตว์และหาของป่าไปเป็นการอยู่รวมกันเป็นหลักแหล่งเป็นหมู่บ้านขนาดย่อม ก่อนที่ “อาณาจักร” ขนาดใหญ่จะเริ่มกำเนิดขึ้นหลังจากที่ชาวสุเมเรียนได้คิดค้น “ภาษาเขียน” อันเป็นพื้นฐานของระบบบัญชีและการปกครองของมวลมนุษย์ขนาดใหญ่ได้สำเร็จในช่วง 5,000 ปีก่อน และภาษาเขียนนี่เองที่เป็นตัวจุดประกายให้เกิดการสร้าง “เรื่องแต่ง” ให้กลายมาเป็น “เรื่องจริง” ที่ทำให้ประชาชนในแต่ละอาณาจักรเชื่อมั่นได้อย่างสนิทใจได้ ซึ่งในช่วงแรกเริ่มของอาณาจักรนั้น เรื่องจริงที่ถูกสร้างขึ้นโดยฝีมือมนุษย์นั้นมีลักษณะคล้ายๆกันก็คือ การนำเอา “เทพเจ้า” หรือ “พระเจ้า” มาเป็นจุดศูนย์กลางของความเชื่อและกฎระเบียบการปกครองของอาณาจักรโดยแต่ละอาณาจักรจะมีผู้นำที่เปรียบเสมือน “ตัวแทนของเทพเจ้า” หรือไม่ก็เป็น “เทพเจ้า” ซะเองเลยอย่างฟาโรห์ของอาณาจักรอียิปต์
ถึงแม้ในยุคปัจจุบัน ความเชื่อเรื่องเทพเจ้าและภูติผีปีศาจจะได้เสื่อมถอยลงไป แต่โลกของเรากลับเต็มไปด้วย “เรื่องแต่ง” โดยฝีมือมนุษย์ในยุคใหม่มากมาย ไม่ว่าจะเป็น ประเทศชาติ หลักการปกครอง เงินและบริษัท ที่ล้วนแล้วแต่จะมีอิทธิพลต่อมนุษย์มากขึ้นเรื่อยๆ ซึ่งเอาจริงๆ เรื่องแต่งเหล่านี้ล้วนมีความสำคัญอย่างมากในการอยู่ร่วมกันของมนุษย์ในยุคปัจจุบัน แต่พวกเราก็ไม่ควรลืมว่าอะไรคือเรื่องแต่งและอะไรคือ “เรื่องจริง”
The Creation of Adam (ขอบคุณภาพจาก Wikipedia)
Chapter 5: The Odd Couple สองหลักการที่ขัดแย้งกันมาตลอดในยุคสมัยใหม่ก็คือ “ศาสนา” กับ “วิทยาศาสตร์” ที่ต่างก็พยายามอธิบายถึง “ความจริง” หนึ่งเดียวของโลกมนุษย์
ศาสนาต่างๆ (รวมถึงลัทธิคอมมิวนิสต์ ทุนนิยมและความเชื่อเรื่องความเท่าเทียมกันของมนุษย์) นั้นล้วนมีพื้นฐานมาจาก “กฎเกณฑ์” ที่มนุษย์สร้างขึ้นโดยอาศัยการเรื่องเล่าที่ว่าอันแท้จริงแล้วกฎเกณฑ์เหล่านี้ถูกกำหนดโดยเทพเจ้าหรือเกิดขึ้นตามกฏของธรรมชาติซึ่งมนุษย์นั้นไม่มีความสามารถที่จะเปลี่ยนแปลงระเบียบเหล่านี้ได้ (ฮิตเลอร์คิดว่าตัวเองต้องจำใจเป็นผู้สังหารชาวยิวให้พ้นโลกจากความเชื่อที่ว่าชาวยิวเป็นกลุ่มมนุษย์ที่มียีนส์ชั้นต่ำและจำเป็นต้องถูกกำจัดเพื่อรักษามนุษยชาติตามกฎแห่งธรรมชาติที่ชาวนาซีเชื่อถือในช่วงเวลานั้น) โดยเป้าหมายสูงสุดของศาสนานั้นคือการสร้าง “ระเบียบ” ให้กับสังคมมนุษย์ ซึ่งผู้ที่นับถือศาสนาหรือเชื่อมั่นในระบบกฎเกณฑ์นั้นจะมีความเชื่อว่าหลักการที่พวกเขาเชื่อมั่นนั้นคือ “สิ่งเดียวที่ถูกต้อง” อันหมายความว่าความเชื่อของผู้ที่นับถือศาสนาอื่นนั้นเป็นเพียงเรื่องเพ้อฝันที่ไร้เหตุผลสิ้นดี
ปัญหาของความขัดแย้งระหว่างศาสนาและวิทยาศาสตร์นั้นจึงเกิดขึ้นจาก “ความขัดแย้งกันของความจริง” ซึ่งวิทยาศาสตร์กำลังมีบทบาทในการปฏิเสธความจริงของศาสนาและความเชื่อต่างๆด้วยหลักฐานที่ชัดเจน แต่อย่างไรก็ตาม สังคมมนุษย์นั้นไม่สามารถพึงพาแต่หลักการทางวิทยาศาสตร์ในการปกครองมนุษย์จำนวนมหาศาลให้อยู่ร่วมกันอย่างสงบสุขได้ สังคมยังต้องการ “หลักการทางจริยธรรม” จากศาสนาที่คอยกำหนดว่าอะไรคือสิ่งที่ถูกต้องและอะไรคือสิ่งที่ผิด (ซึ่งแน่นอนว่าปัญหาที่ตามมาก็คือความขัดแย้งทางความเชื่อระหว่างศาสนาที่ไม่ตรงกัน อาทิ ความเชื่อเรื่องการทำแท้งที่ศาสนาคริสต์มองว่าเป็นเรื่องที่ผิดแต่ชาวเสรีนิยมกลับมองว่าเป็นสิ่งที่มนุษย์สามารถกระทำได้ ทั้งนี้หลักการทางวิทยาศาสตร์นั้นสามารถตอบได้เพียงว่าทารกเริ่มได้รับความรู้สึกเจ็บปวดเมื่ออายุเท่าไหร่ แต่ไม่สามารถบอกได้ว่าการคร่าชีวิตทารกในครรภ์นั้นคือสิ่งที่ถูกต้องหรือไม่)
ศาสนาและวิทยาศาสตร์ในอดีตจึงมีความสัมพันธุ์ที่ต้องพึ่งพาอาศัยกันมาโดยตลอด
Chapter 6: The Modern Covenant “ความทันสมัย (modernist)” นั้นเกิดขึ้นจากการที่มนุษย์ยอมละทิ้ง “ความหมายของชีวิต” จากความเชื่อทางศาสนาที่คอยสร้างกรอบให้มนุษย์ทำตามคำสั่งของพระเจ้าหรือกฎของธรรมชาติไปเป็นการออกตามหา “พลังอำนาจ” อันไร้ซึ่งขอบเขตซึ่งได้รับการสนับสนุนโดยความก้าวหน้าทาง “วิทยาศาสตร์” และอัตราการ “เติบโต” ของระบบเศรษฐกิจสมัยใหม่อย่างก้าวกระโดด
การปฏิวัติทางวิทยาศาสตร์นั้นมีจุดกำเนิดจากการเปลี่ยนแปลงความคิดของมนุษย์ที่แต่เดิมเชื่อมั่นว่าตัวเองค้นพบทรัพยากรทั้งหมดของโลกแล้วซึ่งหมายความว่าการที่มนุษย์จะมีฐานะที่ดีขึ้นได้นั้นจะต้องแลกเปลี่ยนด้วยการถดถอยลงของมนุษย์อีกคน (zero-sum game) กลายมาเป็นความเชื่อที่ว่ามนุษย์สามารถสร้างอัตราการเติบโตของทรัพยากรและพลังงานที่มีอยู่อย่างจำกัดบนโลกได้ด้วยการใช้ “ความรู้” ในการพัฒนาวิทยาศาสตร์และเทคโนโลยีที่สามารถเพิ่มประสิทธิภาพของมนุษยชาติได้อย่างที่ไม่เคยเกิดขึ้นมาก่อน
มนุษย์ในปัจจุบันจึงกลายเป็นสิ่งมีชีวิตที่เสพติด “อัตราการเติบโต” ของเศรษฐกิจที่คอยช่วยให้ความเป็นอยู่ของพวกเราดีขึ้นไปเรื่อยๆ ซึ่งแน่นอนว่าปัญหาที่ตามมาอย่าง “มลพิษ” และ “ภาวะโลกร้อน” นั้นก็จะส่งผลที่รุนแรงมากขึ้นเรื่อยๆ (หลักฐานที่แสดงให้เห็นว่ามนุษย์สนใจการเติบโตมากกว่าสิ่งแวดล้อมที่ชัดเจนที่สุดคืออัตราการปล่อยก๊าซคาร์บอนไดออกไซด์ที่มีแต่จะเพิ่มขึ้นเรื่อยๆถึงแม้ว่าจะมีการทำสัญญาระหว่างประเทศกันหลายรอบแล้วก็ตาม)
Chapter 7: The Humanist Revolution เหตุใดสังคมของมนุษย์สมัยใหม่ในยุคที่ปราศจากความเชื่อทางศาสนาถึงยังคงอยู่ร่วมกันได้อย่างสงบสุข คำตอบก็คือ มนุษย์ได้คิดค้นศาสนาชนิดใหม่ที่มีชื่อว่า “มนุษยนิยม (humanism)” ขึ้นมาในช่วงไม่กี่ทศวรรษที่ผ่านมา
มนุษยนิยมเชื่อมั่นในพลังของ “มนุษย์” และยอมรับให้มนุษย์ทำหน้าที่แทนพระเจ้าหรือกฏแห่งธรรมชาติในการสร้าง “ความหมาย” ให้กับโลกและจักรวาล อันเป็นเหตุให้การตัดสินใจทั้งหมดของมนุษย์นั้นเกิดขึ้นจากการถาม “ความรู้สึก” ของตัวเองว่าสิ่งนั้นเป็นสิ่งที่ควรกระทำหรือไม่โดยไม่ต้องยึดถือคัมภีร์ไบเบิ้ลหรือกฎข้อบังคับของศาสนาอื่นๆอีกต่อไป (มนุษย์เพียงแค่ถามตัวเองลึกๆว่าการกระทำนั้นทำให้เราและผู้อื่นรู้สึกดีหรือไม่ ธุรกิจก็แค่สร้างสินค้าหรือบริการที่เป็นที่ต้องการของลูกค้าเท่านั้นเพราะหลักการทางเศรษฐกิจของมนุษยนิยมก็คือ “ลูกค้าหรือมนุษย์นั้นถูกต้องเสมอ” อันเป็นเหตุให้ธุรกิจสีเทาหรือธุรกิจที่ส่งผลกระทบต่อสิ่งแวดล้อมยังคงดำเนินกิจการได้ในปัจจุบันเพราะมนุษย์ผู้เป็นลูกค้าไม่ได้มองว่าธุรกิจเหล่านั้นเป็นสิ่งที่ผิด !!)
สมการที่อธิบายกลไกของมนุษยนิยมนั้นได้แก่ Knowledge = Experience x Sensitivity ซึ่งมีความหมายว่า “กระบวนการทางความคิด” ของมนุษย์นั้นได้รับอิทธิพลจากการทำงานร่วมกันของ “ประสบการณ์” ที่ประกอบไปด้วย สัมผัส อารมณ์และความคิดที่ถูกสั่งสมมาในมนุษย์แต่ละคนและ “ความสามารถในการประมวลผล” ของประสบการณ์เหล่านั้น ซึ่งหมายความว่ามนุษย์มีการ “พัฒนาการทางความคิด” อยู่ตลอดเวลา ยกตัวอย่างเช่น ผู้เชี่ยวชาญการดื่มชาที่ผ่านประสบการณ์การชิมชามาแล้วทั่วโลกจะมีความสามารถในการรับรู้คุณค่าของชาชั้นดีได้มากกว่าผู้ที่ไร้ประสบการณ์ (สมการการสร้างความรู้ตามหลักการทางวิทยาศาสตร์ Knowledge = Empirical Data x Mathematics นั้นไม่สามารถตอบปัญหาทางจริยธรรมได้ ส่วนสมการของศาสนา Knowledge = Scriptures x Logic ก็ถูกจำกัดด้วยหลักคำสอนของศาสนา)
มนุษยนิยมนั้นแบ่งออกได้เป็น 3 สายหลักๆ ได้แก่
 เสรีนิยม (Liberalism) ผู้เชื่อมั่นในความสามารถของมนุษย์ “แต่ละคน” ในการดำเนินชีวิตของตัวเอง ดังนั้น เสรีภาพในการตัดสินใจของมนุษย์จึงเป็นสิ่งสำคัญ (ข้อเสียของระบบเสรีนิยมนั้นก็คือ “ความไร้ประสิทธิภาพ” ที่เกิดขึ้นจากการใช้เสียงส่วนใหญ่ตามระบอบประชาธิปไตยเป็นตัวตัดสินใจแทนสมาชิกของสังคมซึ่งบางส่วนอาจไม่พอใจกับการตัดสินใจนั้นๆ) สังคมนิยม (Socialism) ผู้เชื่อมั่นในความสามารถของ “สังคมมนุษย์” ในภาพรวมโดยมีกลุ่มแกนนำเป็นผู้ดูแลการตัดสินใจแทนสมาชิกทุกคน โดยอ้างถึงความชอบธรรมในการแก้ปัญหาความไม่เท่าเทียมกันของระบบเสรีนิยม (ซึ่งต่อมาประเทศเสรีนิยมก็ได้นำเอาหลักการบางส่วนของสังคมนิยมมาปรับใช้ อาทิ การสนับสนุนทางการศึกษาและสาธารณสุขของประชาชน) มนุษยนิยมเชิงวิวัฒนาการ (Evolutionary humanism) ผู้เชื่อมั่นในความสามารถของชาติพันธุ์ของตัวเองว่ามีคุณสมบัติที่สูงส่งกว่าชาติพันธุ์อื่นๆ อันเป็นเหตุให้เกิดการ “กำจัด” มนุษย์สายพันธุ์ที่ด้อยกว่า อาทิ Nazism ที่เชื่อมั่นในความสามารถของชาติพันธ์ุอารยันที่สูงส่งกว่าชาวยิว  หลังจากการสิ้นสุดลงของสงครามเย็น มนุษยนิยมสายเสรีนิยมคือ “ศาสนา” ที่ประสบความสำเร็จสูงสุดในสังคมของมนุษย์ในปัจจุบัน อันเป็นผลมาจากความสามารถในการปรับตัวของหลักการให้เข้ากับการเปลี่ยนแปลงทางเทคโนโลยีในยุคปัจจุบัน (กลุ่มอิสลามหัวรุนแรงกำลังจะไม่มีจุดยืนในเร็วๆนี้เนื่องจากการไม่ยอมปรับตัวเข้ากับเทคโนโลยีสมัยใหม่) ซึ่งหมายความว่าเมื่อเทคโนโลยีของมนุษย์มีการพัฒนาการไปเรื่อยๆ ศาสนาแห่งใหม่ที่สามารถตอบสนองความคิดและอารมณ์ของมนุษย์ในยุคแห่งอนาคตก็อาจจะเข้ามาแทนที่มนุษยนิยมก็เป็นได้
Part III – Homo Sapiens Loses Control “Organisms are algorithms and life is data processing”
Chapter 8: The Time Bomb in the Laboratory ในยุคปัจจุบันที่วิทยาศาสตร์เข้ามามีบทบาทในสังคมมากขึ้นเรื่อยๆ “ข้อเท็จจริง” ตามหลักการของเสรีนิยมกำลังที่ว่าด้วยการให้ความสำคัญของ “อิสรภาพทางความคิดของมนุษย์” กำลังได้รับการทดสอบครั้งใหญ่
วิทยาศาสตร์พิสูจน์ให้เห็นแล้วว่ากระบวนการตัดสินใจของมนุษย์นั้นเกิดขึ้นจาก “ปฏิกิริยาเคมี” ภายในร่างกาย ซึ่งถึงแม้ว่ามนุษย์จะอ้างว่าพวกเรามีอิสรภาพในการตัดสินใจด้วยตัวเอง แต่พวกเราก็ได้ถูกกระบวนการทางเคมีตั้ง “กรอบ” ให้กับคำถามและความคิดของพวกเราไม่ต่างกับกระรอกที่สามารถตัดสินใจด้วยตัวเองได้ว่ามันจะเลือกกินวอลนัทที่หล่นอยู่ใต้ต้นไม้หรือไม่โดยไม่เคยต้องถามตัวเองเลยว่าทำไมต้องคิดถึง “วอลนัท” (มนุษย์สามารถตัดสินใจคำตอบของคำถามในหัวของตัวเองได้ แต่อะไรหละคือตัวกำหนดให้พวกเรา “ถาม” คำถามเหล่านั้น) เราไม่ได้เลือกความต้องการของตัวเอง สิ่งที่เกิดขึ้นจริงในระบบประสาทก็คือ เรา “รับรู้” ถึงความต้องการเหล่านั้นที่ไหลผ่านมาในสมองเรา ณ จังหวะเวลานั้นพอดีต่างหาก (เราสามารถทดสอบทฤษฎีนี้ง่ายๆด้วยการตั้งคำถามตัวเองหลังจากที่ “ความคิด” บางอย่างผุดขึ้นมาในใจว่าความคิดเหล่านั้นมันเกิดขึ้นมาได้ยังไงและเราสามารถสั่งสมองให้ “หยุดคิด” ได้หรือไม่)
สิ่งที่ตามมาก็คือ กระบวนการ “ปรับแต่งความรู้สึก” ของมนุษย์ผ่านเทคโนโลยีสมัยใหม่ ที่ปัจจุบันนักวิทยาศาสตร์สามารถสั่งการ “หนูทดลอง” ให้ปฏิบัติภารกิจตามคำสั่งผ่านการฝังเครื่องมือที่คอยกระตุ้นสมองส่วนที่สร้างความสุขให้กับหนูควบคู่กับการฝึกระยะสั้นโดยหนูทดลองที่ร่วมโครงการนั้นจะมีความรู้สึกดีทุกครั้งหลังจากได้กระทำสิ่งที่พวกมันถูกสั่งให้ทำผ่านการกระตุ้นสมอง ซึ่งเทคโนโลยีสำหรับมนุษย์ก็เริ่มได้รับการพัฒนาให้มีประสิทธิภาพมากขึ้นแล้ว อาทิ ไมโครชิปส์สำหรับแก้โรคซึมเศร้าและหมวกสัญญาณแม่เหล็กไฟฟ้าสำหรับกระตุ้นสมองของทหารสหรัฐเพื่อเพิ่มสมรรถภาพในสนามรบ
หลักการของเสรีนิยมที่พูดถึง “ตัวตน” ที่มีเพียงหนึ่งเดียวของมนุษย์แต่ละคน (individual) นั้นก็ได้รับการโต้แย้งอย่างรุนแรงจากทฤษฎีของศาสตร์เศรษฐศาสตร์เชิงพฤติกรรม (behavioral economics) ที่พิสูจน์ว่ามนุษย์หนึ่งคนนั้นมีระบบการตัดสินใจอยู่ 2 ระบบ อันได้แก่ ตัวตนเชิงประสบการณ์ (experience self) ผู้รับรู้ถึงสิ่งที่มนุษย์ได้ประสบพบเจอและตัวตนเชิงบอกเล่า (narrative self) ผู้ทำหน้าที่บอกเล่าเรื่องราวเหล่านั้น โดยการทดลองได้ค้นพบว่าตัวตนเชิงบอกเล่านั้นมักสร้างเรื่องราวที่ไม่ได้สอดรับกับตัวตนเชิงประสบการณ์และตัวตนเชิงบอกเล่านี้เองคือตัวตนหลักที่ทำหน้าที่ชี้นำการตัดสินใจของมนุษย์ (ตัวอย่างการทดลอง: การทดลองจุ่มมือในน้ำเย็นจัดที่ผู้ทดลองจะต้องทำการจุ่มมือใน 2 รูปแบบ รูปแบบแรกผู้ทดลองจะต้องจุ่มมือในน้ำเย็นจัดเป็นระยะเวลาหนึ่ง รูปแบบที่สองนั้นเหมือนกับรูปแบบแรกแต่ผู้ทดลองจะต้องจุ่มมือนานขึ้นอีกในขณะที่น้ำจะอุ่นขึ้นเล็กน้อย ผลของการทดลองพบว่าผู้ทดลองที่ได้รับอิทธิพลจากตัวตนเชิงบอกเล่าชอบการจุ่มมือรูปแบบที่สองมากกว่าทั้งๆที่ตัวตนเชิงประสบการณ์ของพวกเขาต้องทนทุกข์เป็นระยะเวลานานกว่า ผลลัพธ์ของการทดลองนี้นำไปสู่กฎ peak-end rule ที่กล่าวว่ามนุษย์มักจะทำการเฉลี่ยความทรมานหรือความสุขของประสบการณ์ที่พวกเขาพบเจอมากกว่าการรวมผลลัพธ์จากเหตุการณ์เหล่านั้น)
Chapter 9: The Great Decoupling ในศตวรรษที่ 21 มนุษยชาติกำลังจะต้องประสบกับการคุกคามของ “เทคโนโลยี” ที่กำลังจะทำให้มนุษย์จำนวนมากต้องสูญเสีย “คุณค่า” ทางเศรษฐกิจและการทหารไปอย่างหมดสิ้นอันเป็นเสมือน “ชนวนระเบิด” ที่พร้อมจะทำลายความเชื่อของหลักการทางเสรีนิยมที่ให้คุณค่าแก่มนุษย์ทุกๆคนอย่างเท่าเทียมกัน ตัวอย่างเช่น IBM’s Watson ในอนาคตที่สามารถทำหน้าที่แทนคุณหมอทั่วโลกในการวินิจฉัยโรคร้ายของผู้ป่วยพร้อมๆกันทั้งโลกได้อย่างแม่นยำตลอด 24 ชั่วโมง การจู่โจมทางโลกไซเบอร์โดยกลุ่มแฮกเกอร์เพียงหยิบมือสามารถส่งผลร้ายแรงต่อเป้าหมายได้อย่างรวดเร็วและมีประสิทธิภาพกว่าการใช้ทหารนับล้านนายในการสู้รบ
ในอดีตมนุษย์มีความเชื่อว่าสิ่งมีชีวิตที่มีความสามารถในการตระหนักรู้ถึงอารมณ์และความรู้สึกของตัวเองเท่านั้นที่จะสามารถสร้างภูมิปัญญาที่สูงส่งได้ แต่แล้ว เทคโนโลยีทางคอมพิวเตอร์และหุ่นยนต์ในปัจจุบันนั้นได้แสดงให้เห็นถึง “การแยกออกจากกัน” ของ “ปัญญา (Intelligence)” และ “การตระหนักรู้ (consciousness)” ที่ในอนาคต algorithm ที่มีระดับสติปัญญาขั้นสูงจะเข้ามาทำหน้าที่แทนมนุษย์ผู้มีความตระหนักรู้แต่มีระบบประมวลผลที่ด้อยประสิทธิภาพกว่าโดยที่ algorithm เหล่านั้นไม่จำเป็นต้องมีความรู้สึกนึกคิดแต่อย่างใด ไม่แตกต่างจากเหตุการณ์ในอดีตที่ “ม้า” ซึ่งมีความรู้สึกและความผูกพันธุ์กับเจ้าของถูกแทนที่ด้วย “รถยนต์” อันไร้ความรู้สึกที่มีประสิทธิภาพในการพามนุษย์เดินทางจากจุดหนึ่งไปยังอีกจุดหนึ่งที่ดีกว่าได้ภายในระยะเวลาอันรวดเร็ว
คำถามสำคัญที่ตามมาก็คือ “แล้วที่ยืนของมนุษย์นั้นอยู่ตรงไหน” คำตอบของนักวิชาการส่วนใหญ่ในยุคปัจจุบันก็คือ งานที่ต้องใช้ความสามารถในการตระหนักรู้ถึงอารมณ์และความคิดสร้างสรรค์ของมนุษย์ แต่เอาเข้าจริงๆแล้ว สิ่งมีชีวิตทุกชนิดรวมถึงมนุษย์นั้นล้วนมี algorithm สำหรับการตัดสินใจทั้งนั้น งานที่ต้องใช้ความคิดสร้างสรรค์อย่างการแต่งเพลงนั้นตามหลักการของ Life science นั้นถือเป็นกระบวนการในการประมวลผลรูปแบบของเสียงตามหลักการทางคณิตศาสตร์ของมนุษย์เท่านั้น ซึ่งนักประดิษฐ์คนหนึ่งสามารถพัฒนา A.I. นามว่า EMI ที่สามารถประมวลผลเพลงของศิลปินชื่อดังอย่าง Bach มาใช้ในการแต่งเพลงใหม่ของตัวเองกว่า 5,000 เพลงภายใน 1 วันโดยที่ผู้ฟังนั้นเชื่อว่าเพลงเหล่านี้ถูกแต่งขึ้นมาโดยศิลปินเอกได้อย่างสนิทใจ โลกในอนาคตกำลังจะสร้างประชากรกลุ่มใหม่ที่ไร้ซึ่งคุณค่าใดๆต่อสังคม (useless class) ส่วนกลุ่มงานที่อาจได้รับผลกระทบน้อยที่สุดนั้นคือกลุ่มงานที่ต้องใช้ความสามารถเฉพาะตัวขั้นสูงแต่มีผลตอบแทนที่ต่ำจนไม่มีใครอยากลงทุนพัฒนาโปรแกรมมาทำหน้าที่ทดแทน อาทิ นักโบราณคดี
แนวโน้มถัดมาของศตวรรษที่ 21 ก็คือ การเกิดขึ้นของระบบการตัดสินใจที่หลอมรวมมนุษย์เข้าด้วยกันเป็นกลุ่ม ศาสตร์ Life science ได้พิสูจน์ว่าทุกกระบวนการตัดสินใจของมนุษย์นั้นเกิดขึ้นจาก algorithm ภายในของมนุษย์แต่ละคนที่เต็มไปด้วยข้อบกพร่องนานับประการ และเมื่อถึงจุดหนึ่งที่เทคโนโลยีสามารถทำความเข้าใจหลักการของ algorithm เหล่านั้นมากพอที่จะสร้าง external algorithm ที่มีประสิทธิภาพมากกว่าได้สำเร็จ ความสำคัญของปัจเจกบุคคลก็จะสูญสลายไป มนุษย์จะถูกหลอมรวมเป็นกลุ่มก้อนและถูกถ่ายเทพลังอำนาจไปยัง algorithm ที่สามารถตัดสินใจสิ่งต่างๆได้ดีกว่า ตัวอย่างที่เห็นชัดๆในปัจจุบันก็คือ เทคโนโลยีทางการแพทย์อย่าง wearable sensor ที่สามารถตรวจจับกระบวนการทำงานของร่างกายและให้คำแนะนำแก่ผู้ใช้งานอยู่ตลอดเวลา หรือ เหตุการณ์ที่ Angelina Jolie ตัดสินใจผ่าตัดเต้านมหลังรับทราบข้อมูลทางสถิติจากการตรวจสอบทางพันธุกรรมว่าตัวของเธอมีโอกาส 87% ที่จะเป็นมะเร็งเต้านม
ในอนาคต “มนุษย์” อาจจะต้องพึ่งพิง “โปรแกรม” อย่างแยกจากกันไม่ได้ตลอด 24 ชั่วโมง ลองจินตนาการโลกในอนาคตที่มนุษย์ทุกคนยอมให้ข้อมูลทั้งหมดแก่ Google ไม่ว่าจะเป็นข้อมูลทางชีววิทยา (อาทิ ข้อมูลการทำงานของร่างกายแบบ real time ผ่านการฝังไมโครเซนเซอร์ DNA และประวัติทางพันธุกรรม) ข้อมูลการใช้จ่าย อีเมล์และอีกมากมาย เมื่อข้อมูลมีมากพอถึงจุดหนึ่ง Google จะกลายมาเป็น algorithm ที่รู้จักมนุษย์มากกว่าตัวของพวกเขาเองและสามารถให้คำแนะนำในการตัดสินใจได้ดีกว่ามนุษย์ที่เต็มไปด้วยความลำเอียง (bias) และความไร้ประสิทธิภาพในการประมวลผลข้อมูลของตัวเอง
และเมื่อมนุษย์สามารถอัพเกรดร่างกายและสติปัญญาของตัวเองได้สำเร็จ โลกก็จะเริ่มเข้าสู่ยุคที่ความไม่เท่าเทียมกันระหว่างชนชั้นสูงกับชนชั้นล่างนั้นรุนแรงอย่างที่ไม่เคยเกิดขึ้นมาก่อน ความแตกต่างนั้นไม่ได้เกิดขึ้นเฉพาะฐานะทางเศรษฐกิจเท่านั้น แต่กลุ่มชนชั้นสูงนั้นจะกลายมาเป็นสิ่งมีชีวิตที่มีสมรรถภาพทางชีววิทยาที่เหนือกว่าและเมื่อเทคโนโลยีได้ทำให้ “ประโยชน์” ของกลุ่มชนชั้นล่างหมดไป ประชากรเกือบทั้งหมดของโลกมนุษย์อาจจะกลายมาเป็นเพียงขยะทางชีววิทยาที่ไร้คุณค่าใดๆในสายตาของกลุ่มมนุษย์ที่สามารถแปลงกายเป็นเทพเจ้าได้สำเร็จ
Lee Sedol vs. AlphaGo (ขอบคุณภาพจาก BGR India)
Chapter 10: The Ocean of Consciousness ศาสนาใหม่ของมนุษยชาติกำลังถือกำเนิดขึ้นในห้องทดลองทางวิทยาศาสตร์ที่มี Silicon Valley เป็นจุดศูนย์กลาง สิ่งที่สร้างความแตกต่างให้กับศาสนาแห่งใหม่นี้ก็คือความสามารถในการสร้างความสุข ความสงบหรือแม้กระทั่งชีวิตอันเป็นนิรันดร์ให้กับมนุษย์บนโลกใบนี้ (ไม่ใช่โลกหลังความตายเหมือนศาสนาอื่นๆ)
Techno-humanist คือ ศาสนาที่ต่อยอดจากมนุษยนิยมที่ยังคงมีความเชื่อมั่นในความสามารถของมนุษย์อยู่ แต่มนุษย์ตามความหมายของศาสนาใหม่นี้คือมนุษย์สายพันธุ์ Homo Deus ที่ได้รับการอัพเกรดจากเทคโนโลยีทางชีววิทยา นาโนและการเชื่อมต่อคอมพิวเตอร์เข้ากับระบบประสาท อันเป็นเหตุให้ Homo Deus มีความสามารถที่ยังคงสร้างคุณค่าในโลกที่เต็มไปด้วย algorithm ได้ (นักวิทยาศาสตร์เชื่อว่ามนุษย์สามารถขยายประสิทธิภาพของการใช้งานของสมองและจิตใจได้อีกมหาศาลโดยยกตัวอย่างศักยภาพที่มนุษย์ในปัจจุบันไม่สามารถทำได้ อาทิ ประสิทธิภาพในการดมกลิ่นของมนุษย์ในยุคล่าสัตว์ การอ่านคลื่นสะท้อนของค้างคาวและการตรวจจับเสียงที่อยู่ห่างออกไปหลายร้อยกิโลเมตรของปลาวาฬ)
แต่หลักการของ Techno-humanist ก็มีข้อจำกัด เมื่อมนุษย์สามารถอัพเกรดระบบการตัดสินใจของตัวเองจนสามารถกำจัดตัวตนที่ไร้เหตุผลหรือตัวตนที่ก่อให้เกิดความลำบากใจภายในจิตใจของพวกเขาได้ มนุษย์เวอร์ชั่นอัพเกรดเหล่านั้นก็จะมีระบบการตัดสินใจที่ไม่แตกต่างจาก algorithm อันเป็นจุดกำเนิดของศาสนาชนิดใหม่ที่มี “ข้อมูล” เป็นศูนย์กลาง
Chapter 11: The Data Religion Dataism (ข้อมูลนิยม) คือ ศาสนาที่เชื่อมั่นในประสิทธิภาพของ “ข้อมูล” อันเกิดขึ้นจากการผสมผสานความก้าวหน้าของศาสตร์ทางชีววิทยาและคอมพิวเตอร์ที่เชื่อมั่นว่า algorithm ของสิ่งมีชีวิตและคอมพิวเตอร์นั้นสามารถผสมผสานรวมกันได้
ประวัติศาสตร์ของศตวรรษที่ 20 แสดงให้เห็นอย่างชัดเจนว่าระบบการปกครองที่สามารถ “ประมวลผลข้อมูล (data processing)” ได้อย่างครอบคลุมมากกว่านั้นคือระบบการปกครองที่มีประสิทธิภาพและพลังอำนาจที่สูงที่สุด (ระบอบคอมมิวนิสต์ที่มีรัฐบาลกลางทำหน้าที่ประมวลผลข้อมูลแต่เพียงผู้เดียวไม่สามารถสู้กับกำลังของหน่วยประมวลผลข้อมูลจำนวนมหาศาลของระบบกระจายอำนาจตามหลักเสรีนิยมได้) แต่ในยุคปัจจุบันที่การเปลี่ยนแปลงทางเทคโนโลยีนั้นเกิดขึ้นอย่างรวดเร็วพร้อมๆกับการเพิ่มขึ้นของข้อมูลจำนวนมหาศาล ระบบการปกครองทุกรูปแบบในปัจจุบันไม่สามารถติดตามการเปลี่ยนแปลงได้อีกต่อไปอันเป็นเหตุให้โลกต้องการระบบและผู้ปกครองรูปแบบใหม่ที่มีประสิทธิภาพที่ดีกว่า
ประวัติศาสตร์ยังแสดงให้เห็นถึงวิวัฒนาการของกระบวนการประมวลผลข้อมูลของมนุษยชาติ ที่เริ่มตั้งแต่ การขยายจำนวนประชากร (หน่วยประมวลผล) อันก่อให้เกิดความหลากหลายของหน่วยประมวลผลเหล่านั้นที่กระจัดกระจายอยู่ทั่วโลก จนกระทั่งโลกในยุคเกษตรกรรมและอุตสาหกรรมที่หน่วยประมวลผลที่มีความหลากหลายได้กลับมารวมตัวกันเป็นกลุ่มที่ใหญ่ขึ้นและเชื่อมต่อกันได้อย่างมีประสิทธิภาพมากขึ้นเรื่อยๆ ซึ่งแสดงให้เห็นถึงแนวโน้มของการเกิดขึ้นของการเชื่อมต่อทางข้อมูลอย่างสมบูรณ์ของมนุษยชาติและทุกสรรพสิ่ง (Internet-of-all-thing)
อิสรภาพของข้อมูล (freedom of information) คือ “หัวใจ” สำคัญของ Dataism ที่เชื่อมั่นว่า “เมื่อมนุษย์ยินยอมเปิดเผยข้อมูลทั้งหมดให้กับระบบ algorithm หนึ่งเดียวของโลก ทุกกระบวนการตัดสินใจของมนุษย์จะถูกประมวลผลและตัดสินใจผ่านระบบประมวลผลแห่งนั้นอันนำมาซึ่งผลลัพธ์ที่ดีที่สุดให้กับมนุษย์และทุกสรรพสิ่ง” ตัวอย่างของการใช้ระบบข้อมูลมหาศาลในการเพิ่มประสิทธิภาพให้กับโลกมนุษย์นั้นได้แก่ ระบบแบ่งปันรถยนต์ไร้คนขับ (driverless carpooling) ที่เข้ามาขจัดปัญหาของความสิ้นเปลืองของการใช้ทรัพยากร “รถยนต์” ที่ใช้เวลามากกว่า 90% ในการจอดอยู่กับที่เฉยๆอย่างไร้ประโยชน์ หากมนุษย์ทุกคนยอมเปิดเผยข้อมูลตำแหน่งที่อยู่อาศัย ที่ทำงาน จุดหมายปลายทางและเวลาให้กับระบบ algorithm อย่างสมบูรณ์ ระบบประมวลผลนี้ก็จะสามารถจัดสรรการใช้ทรัพยากรรถยนต์แบบแบ่งปันให้กับผู้ที่ต้องการเดินทางด้วยรถยนต์ได้อย่างมีประสิทธิภาพและยังสามารถลดปริมาณรถยนต์ส่วนบุคคลในท้องถนนได้ถึง 20 เท่าพร้อมกับการลดลงของพื้นที่จอดรถอีกจำนวนมหาศาล
Dataism (ขอบคุณภาพจาก Financial Times)
แต่ถึงกระนั้น ทฤษฎีที่กล่าวมาทั้งหมดในหนังสือเล่มนี้เป็นเพียงแค่การพยากรณ์ที่อาศัยการศึกษาทางประวัติศาสตร์ของมนุษยชาติและการทำความเข้าใจเทคโนโลยีในยุคปัจจุบันเท่านั้น มนุษย์ในทุกวันนี้คงไม่มีทางมองเห็นและเข้าใจมนุษย์ในอีก 50 ปีข้างหน้าได้อย่างสมบูรณ์ แต่ 3 แนวโน้มที่กำลังเกิดขึ้นจริงที่ทุกคนควรจะต้องคำนึงถึงอยู่เสมอในการวางแผนอนาคตนั้นก็คือ
 สิ่งมีชีวิตทั้งหมดนั้นคือระบบประมวลผลที่ถูกผลักดันโดย algorithm “สติปัญญา” กับ “การตระหนักรู้” นั้นกำลังถูกแยกออกจากกัน มนุษย์ผู้มีอารมณ์ความรู้สึกกำลังจะถูกแทนที่ด้วย algorithm ที่มีสติปัญญาที่สูงกว่า algorithm กำลังจะมีความสามารถในการเข้าใจมนุษย์มากกว่าตัวของพวกเราเอง   Source :.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_1/homo_deus/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">leaf_bundle</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/homo_deus/"><img src="/topic_1/homo_deus/images/Homo_Deus_hud949c3cfa973b39c16ba4d2c8cf1d6ae_196421_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        [สรุปหนังสือ] Homo Deus : A Brief History of Tomorrow
      </h3>
      <p class="refresh-summary"> “This is the best reason to learn history: not in order to predict the future, but to free yourself of the past and imagine alternative destinies. Of course this is not total freedom – we cannot avoid being shaped by the past. But some freedom is better than none.”
เมื่อเราเข้าใจ “เรื่องราวที่แท้จริง” ของมนุษย์ เมื่อนั้นเราก็จะเข้าใจ “เป้าหมาย” ของการเดินทางของมนุษยชาติ
Homo Deus คือ หนังสือภาคต่อจาก Sapiens ของ Yuval Noah Harari ศาสตราจารย์ทางประวัติศาสตร์ผู้นำเสนอเรื่องราวของ “อนาคต” ของมนุษยชาติผ่านการศึกษาความเป็นมาทางประวัติศาสตร์ของเผ่าพันธุ์ Homo Sapiens ที่พัฒนาตนเองขึ้นจากการเป็น “สัตว์” อันไร้ซึ่งความสำคัญใดๆมาเป็น “เทพเจ้า” ผู้กำหนดชะตาชีวิตของทุกสรรพสิ่ง แต่เมื่อสิ่งที่ถูกมนุษย์สร้างขึ้นอย่าง algorithm เริ่มมีสติปัญญาที่ชาญฉลาดกว่ามันสมองของมนุษย์ โลกของเราทุกคนจะเปลี่ยนแปลงไปอย่างไร
Chapter 1: The New Human Agenda โศกนาฏกรรม 3 อันดับสำคัญที่มนุษยชาติต้องพบเจอตลอดระยะเวลาหลายพันปีที่ผ่านมานั้นประกอบไปด้วย “ความอดอยาก” “โรคระบาด” และ “สงคราม”
Famine: ในยุคเกษตรกรรม หากภัยธรรมชาติส่งผลกระทบต่อผลิตผลทางการเกษตรของหมู่บ้านแห่งหนึ่ง ความทุกข์ทรมานจากความอดอยากนั้นเป็นสิ่งที่แทบจะหลีกเลี่ยงไม่ได้เลย แต่ทุกวันนี้ ผลิตภาพทางการเกษตรและความเป็นอยู่ที่ดีขึ้นของมนุษย์ได้ทำให้ภาวะอดอยากหายสาบสูญไปจากโลกจนเกือบจะหมดสิ้นและมนุษย์กลับมีโอกาสเสียชีวิตจากภาวะการรับประทานอาหารที่ “มากเกินไป” แทน
Plague: เริ่มต้นในช่วงทศวรรษที่ 1330 ภัยร้ายนามว่า the Black Death ได้คร่าชีวิตมนุษย์กว่า 1 ใน 4 ของประชากรทั้งหมดในแถบทวีปยุโรปและเอเชีย ในยุคเริ่มต้นของการล่าอาณานิคมของชาวสเปนในแถบทวีปอเมริกาได้นำพาเชื้อโรคที่ชาวท้องถิ่นไม่เคยได้สัมผัสข้ามทวีปมาแพร่ระบาดไปทั่วอาณาจักร Aztec และ Maya อย่างรวดเร็วจนทำให้กว่า 90% ของประชากรท้องถิ่นต้องจบชีวิตลงภายในระยะเวลาอันสั้น แต่ในปัจจุบัน ภาวะโรคระบาดนั้นสร้างความเสียหายในอัตราที่น้อยลงเป็นอย่างมากทั้งๆที่สภาวะการอาศัยอยู่รวมกันในเมืองที่แออัดและระบบการคมนาคมอันรวดเร็วนั้นถือเป็นแหล่งแพร่กระจายโรคระบาดได้เป็นอย่างดี ทั้งนี้ก็เพราะว่าวิทยาการทางการแพทย์สมัยใหม่ได้รับการพัฒนาจนสามารถตรวจจับสาเหตุและควบคุมการแพร่ระบาดของโรคติดต่อได้อย่างรวดเร็ว (Ebola ที่เกิดขึ้นในปี 2014 คร่าประชากรไปเพียงแค่หลักหมื่นคนเท่านั้น) ความเสี่ยงของโรคระบาดร้ายแรงในอนาคตน่าจะเกิดขึ้นจากฝีมืของมนุษย์มากกว่าจากธรรมชาติ
War: ปัจจุบันสงครามและการก่ออาชญากรรมนั้นเป็นเพียงแค่ 1% ของสาเหตุการตายของมนุษย์ (ซึ่งน้อยกว่าการฆ่าตัวตายและโรคเบาหวาน) ทั้งนี้ก็เพราะว่าสงครามในยุคปัจจุบันนั้นแทบจะ “ไม่มีประโยชน์” ในความคุ้มค่าทางเศรษฐกิจอีกต่อไป การเกิดขึ้นของอาวุธ “นิวเคลียร์” ส่งผลให้การสู้รบกันระหว่างชาติมหาอำนาจเป็นเหมือนการ “ฆ่าตัวตายหมู่” ที่ไร้เหตุผลสิ้นดี ชัยชนะจากสงครามนั้นก็แทบจะไม่มีมูลค่าอีกต่อไปเพราะทรัพย์สมบัติที่มีค่าที่สุดของมนุษย์ในยุคนี้อยู่ใน “คอมพิวเตอร์” และ “มันสมอง” ของมนุษย์ (สงครามที่เกิดขึ้นในช่วงที่ผ่านมานั้นมักเกิดขึ้นกับประเทศที่มีทรัพยากรมหาศาล อาทิ บ่อน้ำมันหรือเหมืองแร่) ในทางกลับกัน “ความสงบสุขอันยั่งยืน” ก็ได้เกิดขึ้นมาแทนที่ “ความสงบสุบชั่วคราว” ผ่านการทำการค้าและการลงทุนระหว่างประเทศที่ทำให้ความคิดถึงการทำสงครามระหว่างกันนั้น “เป็นไปไม่ได้” อีกต่อไป และนี่คือสาเหตที่ประเทศจีนเลือกทำสัญญาทางธุรกิจกับ Apple และ Microsoft แทนการยกทัพมายึด Silicon Valley
เมื่อ “ความท้าทาย” ในอดีตได้รับการแก้ไขจนเกือบสมบูรณ์แล้ว มนุษย์ผู้มีความทะเยอทะยานสูงก็ได้เริ่มออกตามหา “ความท้าทาย” ครั้งใหม่อันประกอบไปด้วย ความเป็นอมตะ ความสุขและการก้าวข้ามขีดจำกัดของมนุษย์ธรรมดาไปเป็น “พระเจ้า”
Immortality: “ความตาย” ถือเป็นส่วนสำคัญอย่างมากของ “ศาสนา” ที่คอยพร่ำสอนให้มนุษย์ทำความดีเพื่อสร้าง “ชีวิตหลังความตาย” ที่ยอดเยี่ยม (ถ้าไม่มีความตาย แนวคิดของสวรรค์และนรกคงไม่เคยเกิดขึ้น) แต่ปัจจุบัน “ความตาย” ที่เกิดขึ้นกับมนุษย์ผ่าน “โรคร้าย” ต่างๆนั้นถูกตีความใหม่ให้กลายมาเป็นเพียงแค่ “ปัญหาทางเทคนิค” ของร่างกายมนุษย์ที่รอให้เทคโนโลยีทางการแพทย์แก้ไขปัญหาให้หายขาด ปัจจุบัน นักวิทยาศาสตร์และบริษัทเทคโนโลยีชั้นนำเริ่มให้ความสำคัญกับ Life Science หรือศาสตร์แห่งชีวิตที่ศึกษาเรื่องการรักษาความผิดปกติของร่างกายและการยืดอายุไขของเซลส์และอวัยวะต่างๆของมนุษย์ ในเร็วๆนี้ เราอาจจะเริ่มเห็นมนุษย์ที่มีอายุไขเพิ่มขึ้นจากเดิม (ซึ่งนั่นก็ทำให้โครงสร้างทางสังคมเปลี่ยนแปลงไปจากปัจจุบันโดยสิ้นเชิง ลองคิดดูว่ามนุษย์อาจจะเกษียณในอายุ 120 ปี ปูตินอาจจะเป็นนายกของรัสเซียต่อไปอีก 60 ปีก็ได้) ส่วนในระยะยาว มนุษย์อาจเริ่มกลายเป็น “a-mortal” หรือ ผู้ที่มีร่างกายเป็นอมตะ (ยกเว้น ได้รับความเสียหายอย่างรุนแรงจนเสียชีวิต ซึ่งก็อาจจะทำให้บุคคลอมตะเหล่านี้เกรงกลัวต่อกิจกรรมที่มีความเสี่ยงอย่างสุดโต่ง) ซึ่งแน่นอนว่า “มนุษย์” ทุกคนจะต้องพยายามทุกวิถีทางให้ตัวเองกลายเป็นมีชีวิตอยู่ชั่วนิรันดร์และเงินทุนมหาศาลจะเป็นตัวกระตุ้นให้การตามล่าความเป็นอมตะดำเนินต่อไปจนสัมฤทธิ์ผล
Happiness: ตั้งแต่การเริ่มต้นของการสร้างอาณาจักรและประเทศชาติ การพัฒนาโครงสร้างพื้นฐาน การศึกษา สุขอนามัยและการยกระดับความเป็นอยู่อาศัยของประชาชนนั้นมีวัตถุประสงค์หลักคือการสร้าง “ความแข็งแกร่ง” ให้กับประเทศชาติโดยปราศจากการคำนึงถึง “ความสุข” ที่แท้จริงของประชาชน (ประเทศพัฒนาแล้วมากมายมีอัตราการฆ่าตัวตายสูงขึ้นตามการเพิ่มขึ้นของ GDP per capita) การสร้างความสุขที่แท้จริงของมนุษย์นั้นต้องเริ่มต้นจากการแก้ไข “อุปสรรค” สำคัญ 2 ข้อ ได้แก่ 1. อุปสรรคทาง “จิตวิทยา” ที่ว่าด้วยความสุขของมนุษย์นั้นเกิดขึ้นจากการที่ความจริงที่พวกเขาต้องเผชิญนั้นตรงกับความคาดหวังของพวกเขามากน้อยแค่ไหน (ซึ่งปัจจุบัน ความก้าวหน้าทางเทคโนโลยีได้นำพาให้ความคาดหวังของมนุษย์สูงขึ้นๆไปเรื่อยๆ) 2. อุปสรรคทาง “ชีววิทยา” ที่พิสูจน์ไว้แล้วว่าความสุขของมนุษย์ขึ้นอยู่กับ “ฮอร์โมน” ภายในร่างกายเท่านั้น อันเป็นไปตามการวิวัฒนาการของธรรมชาติที่สร้างให้มนุษย์มีความรู้สึกดีหลังจากได้กระทำสิ่งที่เป็นประโยชน์ต่อการดำรงชีวิตได้สำเร็จก่อนที่ความรู้สึกนั้นจะจางหายไปในเวลาต่อมา เทคโนโลยีในอนาคตที่จะเข้ามาเติมเต็มช่องว่างก้อนใหญ่ของมนุษย์ก็คือการสร้าง “ความสุขชั่วนิรันดร์” ผ่านการ “จัดระเบียบ” ระบบการทำงานของสารเคมีในร่างกายมนุษย์ให้สามารถหลั่งฮอร์โมนให้มนุษย์มีความสุขในเชิงบวกได้ตลอดเวลา (ปัจจุบันมีมนุษย์หลายล้านคนกินยาระงับประสาท ยาแก้โรคซึมเศร้าและยาเสพย์ติด ที่ล้วนมีผลต่อการปรับเปลี่ยนสารเคมีที่มีผลต่ออารมณ์ความรู้สึกของมนุษย์ทั้งนั้น)
Divinity: หลังจากที่มนุษย์สามารถออกแบบร่างกายและจิตใจของตัวเองได้แล้ว เป้าหมายที่เหลือของพวกเขาก็คงเป็นการอัพเกรด “พลัง” ให้กับตัวเองให้เทียบชั้นกับ “เทพเจ้า” ตั้งแต่ การเสริมสร้างความสามารถของร่างกายผ่านการปรับเปลี่ยนพันธุกรรม การผสมผสานหุ่นยนต์เข้ากับร่างกายและสมองของมนุษย์ ไปจนถึงการอัพโหลดจิตใจของมนุษย์เข้าไปยังเครื่องจักรที่ไม่มีวันตาย ทั้งหมดนี้อาจจะดูเพ้อฝัน แต่สิ่งหนึ่งที่สามารถมั่นใจได้เลยก็คือ มนุษย์ในปัจจุบันคงไม่สามารถจินตนาการถึง “มนุษย์อนาคต” ที่ก้าวผ่านจากสายพันธุ์ Homo Sapiens (มนุษย์ฉลาด) มาเป็น Homo Deus (มนุษย์เทพเจ้า) ได้อย่างแน่นอน
ประวัติศาสตร์เป็นข้อพิสูจน์ชั้นดีว่าการ “อัพเกรดมนุษย์” นั้นเป็นสิ่งที่ไม่อาจหลีกเลี่ยงได้ในอนาคต เทคโนโลยีในช่วงเริ่มต้นนั้นอาจเกิดขึ้นในรูปแบบของกระบวนการแก้ไขความผิดปกติต่างๆของมนุษย์ (อาทิ การเลือกเอ็มบรีโอของเด็กทารกที่มีสุขภาพสมบูรณ์ หรือ การแก้ไขดีเอ็นเอที่มีปัญหาของเอ็มบรีโอ) แต่หากกระบวนการเหล่านั้นสามารถทำให้มนุษย์ปกติมีคุณสมบัติที่เพิ่มขึ้น สุดท้ายโลกจะไม่อาจควบคุมการแพร่กระจายของเทคโนโลยีนี้ได้ (หากอเมริกาสั่งแบนการตัดต่อพันธุกรรม แต่เกาหลีเหนือสนับสนุนโครงการนี้เต็มที่เพื่อสร้างมนุษย์อัจฉริยะ อเมริกาคงไม่มีทางยอมปล่อยโอกาสนี้ไปแน่ๆ)
แต่ก็อย่าลืมเด็ดขาดว่า “อนาคต” ที่พวกเราพอจะมองเห็นอยู่ลิบๆนั้นเกิดขึ้นจากการร้อยเรียงเรื่องราวและความเชื่อของมนุษย์ที่ถูกสร้างขึ้นโดยมนุษย์ตั้งแต่ในอดีตจนถึงปัจจุบัน นั่นหมายความว่าโลกและมนุษยชาติอาจจะไม่ใช่สิ่งที่พวกเราจินตนาการถึงได้เลยเมื่อ “อนาคต” ที่แท้จริงเดินทางมาถึง
Part I – Homo Sapiens Conquers the World Chapter 2: The Anthropocene การพยากรณ์ความสัมพันธุ์ระหว่าง Homo Sapiens ยุคปัจจุบันกับ Homo Deus ในยุคอนาคตนั้นคงเป็นสิ่งที่ไม่ง่ายนัก แต่เราก็อาจพอจะใช้ประวัติศาสตร์ของความสัมพันธุ์ระหว่าง “มนุษย์” กับ “สัตว์” ชนิดอื่นมาใช้เป็นกรอบความคิดคร่าวๆได้
Antropocene คือ “ยุคแห่งมนุษย์” ที่เริ่มต้นขึ้นเมื่อ 70,000 ปีก่อนหลังจากการปฏิวัติทางความตระหนักรู้ของมนุษย์สายพันธุ์ Homo Sapiens ที่ทำให้ “มนุษย์” กลายเป็นสิ่งมีชีวิตสายพันธุ์เดียวในโลกที่กุมชะตากรรมของสิ่งมีชีวิตทั้งหมด (ปัจจุบัน สัดส่วนน้ำหนักรวมของสิ่งมีชีวิตเกิน 90% ตกเป็นของมนุษย์ สัตว์เลี้ยงและสัตว์อุตสาหกรรม)
ความสัมพันธุ์ของ Homo Sapiens และสิ่งมีชีวิตชนิดอื่นๆในยุคของนักล่าสัตว์และนักหาของป่านั้นอยู่ในรูปของความเชื่อที่ว่าสิ่งมีชีวิตและไม่มีชีวิตทั้งหมดล้วนมีจิตวิญญาณ (Animism) ที่มนุษย์จะต้องทำความเคารพและเกื้อกูลระหว่างกัน (การล่าสัตว์ในยุคโบราณนั้นต้องประกอบด้วยพิธีขอขมาสัตว์ที่ถูกล่านั้นๆ) ก่อนที่การปฏิวัติทางเกษตรกรรมจะเปลี่ยนพฤติกรรมและความเชื่อของมนุษย์ให้กลายเป็นการเคารพต่อ “เทพเจ้า” ผู้มีพลังอำนาจมหาศาลที่มาพร้อมกับการเปลี่ยนความคิดของการนับถือสัตว์กลายเป็นการมองว่าสิ่งมีชีวิตชนิดอื่นๆนั้นมีศักดิ์ที่ต้อยต่ำกว่ามนุษย์อันเป็นเหตุให้ปศุสัตว์ผู้ถูกเลือกอย่าง หมู วัว ไก่และแกะต้องกลายมาเป็นสัตว์ที่ต้องทนทุกข์ทรมานมากที่สุดในโลก (พร้อมกับการแพร่กระจายจำนวนอย่างรวดเร็ว) ซึ่งการปฏิวัติทางวิทยาศาสตร์ก็ยิ่งเข้ามาซ้ำเติมความโหดร้ายของมนุษย์ที่มีต่อสัตว์เหล่านั้นเข้าไปอีก (ยกเว้นในช่วงไม่กี่ทศวรรษที่ผ่านมาที่มนุษย์เริ่มหวนกลับมาให้ความสำคัญกับสิ่งมีชีวิตชนิดอื่น)
ในอนาคตนั้น เราไม่อาจรับรู้ได้เลยว่า “มนุษย์เวอร์ชั่นอัพเกรด” หรือ A.I. ผู้มีสติปัญญาที่หลักแหลมกว่า Homo Sapiens ในปัจจุบันมากจะจัดการกับพวกเราเหมือนในอดีตที่ผ่านมาหรือไม่
Antropocene หรือ “ยุคแห่งมนุษย์” (ขอบคุณภาพจาก Motherboard – Vice)
Chapter 3: The Human Spark “อะไร” คือสิ่งที่ทำให้มนุษย์มีความพิเศษมากกว่าสิ่งมีชีวิตชนิดอื่น
หากคำถามนี้ถูกถามในยุคที่วิทยาศาสตร์ยังไม่ได้รรับการยอมรับอย่างกว้างขวาง คำตอบคงเป็น “วิญญาณ” ที่มีเพียงมนุษย์เท่านั้นที่ได้ครอบครอง อันเป็นเหตุให้มนุษย์มองเห็นสิ่งมีชีวิตอื่นๆเป็นเพียงร่างที่ไร้วิญญาณและสามารถย่ำยีได้ตามชอบ (ปัจจุบันชาวอเมริกันเพียงแค่ 15% เท่านั้นที่เชื่อว่ามนุษย์เกิดขึ้นจากการวิวัฒนาการตามธรรมชาติเท่านั้นโดยไม่ต้องพึ่งพระเจ้า)
ส่วนคำตอบที่น่าจะได้รับการยอมรับในยุคปัจจุบันมากกว่าก็คือ “จิตใจ” (mind) และ “การตระหนักรู้” (consciousness) ซึ่งปัจจุบัน เทคโนโลยีทางวิทยาศาสตร์สามารถตรวจจับกระบวนการทำงานของระบบประสาทภายในสมองของสิ่งมีชีวิตได้อย่างมีประสิทธิภาพในระดับหนึ่งแต่ก็ยังไม่สามารถตรวจจับกระบวนการทำงานของ “จิตใจ” ที่ทำให้มนุษย์รู้สึกรัก โลภ โกรธหรือกลัวได้เลย (มีเพียงตัวของเราเองเท่านั้นที่เชื่อมั่นในการมีอยู่ของจิตใจและความรู้สึกของเรา ไม่แน่เราอาจจะเป็นเพียงผู้เล่นในโลกจำลองของสิ่งมีชีวิตชั้นสูงหรือมนุษย์ในอนาคตอยู่ก็เป็นได้)
นักวิทยาศาสตร์หลายคนพยายามจับเอาคอนเส็ปต์ของระบบคอมพิวเตอร์อย่าง algorithm ที่เป็นเหมือนชุดคำสั่งขนาดใหญ่มาใช้อธิบายถึงพฤติกรรมของสิ่งมีชีวิตต่างๆแทนการใช้จิตใจ อาทิ เมื่อ ลิงเห็นเสือ ประสาทตาของลิงก็จะทำการส่งกระแสไฟฟ้าเข้าไปยังสมองเพื่อประมวลผลผ่าน algorithm ของลิงตัวนั้นและส่งผลลัพธ์ออกมาในรูปของกระแสประสาทเพื่อให้ลิงวิ่งหนีเสือ โดยที่ลิงไม่ได้มีความรู้สึกกลัวหรือตกใจแต่อย่างใดเลย
แต่เอาจริงๆแล้ว การทดลองหลายครั้งในอดีตก็ได้พิสูจน์ว่าสิ่งมีชีวิตชนิดอื่นๆก็มีความตระหนักรู้ได้ไม่แพ้มนุษย์ อาทิ Clever Hans ม้าผู้แสนฉลาดแห่งเยอรมนีในยุคปี 1900s ที่มีความสามารถในการตอบปัญหาบวกลบคูณหารเลขผ่านการเคาะเท้าเป็นจำนวนครั้งตามคำตอบที่ถูกต้องได้อย่างแม่นยำ ซึ่งนักจิตวิทยาได้ค้นพบว่า Clever Hans ไม่ได้มีความสามารถในการคิดเลขเหมือนกับมนุษย์ แต่เจ้าม้าตัวนี้ใช้วิธีการเคาะเท้าพร้อมๆกับการสังเกตสีหน้าของมนุษย์ผู้เป็นคนถามคำถามที่มักจะแสดงอาการอย่างชัดเจนเมื่อ Clever Hans กระแทกเท้าจนใกล้ถึงคำตอบที่ถูกต้องและมันก็จะหยุดกระแทกเท้าไปในที่สุด
แต่จากการศึกษาประวัติศาสตร์ของมนุษยชาติ “ปัจจัย” ที่น่าจะส่งผลให้ Homo Sapiens กลายมาเป็นสิ่งมีชีวิตที่ยิ่งใหญ่ที่สุดในโลกได้ภายในเวลาเพียงแค่เสี้ยวหนึ่งของอายุของดาวดวงนี้ ไม่ใช่ “วิญญาณ” หรือ “จิตใจอันสูงส่ง” แต่กลับกลายเป็น “ความสามารถในการทำงานร่วมกันอย่างซับซ้อนและมีประสิทธิภาพ” อย่างที่สิ่งมีชีวิตชนิดอื่นสามารถทำได้ ซึ่งสาเหตุที่มนุษย์สามารถทำงานร่วมกันเป็นกลุ่มขนาดหลักพันหลักล้านคนได้นั้นเกิดจากการที่มนุษย์สามารถคิดค้น “ความเชื่อที่ถูกสร้างขึ้นโดยฝีมือมนุษย์” (imagined order) อาทิ ศาสนา พระเจ้า หลักมนุษยธรรม ประชาธิปไตยและระบบเงิน
มนุษย์คือสิ่งมีชีวิตเดียวในโลกที่สามารถจินตนาการ “ความหมาย” ของการมีชีวิตอยู่และการทำงานร่วมกันระหว่างมนุษย์ด้วยกันแองได้โดยไม่จำเป็นต้องพึ่ง “ความจริง” ที่เกิดขึ้นตามหลักการทางวิทยาศาสตร์และธรรมชาติ อาทิ นักรบชาวคริสเตียนเชื่อมั่นว่าตัวเองจะได้ขึ้นสวรรค์หากเข้าร่วมสงครามครูเสดเพื่อคร่าชีวิตนักรบชาวอิสลามที่มีความเชื่อคล้ายๆกัน
และเมื่อการเวลาเปลี่ยนไป ความเชื่อและความหมายของชีวิตก็เปลี่ยนแปลงไป ปัจจุบันความเชื่อเกี่ยวกับเทพเจ้าและศาสนากำลังเสื่อมความนิยม ขณะที่ความเชื่อเรื่องประชาธิปไตย ความเท่าเทียมกันและสิทธิมนุษยชนกำลังได้ความนิยมที่เพิ่มขึ้นเรื่อยๆ ส่วนในอนาคต ความเชื่อรูปแบบใหม่ที่มนุษย์ในยุคปัจจุบันอาจจะยังคาดไม่ถึงก็อาจจะเกิดขึ้นได้ในไม่ช้า
ม้าแสนรู้ Clever Hans ที่ไม่ได้ฉลาดเหมือนที่มนุษย์คิด (ขอบคุณภาพจาก Wikipedia)
Part II – Homo Sapiens Gives Meaning to the World Chapter 4: The Storytellers มนุษย์คือสิ่งมีชีวิตชนิดเดียวที่อาศัยอยู่ในโลกที่มีความจริงซ้อนกันอยู่ 3 ชั้น (three-layered reality) อันประกอบไปด้วย ความจริงเชิงวัตถุ (objective reality) อาทิ อากาศและสิ่งแวดล้อม ความจริงเชิงปัจเจกบุคคล (subjective reality) อันได้แก่ อารมณ์และความรู้สึกภายใน และความจริงที่ถูกสร้างขึ้นโดยมนุษย์ด้วยกันเอง (imagined reality)
ประวัติศาสตร์ของมนุษย์นั้นถูกสร้างขึ้นจากการร้อยเรียงกันของเรื่องเล่าและความเชื่อของมนุษย์ ตั้งแต่ 70,000 ปีก่อนที่มนุษย์เริ่มสามารถจินตนาการถึง “สิ่งที่ไม่มีอยู่จริง” อันเป็นจุดเริ่มต้นของการสร้าง “ความจริง” โดยฝีมือของจินตนาการของมนุษย์เพื่อสร้างความสามารถในการทำงานร่วมกันของมนุษย์หมู่มากได้สำเร็จ
โดยจุดเปลี่ยนครั้งสำคัญของมนุษยชาติ คือ การปฏิวัติทางเกษตรกรรมเมื่อ 12,000 ปีก่อนที่ทำให้มนุษย์เริ่มเปลี่ยนพฤติกรรมจากการออกล่าสัตว์และหาของป่าไปเป็นการอยู่รวมกันเป็นหลักแหล่งเป็นหมู่บ้านขนาดย่อม ก่อนที่ “อาณาจักร” ขนาดใหญ่จะเริ่มกำเนิดขึ้นหลังจากที่ชาวสุเมเรียนได้คิดค้น “ภาษาเขียน” อันเป็นพื้นฐานของระบบบัญชีและการปกครองของมวลมนุษย์ขนาดใหญ่ได้สำเร็จในช่วง 5,000 ปีก่อน และภาษาเขียนนี่เองที่เป็นตัวจุดประกายให้เกิดการสร้าง “เรื่องแต่ง” ให้กลายมาเป็น “เรื่องจริง” ที่ทำให้ประชาชนในแต่ละอาณาจักรเชื่อมั่นได้อย่างสนิทใจได้ ซึ่งในช่วงแรกเริ่มของอาณาจักรนั้น เรื่องจริงที่ถูกสร้างขึ้นโดยฝีมือมนุษย์นั้นมีลักษณะคล้ายๆกันก็คือ การนำเอา “เทพเจ้า” หรือ “พระเจ้า” มาเป็นจุดศูนย์กลางของความเชื่อและกฎระเบียบการปกครองของอาณาจักรโดยแต่ละอาณาจักรจะมีผู้นำที่เปรียบเสมือน “ตัวแทนของเทพเจ้า” หรือไม่ก็เป็น “เทพเจ้า” ซะเองเลยอย่างฟาโรห์ของอาณาจักรอียิปต์
ถึงแม้ในยุคปัจจุบัน ความเชื่อเรื่องเทพเจ้าและภูติผีปีศาจจะได้เสื่อมถอยลงไป แต่โลกของเรากลับเต็มไปด้วย “เรื่องแต่ง” โดยฝีมือมนุษย์ในยุคใหม่มากมาย ไม่ว่าจะเป็น ประเทศชาติ หลักการปกครอง เงินและบริษัท ที่ล้วนแล้วแต่จะมีอิทธิพลต่อมนุษย์มากขึ้นเรื่อยๆ ซึ่งเอาจริงๆ เรื่องแต่งเหล่านี้ล้วนมีความสำคัญอย่างมากในการอยู่ร่วมกันของมนุษย์ในยุคปัจจุบัน แต่พวกเราก็ไม่ควรลืมว่าอะไรคือเรื่องแต่งและอะไรคือ “เรื่องจริง”
The Creation of Adam (ขอบคุณภาพจาก Wikipedia)
Chapter 5: The Odd Couple สองหลักการที่ขัดแย้งกันมาตลอดในยุคสมัยใหม่ก็คือ “ศาสนา” กับ “วิทยาศาสตร์” ที่ต่างก็พยายามอธิบายถึง “ความจริง” หนึ่งเดียวของโลกมนุษย์
ศาสนาต่างๆ (รวมถึงลัทธิคอมมิวนิสต์ ทุนนิยมและความเชื่อเรื่องความเท่าเทียมกันของมนุษย์) นั้นล้วนมีพื้นฐานมาจาก “กฎเกณฑ์” ที่มนุษย์สร้างขึ้นโดยอาศัยการเรื่องเล่าที่ว่าอันแท้จริงแล้วกฎเกณฑ์เหล่านี้ถูกกำหนดโดยเทพเจ้าหรือเกิดขึ้นตามกฏของธรรมชาติซึ่งมนุษย์นั้นไม่มีความสามารถที่จะเปลี่ยนแปลงระเบียบเหล่านี้ได้ (ฮิตเลอร์คิดว่าตัวเองต้องจำใจเป็นผู้สังหารชาวยิวให้พ้นโลกจากความเชื่อที่ว่าชาวยิวเป็นกลุ่มมนุษย์ที่มียีนส์ชั้นต่ำและจำเป็นต้องถูกกำจัดเพื่อรักษามนุษยชาติตามกฎแห่งธรรมชาติที่ชาวนาซีเชื่อถือในช่วงเวลานั้น) โดยเป้าหมายสูงสุดของศาสนานั้นคือการสร้าง “ระเบียบ” ให้กับสังคมมนุษย์ ซึ่งผู้ที่นับถือศาสนาหรือเชื่อมั่นในระบบกฎเกณฑ์นั้นจะมีความเชื่อว่าหลักการที่พวกเขาเชื่อมั่นนั้นคือ “สิ่งเดียวที่ถูกต้อง” อันหมายความว่าความเชื่อของผู้ที่นับถือศาสนาอื่นนั้นเป็นเพียงเรื่องเพ้อฝันที่ไร้เหตุผลสิ้นดี
ปัญหาของความขัดแย้งระหว่างศาสนาและวิทยาศาสตร์นั้นจึงเกิดขึ้นจาก “ความขัดแย้งกันของความจริง” ซึ่งวิทยาศาสตร์กำลังมีบทบาทในการปฏิเสธความจริงของศาสนาและความเชื่อต่างๆด้วยหลักฐานที่ชัดเจน แต่อย่างไรก็ตาม สังคมมนุษย์นั้นไม่สามารถพึงพาแต่หลักการทางวิทยาศาสตร์ในการปกครองมนุษย์จำนวนมหาศาลให้อยู่ร่วมกันอย่างสงบสุขได้ สังคมยังต้องการ “หลักการทางจริยธรรม” จากศาสนาที่คอยกำหนดว่าอะไรคือสิ่งที่ถูกต้องและอะไรคือสิ่งที่ผิด (ซึ่งแน่นอนว่าปัญหาที่ตามมาก็คือความขัดแย้งทางความเชื่อระหว่างศาสนาที่ไม่ตรงกัน อาทิ ความเชื่อเรื่องการทำแท้งที่ศาสนาคริสต์มองว่าเป็นเรื่องที่ผิดแต่ชาวเสรีนิยมกลับมองว่าเป็นสิ่งที่มนุษย์สามารถกระทำได้ ทั้งนี้หลักการทางวิทยาศาสตร์นั้นสามารถตอบได้เพียงว่าทารกเริ่มได้รับความรู้สึกเจ็บปวดเมื่ออายุเท่าไหร่ แต่ไม่สามารถบอกได้ว่าการคร่าชีวิตทารกในครรภ์นั้นคือสิ่งที่ถูกต้องหรือไม่)
ศาสนาและวิทยาศาสตร์ในอดีตจึงมีความสัมพันธุ์ที่ต้องพึ่งพาอาศัยกันมาโดยตลอด
Chapter 6: The Modern Covenant “ความทันสมัย (modernist)” นั้นเกิดขึ้นจากการที่มนุษย์ยอมละทิ้ง “ความหมายของชีวิต” จากความเชื่อทางศาสนาที่คอยสร้างกรอบให้มนุษย์ทำตามคำสั่งของพระเจ้าหรือกฎของธรรมชาติไปเป็นการออกตามหา “พลังอำนาจ” อันไร้ซึ่งขอบเขตซึ่งได้รับการสนับสนุนโดยความก้าวหน้าทาง “วิทยาศาสตร์” และอัตราการ “เติบโต” ของระบบเศรษฐกิจสมัยใหม่อย่างก้าวกระโดด
การปฏิวัติทางวิทยาศาสตร์นั้นมีจุดกำเนิดจากการเปลี่ยนแปลงความคิดของมนุษย์ที่แต่เดิมเชื่อมั่นว่าตัวเองค้นพบทรัพยากรทั้งหมดของโลกแล้วซึ่งหมายความว่าการที่มนุษย์จะมีฐานะที่ดีขึ้นได้นั้นจะต้องแลกเปลี่ยนด้วยการถดถอยลงของมนุษย์อีกคน (zero-sum game) กลายมาเป็นความเชื่อที่ว่ามนุษย์สามารถสร้างอัตราการเติบโตของทรัพยากรและพลังงานที่มีอยู่อย่างจำกัดบนโลกได้ด้วยการใช้ “ความรู้” ในการพัฒนาวิทยาศาสตร์และเทคโนโลยีที่สามารถเพิ่มประสิทธิภาพของมนุษยชาติได้อย่างที่ไม่เคยเกิดขึ้นมาก่อน
มนุษย์ในปัจจุบันจึงกลายเป็นสิ่งมีชีวิตที่เสพติด “อัตราการเติบโต” ของเศรษฐกิจที่คอยช่วยให้ความเป็นอยู่ของพวกเราดีขึ้นไปเรื่อยๆ ซึ่งแน่นอนว่าปัญหาที่ตามมาอย่าง “มลพิษ” และ “ภาวะโลกร้อน” นั้นก็จะส่งผลที่รุนแรงมากขึ้นเรื่อยๆ (หลักฐานที่แสดงให้เห็นว่ามนุษย์สนใจการเติบโตมากกว่าสิ่งแวดล้อมที่ชัดเจนที่สุดคืออัตราการปล่อยก๊าซคาร์บอนไดออกไซด์ที่มีแต่จะเพิ่มขึ้นเรื่อยๆถึงแม้ว่าจะมีการทำสัญญาระหว่างประเทศกันหลายรอบแล้วก็ตาม)
Chapter 7: The Humanist Revolution เหตุใดสังคมของมนุษย์สมัยใหม่ในยุคที่ปราศจากความเชื่อทางศาสนาถึงยังคงอยู่ร่วมกันได้อย่างสงบสุข คำตอบก็คือ มนุษย์ได้คิดค้นศาสนาชนิดใหม่ที่มีชื่อว่า “มนุษยนิยม (humanism)” ขึ้นมาในช่วงไม่กี่ทศวรรษที่ผ่านมา
มนุษยนิยมเชื่อมั่นในพลังของ “มนุษย์” และยอมรับให้มนุษย์ทำหน้าที่แทนพระเจ้าหรือกฏแห่งธรรมชาติในการสร้าง “ความหมาย” ให้กับโลกและจักรวาล อันเป็นเหตุให้การตัดสินใจทั้งหมดของมนุษย์นั้นเกิดขึ้นจากการถาม “ความรู้สึก” ของตัวเองว่าสิ่งนั้นเป็นสิ่งที่ควรกระทำหรือไม่โดยไม่ต้องยึดถือคัมภีร์ไบเบิ้ลหรือกฎข้อบังคับของศาสนาอื่นๆอีกต่อไป (มนุษย์เพียงแค่ถามตัวเองลึกๆว่าการกระทำนั้นทำให้เราและผู้อื่นรู้สึกดีหรือไม่ ธุรกิจก็แค่สร้างสินค้าหรือบริการที่เป็นที่ต้องการของลูกค้าเท่านั้นเพราะหลักการทางเศรษฐกิจของมนุษยนิยมก็คือ “ลูกค้าหรือมนุษย์นั้นถูกต้องเสมอ” อันเป็นเหตุให้ธุรกิจสีเทาหรือธุรกิจที่ส่งผลกระทบต่อสิ่งแวดล้อมยังคงดำเนินกิจการได้ในปัจจุบันเพราะมนุษย์ผู้เป็นลูกค้าไม่ได้มองว่าธุรกิจเหล่านั้นเป็นสิ่งที่ผิด !!)
สมการที่อธิบายกลไกของมนุษยนิยมนั้นได้แก่ Knowledge = Experience x Sensitivity ซึ่งมีความหมายว่า “กระบวนการทางความคิด” ของมนุษย์นั้นได้รับอิทธิพลจากการทำงานร่วมกันของ “ประสบการณ์” ที่ประกอบไปด้วย สัมผัส อารมณ์และความคิดที่ถูกสั่งสมมาในมนุษย์แต่ละคนและ “ความสามารถในการประมวลผล” ของประสบการณ์เหล่านั้น ซึ่งหมายความว่ามนุษย์มีการ “พัฒนาการทางความคิด” อยู่ตลอดเวลา ยกตัวอย่างเช่น ผู้เชี่ยวชาญการดื่มชาที่ผ่านประสบการณ์การชิมชามาแล้วทั่วโลกจะมีความสามารถในการรับรู้คุณค่าของชาชั้นดีได้มากกว่าผู้ที่ไร้ประสบการณ์ (สมการการสร้างความรู้ตามหลักการทางวิทยาศาสตร์ Knowledge = Empirical Data x Mathematics นั้นไม่สามารถตอบปัญหาทางจริยธรรมได้ ส่วนสมการของศาสนา Knowledge = Scriptures x Logic ก็ถูกจำกัดด้วยหลักคำสอนของศาสนา)
มนุษยนิยมนั้นแบ่งออกได้เป็น 3 สายหลักๆ ได้แก่
 เสรีนิยม (Liberalism) ผู้เชื่อมั่นในความสามารถของมนุษย์ “แต่ละคน” ในการดำเนินชีวิตของตัวเอง ดังนั้น เสรีภาพในการตัดสินใจของมนุษย์จึงเป็นสิ่งสำคัญ (ข้อเสียของระบบเสรีนิยมนั้นก็คือ “ความไร้ประสิทธิภาพ” ที่เกิดขึ้นจากการใช้เสียงส่วนใหญ่ตามระบอบประชาธิปไตยเป็นตัวตัดสินใจแทนสมาชิกของสังคมซึ่งบางส่วนอาจไม่พอใจกับการตัดสินใจนั้นๆ) สังคมนิยม (Socialism) ผู้เชื่อมั่นในความสามารถของ “สังคมมนุษย์” ในภาพรวมโดยมีกลุ่มแกนนำเป็นผู้ดูแลการตัดสินใจแทนสมาชิกทุกคน โดยอ้างถึงความชอบธรรมในการแก้ปัญหาความไม่เท่าเทียมกันของระบบเสรีนิยม (ซึ่งต่อมาประเทศเสรีนิยมก็ได้นำเอาหลักการบางส่วนของสังคมนิยมมาปรับใช้ อาทิ การสนับสนุนทางการศึกษาและสาธารณสุขของประชาชน) มนุษยนิยมเชิงวิวัฒนาการ (Evolutionary humanism) ผู้เชื่อมั่นในความสามารถของชาติพันธุ์ของตัวเองว่ามีคุณสมบัติที่สูงส่งกว่าชาติพันธุ์อื่นๆ อันเป็นเหตุให้เกิดการ “กำจัด” มนุษย์สายพันธุ์ที่ด้อยกว่า อาทิ Nazism ที่เชื่อมั่นในความสามารถของชาติพันธ์ุอารยันที่สูงส่งกว่าชาวยิว  หลังจากการสิ้นสุดลงของสงครามเย็น มนุษยนิยมสายเสรีนิยมคือ “ศาสนา” ที่ประสบความสำเร็จสูงสุดในสังคมของมนุษย์ในปัจจุบัน อันเป็นผลมาจากความสามารถในการปรับตัวของหลักการให้เข้ากับการเปลี่ยนแปลงทางเทคโนโลยีในยุคปัจจุบัน (กลุ่มอิสลามหัวรุนแรงกำลังจะไม่มีจุดยืนในเร็วๆนี้เนื่องจากการไม่ยอมปรับตัวเข้ากับเทคโนโลยีสมัยใหม่) ซึ่งหมายความว่าเมื่อเทคโนโลยีของมนุษย์มีการพัฒนาการไปเรื่อยๆ ศาสนาแห่งใหม่ที่สามารถตอบสนองความคิดและอารมณ์ของมนุษย์ในยุคแห่งอนาคตก็อาจจะเข้ามาแทนที่มนุษยนิยมก็เป็นได้
Part III – Homo Sapiens Loses Control “Organisms are algorithms and life is data processing”
Chapter 8: The Time Bomb in the Laboratory ในยุคปัจจุบันที่วิทยาศาสตร์เข้ามามีบทบาทในสังคมมากขึ้นเรื่อยๆ “ข้อเท็จจริง” ตามหลักการของเสรีนิยมกำลังที่ว่าด้วยการให้ความสำคัญของ “อิสรภาพทางความคิดของมนุษย์” กำลังได้รับการทดสอบครั้งใหญ่
วิทยาศาสตร์พิสูจน์ให้เห็นแล้วว่ากระบวนการตัดสินใจของมนุษย์นั้นเกิดขึ้นจาก “ปฏิกิริยาเคมี” ภายในร่างกาย ซึ่งถึงแม้ว่ามนุษย์จะอ้างว่าพวกเรามีอิสรภาพในการตัดสินใจด้วยตัวเอง แต่พวกเราก็ได้ถูกกระบวนการทางเคมีตั้ง “กรอบ” ให้กับคำถามและความคิดของพวกเราไม่ต่างกับกระรอกที่สามารถตัดสินใจด้วยตัวเองได้ว่ามันจะเลือกกินวอลนัทที่หล่นอยู่ใต้ต้นไม้หรือไม่โดยไม่เคยต้องถามตัวเองเลยว่าทำไมต้องคิดถึง “วอลนัท” (มนุษย์สามารถตัดสินใจคำตอบของคำถามในหัวของตัวเองได้ แต่อะไรหละคือตัวกำหนดให้พวกเรา “ถาม” คำถามเหล่านั้น) เราไม่ได้เลือกความต้องการของตัวเอง สิ่งที่เกิดขึ้นจริงในระบบประสาทก็คือ เรา “รับรู้” ถึงความต้องการเหล่านั้นที่ไหลผ่านมาในสมองเรา ณ จังหวะเวลานั้นพอดีต่างหาก (เราสามารถทดสอบทฤษฎีนี้ง่ายๆด้วยการตั้งคำถามตัวเองหลังจากที่ “ความคิด” บางอย่างผุดขึ้นมาในใจว่าความคิดเหล่านั้นมันเกิดขึ้นมาได้ยังไงและเราสามารถสั่งสมองให้ “หยุดคิด” ได้หรือไม่)
สิ่งที่ตามมาก็คือ กระบวนการ “ปรับแต่งความรู้สึก” ของมนุษย์ผ่านเทคโนโลยีสมัยใหม่ ที่ปัจจุบันนักวิทยาศาสตร์สามารถสั่งการ “หนูทดลอง” ให้ปฏิบัติภารกิจตามคำสั่งผ่านการฝังเครื่องมือที่คอยกระตุ้นสมองส่วนที่สร้างความสุขให้กับหนูควบคู่กับการฝึกระยะสั้นโดยหนูทดลองที่ร่วมโครงการนั้นจะมีความรู้สึกดีทุกครั้งหลังจากได้กระทำสิ่งที่พวกมันถูกสั่งให้ทำผ่านการกระตุ้นสมอง ซึ่งเทคโนโลยีสำหรับมนุษย์ก็เริ่มได้รับการพัฒนาให้มีประสิทธิภาพมากขึ้นแล้ว อาทิ ไมโครชิปส์สำหรับแก้โรคซึมเศร้าและหมวกสัญญาณแม่เหล็กไฟฟ้าสำหรับกระตุ้นสมองของทหารสหรัฐเพื่อเพิ่มสมรรถภาพในสนามรบ
หลักการของเสรีนิยมที่พูดถึง “ตัวตน” ที่มีเพียงหนึ่งเดียวของมนุษย์แต่ละคน (individual) นั้นก็ได้รับการโต้แย้งอย่างรุนแรงจากทฤษฎีของศาสตร์เศรษฐศาสตร์เชิงพฤติกรรม (behavioral economics) ที่พิสูจน์ว่ามนุษย์หนึ่งคนนั้นมีระบบการตัดสินใจอยู่ 2 ระบบ อันได้แก่ ตัวตนเชิงประสบการณ์ (experience self) ผู้รับรู้ถึงสิ่งที่มนุษย์ได้ประสบพบเจอและตัวตนเชิงบอกเล่า (narrative self) ผู้ทำหน้าที่บอกเล่าเรื่องราวเหล่านั้น โดยการทดลองได้ค้นพบว่าตัวตนเชิงบอกเล่านั้นมักสร้างเรื่องราวที่ไม่ได้สอดรับกับตัวตนเชิงประสบการณ์และตัวตนเชิงบอกเล่านี้เองคือตัวตนหลักที่ทำหน้าที่ชี้นำการตัดสินใจของมนุษย์ (ตัวอย่างการทดลอง: การทดลองจุ่มมือในน้ำเย็นจัดที่ผู้ทดลองจะต้องทำการจุ่มมือใน 2 รูปแบบ รูปแบบแรกผู้ทดลองจะต้องจุ่มมือในน้ำเย็นจัดเป็นระยะเวลาหนึ่ง รูปแบบที่สองนั้นเหมือนกับรูปแบบแรกแต่ผู้ทดลองจะต้องจุ่มมือนานขึ้นอีกในขณะที่น้ำจะอุ่นขึ้นเล็กน้อย ผลของการทดลองพบว่าผู้ทดลองที่ได้รับอิทธิพลจากตัวตนเชิงบอกเล่าชอบการจุ่มมือรูปแบบที่สองมากกว่าทั้งๆที่ตัวตนเชิงประสบการณ์ของพวกเขาต้องทนทุกข์เป็นระยะเวลานานกว่า ผลลัพธ์ของการทดลองนี้นำไปสู่กฎ peak-end rule ที่กล่าวว่ามนุษย์มักจะทำการเฉลี่ยความทรมานหรือความสุขของประสบการณ์ที่พวกเขาพบเจอมากกว่าการรวมผลลัพธ์จากเหตุการณ์เหล่านั้น)
Chapter 9: The Great Decoupling ในศตวรรษที่ 21 มนุษยชาติกำลังจะต้องประสบกับการคุกคามของ “เทคโนโลยี” ที่กำลังจะทำให้มนุษย์จำนวนมากต้องสูญเสีย “คุณค่า” ทางเศรษฐกิจและการทหารไปอย่างหมดสิ้นอันเป็นเสมือน “ชนวนระเบิด” ที่พร้อมจะทำลายความเชื่อของหลักการทางเสรีนิยมที่ให้คุณค่าแก่มนุษย์ทุกๆคนอย่างเท่าเทียมกัน ตัวอย่างเช่น IBM’s Watson ในอนาคตที่สามารถทำหน้าที่แทนคุณหมอทั่วโลกในการวินิจฉัยโรคร้ายของผู้ป่วยพร้อมๆกันทั้งโลกได้อย่างแม่นยำตลอด 24 ชั่วโมง การจู่โจมทางโลกไซเบอร์โดยกลุ่มแฮกเกอร์เพียงหยิบมือสามารถส่งผลร้ายแรงต่อเป้าหมายได้อย่างรวดเร็วและมีประสิทธิภาพกว่าการใช้ทหารนับล้านนายในการสู้รบ
ในอดีตมนุษย์มีความเชื่อว่าสิ่งมีชีวิตที่มีความสามารถในการตระหนักรู้ถึงอารมณ์และความรู้สึกของตัวเองเท่านั้นที่จะสามารถสร้างภูมิปัญญาที่สูงส่งได้ แต่แล้ว เทคโนโลยีทางคอมพิวเตอร์และหุ่นยนต์ในปัจจุบันนั้นได้แสดงให้เห็นถึง “การแยกออกจากกัน” ของ “ปัญญา (Intelligence)” และ “การตระหนักรู้ (consciousness)” ที่ในอนาคต algorithm ที่มีระดับสติปัญญาขั้นสูงจะเข้ามาทำหน้าที่แทนมนุษย์ผู้มีความตระหนักรู้แต่มีระบบประมวลผลที่ด้อยประสิทธิภาพกว่าโดยที่ algorithm เหล่านั้นไม่จำเป็นต้องมีความรู้สึกนึกคิดแต่อย่างใด ไม่แตกต่างจากเหตุการณ์ในอดีตที่ “ม้า” ซึ่งมีความรู้สึกและความผูกพันธุ์กับเจ้าของถูกแทนที่ด้วย “รถยนต์” อันไร้ความรู้สึกที่มีประสิทธิภาพในการพามนุษย์เดินทางจากจุดหนึ่งไปยังอีกจุดหนึ่งที่ดีกว่าได้ภายในระยะเวลาอันรวดเร็ว
คำถามสำคัญที่ตามมาก็คือ “แล้วที่ยืนของมนุษย์นั้นอยู่ตรงไหน” คำตอบของนักวิชาการส่วนใหญ่ในยุคปัจจุบันก็คือ งานที่ต้องใช้ความสามารถในการตระหนักรู้ถึงอารมณ์และความคิดสร้างสรรค์ของมนุษย์ แต่เอาเข้าจริงๆแล้ว สิ่งมีชีวิตทุกชนิดรวมถึงมนุษย์นั้นล้วนมี algorithm สำหรับการตัดสินใจทั้งนั้น งานที่ต้องใช้ความคิดสร้างสรรค์อย่างการแต่งเพลงนั้นตามหลักการของ Life science นั้นถือเป็นกระบวนการในการประมวลผลรูปแบบของเสียงตามหลักการทางคณิตศาสตร์ของมนุษย์เท่านั้น ซึ่งนักประดิษฐ์คนหนึ่งสามารถพัฒนา A.I. นามว่า EMI ที่สามารถประมวลผลเพลงของศิลปินชื่อดังอย่าง Bach มาใช้ในการแต่งเพลงใหม่ของตัวเองกว่า 5,000 เพลงภายใน 1 วันโดยที่ผู้ฟังนั้นเชื่อว่าเพลงเหล่านี้ถูกแต่งขึ้นมาโดยศิลปินเอกได้อย่างสนิทใจ โลกในอนาคตกำลังจะสร้างประชากรกลุ่มใหม่ที่ไร้ซึ่งคุณค่าใดๆต่อสังคม (useless class) ส่วนกลุ่มงานที่อาจได้รับผลกระทบน้อยที่สุดนั้นคือกลุ่มงานที่ต้องใช้ความสามารถเฉพาะตัวขั้นสูงแต่มีผลตอบแทนที่ต่ำจนไม่มีใครอยากลงทุนพัฒนาโปรแกรมมาทำหน้าที่ทดแทน อาทิ นักโบราณคดี
แนวโน้มถัดมาของศตวรรษที่ 21 ก็คือ การเกิดขึ้นของระบบการตัดสินใจที่หลอมรวมมนุษย์เข้าด้วยกันเป็นกลุ่ม ศาสตร์ Life science ได้พิสูจน์ว่าทุกกระบวนการตัดสินใจของมนุษย์นั้นเกิดขึ้นจาก algorithm ภายในของมนุษย์แต่ละคนที่เต็มไปด้วยข้อบกพร่องนานับประการ และเมื่อถึงจุดหนึ่งที่เทคโนโลยีสามารถทำความเข้าใจหลักการของ algorithm เหล่านั้นมากพอที่จะสร้าง external algorithm ที่มีประสิทธิภาพมากกว่าได้สำเร็จ ความสำคัญของปัจเจกบุคคลก็จะสูญสลายไป มนุษย์จะถูกหลอมรวมเป็นกลุ่มก้อนและถูกถ่ายเทพลังอำนาจไปยัง algorithm ที่สามารถตัดสินใจสิ่งต่างๆได้ดีกว่า ตัวอย่างที่เห็นชัดๆในปัจจุบันก็คือ เทคโนโลยีทางการแพทย์อย่าง wearable sensor ที่สามารถตรวจจับกระบวนการทำงานของร่างกายและให้คำแนะนำแก่ผู้ใช้งานอยู่ตลอดเวลา หรือ เหตุการณ์ที่ Angelina Jolie ตัดสินใจผ่าตัดเต้านมหลังรับทราบข้อมูลทางสถิติจากการตรวจสอบทางพันธุกรรมว่าตัวของเธอมีโอกาส 87% ที่จะเป็นมะเร็งเต้านม
ในอนาคต “มนุษย์” อาจจะต้องพึ่งพิง “โปรแกรม” อย่างแยกจากกันไม่ได้ตลอด 24 ชั่วโมง ลองจินตนาการโลกในอนาคตที่มนุษย์ทุกคนยอมให้ข้อมูลทั้งหมดแก่ Google ไม่ว่าจะเป็นข้อมูลทางชีววิทยา (อาทิ ข้อมูลการทำงานของร่างกายแบบ real time ผ่านการฝังไมโครเซนเซอร์ DNA และประวัติทางพันธุกรรม) ข้อมูลการใช้จ่าย อีเมล์และอีกมากมาย เมื่อข้อมูลมีมากพอถึงจุดหนึ่ง Google จะกลายมาเป็น algorithm ที่รู้จักมนุษย์มากกว่าตัวของพวกเขาเองและสามารถให้คำแนะนำในการตัดสินใจได้ดีกว่ามนุษย์ที่เต็มไปด้วยความลำเอียง (bias) และความไร้ประสิทธิภาพในการประมวลผลข้อมูลของตัวเอง
และเมื่อมนุษย์สามารถอัพเกรดร่างกายและสติปัญญาของตัวเองได้สำเร็จ โลกก็จะเริ่มเข้าสู่ยุคที่ความไม่เท่าเทียมกันระหว่างชนชั้นสูงกับชนชั้นล่างนั้นรุนแรงอย่างที่ไม่เคยเกิดขึ้นมาก่อน ความแตกต่างนั้นไม่ได้เกิดขึ้นเฉพาะฐานะทางเศรษฐกิจเท่านั้น แต่กลุ่มชนชั้นสูงนั้นจะกลายมาเป็นสิ่งมีชีวิตที่มีสมรรถภาพทางชีววิทยาที่เหนือกว่าและเมื่อเทคโนโลยีได้ทำให้ “ประโยชน์” ของกลุ่มชนชั้นล่างหมดไป ประชากรเกือบทั้งหมดของโลกมนุษย์อาจจะกลายมาเป็นเพียงขยะทางชีววิทยาที่ไร้คุณค่าใดๆในสายตาของกลุ่มมนุษย์ที่สามารถแปลงกายเป็นเทพเจ้าได้สำเร็จ
Lee Sedol vs. AlphaGo (ขอบคุณภาพจาก BGR India)
Chapter 10: The Ocean of Consciousness ศาสนาใหม่ของมนุษยชาติกำลังถือกำเนิดขึ้นในห้องทดลองทางวิทยาศาสตร์ที่มี Silicon Valley เป็นจุดศูนย์กลาง สิ่งที่สร้างความแตกต่างให้กับศาสนาแห่งใหม่นี้ก็คือความสามารถในการสร้างความสุข ความสงบหรือแม้กระทั่งชีวิตอันเป็นนิรันดร์ให้กับมนุษย์บนโลกใบนี้ (ไม่ใช่โลกหลังความตายเหมือนศาสนาอื่นๆ)
Techno-humanist คือ ศาสนาที่ต่อยอดจากมนุษยนิยมที่ยังคงมีความเชื่อมั่นในความสามารถของมนุษย์อยู่ แต่มนุษย์ตามความหมายของศาสนาใหม่นี้คือมนุษย์สายพันธุ์ Homo Deus ที่ได้รับการอัพเกรดจากเทคโนโลยีทางชีววิทยา นาโนและการเชื่อมต่อคอมพิวเตอร์เข้ากับระบบประสาท อันเป็นเหตุให้ Homo Deus มีความสามารถที่ยังคงสร้างคุณค่าในโลกที่เต็มไปด้วย algorithm ได้ (นักวิทยาศาสตร์เชื่อว่ามนุษย์สามารถขยายประสิทธิภาพของการใช้งานของสมองและจิตใจได้อีกมหาศาลโดยยกตัวอย่างศักยภาพที่มนุษย์ในปัจจุบันไม่สามารถทำได้ อาทิ ประสิทธิภาพในการดมกลิ่นของมนุษย์ในยุคล่าสัตว์ การอ่านคลื่นสะท้อนของค้างคาวและการตรวจจับเสียงที่อยู่ห่างออกไปหลายร้อยกิโลเมตรของปลาวาฬ)
แต่หลักการของ Techno-humanist ก็มีข้อจำกัด เมื่อมนุษย์สามารถอัพเกรดระบบการตัดสินใจของตัวเองจนสามารถกำจัดตัวตนที่ไร้เหตุผลหรือตัวตนที่ก่อให้เกิดความลำบากใจภายในจิตใจของพวกเขาได้ มนุษย์เวอร์ชั่นอัพเกรดเหล่านั้นก็จะมีระบบการตัดสินใจที่ไม่แตกต่างจาก algorithm อันเป็นจุดกำเนิดของศาสนาชนิดใหม่ที่มี “ข้อมูล” เป็นศูนย์กลาง
Chapter 11: The Data Religion Dataism (ข้อมูลนิยม) คือ ศาสนาที่เชื่อมั่นในประสิทธิภาพของ “ข้อมูล” อันเกิดขึ้นจากการผสมผสานความก้าวหน้าของศาสตร์ทางชีววิทยาและคอมพิวเตอร์ที่เชื่อมั่นว่า algorithm ของสิ่งมีชีวิตและคอมพิวเตอร์นั้นสามารถผสมผสานรวมกันได้
ประวัติศาสตร์ของศตวรรษที่ 20 แสดงให้เห็นอย่างชัดเจนว่าระบบการปกครองที่สามารถ “ประมวลผลข้อมูล (data processing)” ได้อย่างครอบคลุมมากกว่านั้นคือระบบการปกครองที่มีประสิทธิภาพและพลังอำนาจที่สูงที่สุด (ระบอบคอมมิวนิสต์ที่มีรัฐบาลกลางทำหน้าที่ประมวลผลข้อมูลแต่เพียงผู้เดียวไม่สามารถสู้กับกำลังของหน่วยประมวลผลข้อมูลจำนวนมหาศาลของระบบกระจายอำนาจตามหลักเสรีนิยมได้) แต่ในยุคปัจจุบันที่การเปลี่ยนแปลงทางเทคโนโลยีนั้นเกิดขึ้นอย่างรวดเร็วพร้อมๆกับการเพิ่มขึ้นของข้อมูลจำนวนมหาศาล ระบบการปกครองทุกรูปแบบในปัจจุบันไม่สามารถติดตามการเปลี่ยนแปลงได้อีกต่อไปอันเป็นเหตุให้โลกต้องการระบบและผู้ปกครองรูปแบบใหม่ที่มีประสิทธิภาพที่ดีกว่า
ประวัติศาสตร์ยังแสดงให้เห็นถึงวิวัฒนาการของกระบวนการประมวลผลข้อมูลของมนุษยชาติ ที่เริ่มตั้งแต่ การขยายจำนวนประชากร (หน่วยประมวลผล) อันก่อให้เกิดความหลากหลายของหน่วยประมวลผลเหล่านั้นที่กระจัดกระจายอยู่ทั่วโลก จนกระทั่งโลกในยุคเกษตรกรรมและอุตสาหกรรมที่หน่วยประมวลผลที่มีความหลากหลายได้กลับมารวมตัวกันเป็นกลุ่มที่ใหญ่ขึ้นและเชื่อมต่อกันได้อย่างมีประสิทธิภาพมากขึ้นเรื่อยๆ ซึ่งแสดงให้เห็นถึงแนวโน้มของการเกิดขึ้นของการเชื่อมต่อทางข้อมูลอย่างสมบูรณ์ของมนุษยชาติและทุกสรรพสิ่ง (Internet-of-all-thing)
อิสรภาพของข้อมูล (freedom of information) คือ “หัวใจ” สำคัญของ Dataism ที่เชื่อมั่นว่า “เมื่อมนุษย์ยินยอมเปิดเผยข้อมูลทั้งหมดให้กับระบบ algorithm หนึ่งเดียวของโลก ทุกกระบวนการตัดสินใจของมนุษย์จะถูกประมวลผลและตัดสินใจผ่านระบบประมวลผลแห่งนั้นอันนำมาซึ่งผลลัพธ์ที่ดีที่สุดให้กับมนุษย์และทุกสรรพสิ่ง” ตัวอย่างของการใช้ระบบข้อมูลมหาศาลในการเพิ่มประสิทธิภาพให้กับโลกมนุษย์นั้นได้แก่ ระบบแบ่งปันรถยนต์ไร้คนขับ (driverless carpooling) ที่เข้ามาขจัดปัญหาของความสิ้นเปลืองของการใช้ทรัพยากร “รถยนต์” ที่ใช้เวลามากกว่า 90% ในการจอดอยู่กับที่เฉยๆอย่างไร้ประโยชน์ หากมนุษย์ทุกคนยอมเปิดเผยข้อมูลตำแหน่งที่อยู่อาศัย ที่ทำงาน จุดหมายปลายทางและเวลาให้กับระบบ algorithm อย่างสมบูรณ์ ระบบประมวลผลนี้ก็จะสามารถจัดสรรการใช้ทรัพยากรรถยนต์แบบแบ่งปันให้กับผู้ที่ต้องการเดินทางด้วยรถยนต์ได้อย่างมีประสิทธิภาพและยังสามารถลดปริมาณรถยนต์ส่วนบุคคลในท้องถนนได้ถึง 20 เท่าพร้อมกับการลดลงของพื้นที่จอดรถอีกจำนวนมหาศาล
Dataism (ขอบคุณภาพจาก Financial Times)
แต่ถึงกระนั้น ทฤษฎีที่กล่าวมาทั้งหมดในหนังสือเล่มนี้เป็นเพียงแค่การพยากรณ์ที่อาศัยการศึกษาทางประวัติศาสตร์ของมนุษยชาติและการทำความเข้าใจเทคโนโลยีในยุคปัจจุบันเท่านั้น มนุษย์ในทุกวันนี้คงไม่มีทางมองเห็นและเข้าใจมนุษย์ในอีก 50 ปีข้างหน้าได้อย่างสมบูรณ์ แต่ 3 แนวโน้มที่กำลังเกิดขึ้นจริงที่ทุกคนควรจะต้องคำนึงถึงอยู่เสมอในการวางแผนอนาคตนั้นก็คือ
 สิ่งมีชีวิตทั้งหมดนั้นคือระบบประมวลผลที่ถูกผลักดันโดย algorithm “สติปัญญา” กับ “การตระหนักรู้” นั้นกำลังถูกแยกออกจากกัน มนุษย์ผู้มีอารมณ์ความรู้สึกกำลังจะถูกแทนที่ด้วย algorithm ที่มีสติปัญญาที่สูงกว่า algorithm กำลังจะมีความสามารถในการเข้าใจมนุษย์มากกว่าตัวของพวกเราเอง   Source :.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_1/homo_deus/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_4/example_2_subtopic_4/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Example 2 of subtopic 4
      </h3>
      <p class="refresh-summary">This summary is the real content of the article.
</p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_4/example_2_subtopic_4/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_3/example_2_subtopic_3/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Subtopic 3 is very cool!
      </h3>
      <p class="refresh-summary">The summary is a custom custom one</p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_3/example_2_subtopic_3/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/example_2_topic_2/"><img src="/topic_2/example_2_topic_2/summary_2_hu8dbdcdbe8d39d486b4229abea415a569_6024271_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Topic 2 is very cool!
      </h3>
      <p class="refresh-summary">The summary image should be a custom one</p> 
      <div class="action has-text-right">
        <a href="/topic_2/example_2_topic_2/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">love</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_4/example_1_subtopic_4/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Subtopic 4 is very cool!
      </h3>
      <p class="refresh-summary">The summary image should be the default</p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_4/example_1_subtopic_4/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">no_summary</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/example_1_topic_2/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Example 1 of Topic 2
      </h3>
      <p class="refresh-summary">This summary is the real content of the article.
</p> 
      <div class="action has-text-right">
        <a href="/topic_2/example_1_topic_2/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_4/example_2_subtopic_4/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Example 2 of subtopic 4
      </h3>
      <p class="refresh-summary">This summary is the real content of the article.
</p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_4/example_2_subtopic_4/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_3/example_1_subtopic_3/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Subtopic 3 is very cool!
      </h3>
      <p class="refresh-summary">This is the real text of the article.
def sum_function(a, b): c = a &#43; b return c  </p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_3/example_1_subtopic_3/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">Requests</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/web-scraping-request/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup
      </h3>
      <p class="refresh-summary"> After the 2016 election I became much more interested in media bias and the manipulation of individuals through advertising. This series will be a walkthrough of a web scraping project that monitors political news from both left and right wing media outlets and performs an analysis on the rhetoric being used, the ads being displayed, and the sentiment of certain topics.
The first part of the series will we be getting media bias data and focus on only working locally on your computer, but if you wish to learn how to deploy something like this into production, feel free to leave a comment and let me know.
Limit your impact when scraping Every time you load a web page you&amp;rsquo;re making a request to a server, and when you&amp;rsquo;re just a human with a browser there&amp;rsquo;s not a lot of damage you can do. With a Python script that can execute thousands of requests a second if coded incorrectly, you could end up costing the website owner a lot of money and possibly bring down their site (see Denial-of-service attack (DoS)).
With this in mind, we want to be very careful with how we program scrapers to avoid crashing sites and causing damage. Every time we scrape a website we want to attempt to make only one request per page. We don&amp;rsquo;t want to be making a request every time our parsing or other logic doesn&amp;rsquo;t work out, so we need to parse only after we&amp;rsquo;ve saved the page locally.
If I&amp;rsquo;m just doing some quick tests, I&amp;rsquo;ll usually start out in a Jupyter notebook because you can request a web page in one cell and have that web page available to every cell below it without making a new request. Since this article is available as a Jupyter notebook, you will see how it works if you choose that format.
How to save HTML locally After we make a request and retrieve a web page&amp;rsquo;s content, we can store that content locally with Python&amp;rsquo;s open() function. To do so we need to use the argument wb, which stands for &amp;ldquo;write bytes&amp;rdquo;. This let&amp;rsquo;s us avoid any encoding issues when saving.
Below is a function that wraps the open() function to reduce a lot of repetitive coding later on:
def save_html(html, path): with open(path, &#39;wb&#39;) as f: f.write(html) save_html(r.content, &#39;google_com&#39;)  Assume we have captured the HTML from google.com in html, which you&amp;rsquo;ll see later how to do. After running this function we will now have a file in the same directory as this notebook called google_com that contains the HTML.
How to open/read HTML from a local file To retrieve our saved file we&amp;rsquo;ll make another function to wrap reading the HTML back into html. We need to use rb for &amp;ldquo;read bytes&amp;rdquo; in this case.
def open_html(path): with open(path, &#39;rb&#39;) as f: return f.read() html = open_html(&#39;google_com&#39;)  The open function is doing just the opposite: read the HTML from google_com. If our script fails, notebook closes, computer shutsdown, etc., we no longer need to request google.com again, lessening our impact on their servers. While it doesn&amp;rsquo;t matter much with Google since they have a lot of resources, smaller sites with smaller servers will benefit from this.
I save almost every page and parse later when web scraping as a safety precaution.
Follow the rules for scrapers and bots Each site usually has a robots.txt on the root of their domain. This is where the website owner explicitly states what bots are allowed to do on their site. Simply go to example.com/robots.txt and you should find a text file that looks something like this:
User-agent: * Crawl-delay: 10 Allow: /pages/ Disallow: /scripts/ # more stuff  The User-agent field is the name of the bot and the rules that follow are what the bot should follow. Some robots.txt will have many User-agents with different rules. Common bots are googlebot, bingbot, and applebot, all of which you can probably guess the purpose and origin of.
We don&amp;rsquo;t really need to provide a User-agent when scraping, so User-agent: * is what we would follow. A * means that the following rules apply to all bots (that&amp;rsquo;s us).
The Crawl-delay tells us the number of seconds to wait before requests, so in this example we need to wait 10 seconds before making another request.
Allow gives us specific URLs we&amp;rsquo;re allowed to request with bots, and vice versa for Disallow. In this example we&amp;rsquo;re allowed to request anything in the /pages/ subfolder which means anything that starts with example.com/pages/. On the other hand, we are disallowed from scraping anything from the /scripts/ subfolder.
Many times you&amp;rsquo;ll see a * next to Allow or Disallow which means you are either allowed or not allowed to scrape everything on the site.
Sometimes there will be a disallow all pages followed by allowed pages like this:
Disallow: * Allow: /pages/  This means that you&amp;rsquo;re not allowed to scrape anything except the subfolder /pages/. Essentially, you just want to read the rules in order where the next rule overrides the previous rule.
Scraping Project: Getting Media Bias Data This project will primarily be run through a Jupyter notebook, which is done for teaching purposes and is not the usual way scrapers are programmed. After showing you the pieces, we&amp;rsquo;ll put it all together into a Python script that can be run from command line or your IDE of choice.
Making web requests With Python&amp;rsquo;s requests library we&amp;rsquo;re getting a web page by using get() on the URL. The response r contains many things, but using r.content will give us the HTML. Once we have the HTML we can then parse it for the data we&amp;rsquo;re interested in analyzing.
There&amp;rsquo;s an interesting website called AllSides that has a media bias rating table where users can agree or disagree with the rating.
Since there&amp;rsquo;s nothing in their robots.txt that disallows us from scraping this section of the site, I&amp;rsquo;m assuming it&amp;rsquo;s okay to go ahead and extract this data for our project. Let&amp;rsquo;s request the this first page:
!pip install requests  import requests url = &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39; r = requests.get(url) print(r.content[:100])  Since we essentially have a giant string of HTML, we can print a slice of 100 characters to confirm we have the source of the page. Let&amp;rsquo;s start extracting data.
Parsing HTML with BeautifulSoup What does BeautifulSoup do? We used requests to get the page from the AllSides server, but now we need the BeautifulSoup library to parse HTML and XML. When we pass our HTML to the BeautifulSoup constructor we get an object in return that we can then navigate like the original tree structure of the DOM.
This way we can find elements using names of tags, classes, IDs, and through relationships to other elements, like getting the children and siblings of elements.
Creating a new soup object We create a new BeautifulSoup object by passing the constructor our newly acquired HTML content and the type of parser we want to use:
!pip install beautifulsoup4  from bs4 import BeautifulSoup soup = BeautifulSoup(r.content, &#39;html.parser&#39;)  This soup object defines a bunch of methods — many of which can achieve the same result — that we can use to extract data from the HTML. Let&amp;rsquo;s start with finding elements.
Finding elements and data To find elements and data inside our HTML we&amp;rsquo;ll be using select_one, which returns a single element, and select, which returns a list of elements (even if only one item exists). Both of these methods use CSS selectors to find elements, so if you&amp;rsquo;re rusty on how CSS selectors work here&amp;rsquo;s a quick refresher:
A CSS selector refresher 1. To get a tag, such as &amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;, &amp;lt;body&amp;gt;&amp;lt;/body&amp;gt;, use the naked name for the tag. E.g. select_one(&#39;a&#39;) gets an anchor/link element, select_one(&#39;body&#39;) gets the body element 2. .temp gets an element with a class of temp, E.g. to get &amp;lt;a class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp&#39;) 3. #temp gets an element with an id of temp, E.g. to get &amp;lt;a id=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;#temp&#39;) 4. .temp.example gets an element with both classes temp and example, E.g. to get &amp;lt;a class=&amp;quot;temp example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp.example&#39;) 5. .temp a gets an anchor element nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp a&#39;). Note the space between .temp and a. 6. .temp .example gets an element with class example nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a class=&amp;quot;example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp .example&#39;). Again, note the space between .temp and .example. The space tells the selector that the class after the space is a child of the class before the space. 7. ids, such as &amp;lt;a id=one&amp;gt;&amp;lt;/a&amp;gt;, are unique so you can usually use the id selector by itself to get the right element. No need to do nested selectors when using ids.
There&amp;rsquo;s many more selectors for for doing various tasks, like selecting certain child elements, specific links, etc., that you can look up when needed. The selectors above get us pretty close to everything we would need for now.
Tips on figuring out how to select certain elements
Most browsers have a quick way of finding the selector for an element using their developer tools. In Chrome, we can quickly find selectors for elements by 1. Right-click on the the element then select &amp;ldquo;Inspect&amp;rdquo; in the menu. Developer tools opens and and highlights the element we right-clicked 2. Right-click the code element in developer tools, hover over &amp;ldquo;Copy&amp;rdquo; in the menu, then click &amp;ldquo;Copy selector&amp;rdquo;
Sometimes it&amp;rsquo;ll be a little off and we need to scan up a few elements to find the right one. Here&amp;rsquo;s what it looks like to find the selector and Xpath, another type of selector, in Chrome:


Let&amp;rsquo;s start! Getting data out of a table Our data is housed in a table on AllSides, and by inspecting the header element we can find the code that renders the table and rows. What we need to do is select all the rows from the table and then parse out the information from each row.


Simplifying the table&amp;rsquo;s HTML, the structure looks like this (comments &amp;lt;!-- --&amp;gt; added by me):
&amp;lt;table&amp;gt; &amp;lt;thead&amp;gt; &amp;lt;!-- header information --&amp;gt; &amp;lt;/thead&amp;gt; &amp;lt;tbody&amp;gt; &amp;lt;tr class=&amp;quot;odd views-row-first&amp;quot;&amp;gt; &amp;lt;!-- begin table row --&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- outlet name --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- bias data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing-1 what-do-you-think&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree buttons --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;!-- end table row --&amp;gt; &amp;lt;!-- more rows --&amp;gt; &amp;lt;/tbody&amp;gt; &amp;lt;/table&amp;gt;  So to get each row, we just select all &amp;lt;tr&amp;gt; inside &amp;lt;tbody&amp;gt;:
rows = soup.select(&#39;tbody tr&#39;)  tbody tr tells the selector to extract all &amp;lt;tr&amp;gt; (table row) tags that are children of the &amp;lt;tbody&amp;gt; body tag. If there were more than one table on this page we would have to make a more specific selector, but since this is the only table, we&amp;rsquo;re good to go.
Now we have a list of HTML table rows that each contain four cells: - News source name and link - Bias data - Agreement buttons - Community feedback data
Below is a breakdown of how to extract each one.
News source name 

Let&amp;rsquo;s look at the first cell:
&amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/news-source/abc-news-media-bias&amp;quot;&amp;gt;ABC News&amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  The outlet name (ABC News) is the text of an anchor tag that&amp;rsquo;s nested inside a &amp;lt;td&amp;gt; tag, which is a cell — or table data tag.
Getting the outlet name is pretty easy: just get the first row in rows and run a select_one off that object:
row = rows[0] name = row.select_one(&#39;.source-title&#39;).text.strip() print(name)  The only class we needed to use in this case was .source-title since .views-field looks to be just a class each row is given for styling and doesn&amp;rsquo;t provide any uniqueness.
Notice that we didn&amp;rsquo;t need to worry about selecting the anchor tag a that contains the text. When we use .text is gets all text in that element, and since &amp;ldquo;ABC News&amp;rdquo; is the only text, that&amp;rsquo;s all we need to do. Bear in mind that using select or select_one will give you the whole element with the tags included, so we need .text to give us the text between the tags.
.strip() ensures all the whitespace surrounding the name is removed. This is a good thing to always do since many websites use whitespace as a way to visually pad the text inside elements.
You&amp;rsquo;ll notice that we can run BeautifulSoup methods right off one of the rows. That&amp;rsquo;s because the rows become their own BeautifulSoup objects when we make a select from another BeautifulSoup object. On the other hand, our name variable is no longer a BeautifulSoup object because we called .text.
News source page link We also need the link to this news source&amp;rsquo;s page on AllSides. If we look back at the HTML we&amp;rsquo;ll see that in this case we do want to select the anchor in order to get the href that contains the link, so let&amp;rsquo;s do that:
allsides_page = row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] allsides_page = &#39;https://www.allsides.com&#39; &#43; allsides_page print(allsides_page)  It is a relative path in the HTML, so we prepend the site&amp;rsquo;s URL to make it a link we can request later.
Getting the link was a bit different than just selecting an element. We had to access an attribute (href) of the element, which is done using brackets, like how we would access a Python dictionary. This will be the same for other attributes of elements, like src in images and videos.
Bias rating 

We can see that the rating is displayed as an image so how can we get the rating in words? Looking at the HTML notice the link that surrounds the image has the text we need:
&amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/media-bias/left-center&amp;quot;&amp;gt; &amp;lt;img src=&amp;quot;...&amp;quot; width=&amp;quot;144&amp;quot; height=&amp;quot;24&amp;quot; alt=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot; title=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot;&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  We could also pull the alt attribute, but the link looks easier. Let&amp;rsquo;s grab it:
bias = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;] bias = bias.split(&#39;/&#39;)[-1] print(bias)  Here we selected the anchor tag by using the class name and tag together: .views-field-field-bias-image is the class of the &amp;lt;td&amp;gt; and &amp;lt;a&amp;gt; is for the anchor nested inside.
After that we extract the href just like before, but now we only want the last part of the URL for the name of the bias so we split on slashes and get the last element of that split (left-center).
Community feedback data 

The last thing to scrape is the agree/disagree ratio from the community feedback area. The HTML of this cell is pretty convoluted due to the styling, but here&amp;rsquo;s the basic structure:
&amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;getratingval&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;rate-widget-4 rate-widget clear-block rate-average rate-widget-yesno&amp;quot; id=&amp;quot;rate-node-76-4-1&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;item-list&amp;quot;&amp;gt; &amp;lt;ul&amp;gt; &amp;lt;li class=&amp;quot;first&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-3&amp;quot;&amp;gt;agree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;li class=&amp;quot;last&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-4&amp;quot;&amp;gt;disagree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;rate-details&amp;quot;&amp;gt; &amp;lt;span class=&amp;quot;agree&amp;quot;&amp;gt;8241&amp;lt;/span&amp;gt;/&amp;lt;span class=&amp;quot;disagree&amp;quot;&amp;gt;6568&amp;lt;/span&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/td&amp;gt;  The numbers we want are located in two span elements in the last div. Both span elements have classes that are unique in this cell so we can use them to make the selection:
agree = row.select_one(&#39;.agree&#39;).text agree = int(agree) disagree = row.select_one(&#39;.disagree&#39;).text disagree = int(disagree) agree_ratio = agree / disagree print(f&amp;quot;Agree: {agree}, Disagree: {disagree}, Ratio {agree_ratio:.2f}&amp;quot;)  Using .text will return a string, so we need to convert them to integers in order to calculate the ratio.
Side note: If you&amp;rsquo;ve never seen this way of formatting print statements in Python, the f at the front allows us to insert variables right into the string using curly braces. The :.2f is a way to format floats to only show two decimals places.
If you look at the page in your browser you&amp;rsquo;ll notice that they say how much the community is in agreement by using &amp;ldquo;somewhat agree&amp;rdquo;, &amp;ldquo;strongly agree&amp;rdquo;, etc. so how do we get that? If we try to select it:
print(row.select_one(&#39;.community-feedback-rating-page&#39;))  It shows up as None because this element is rendered with Javascript and requests can&amp;rsquo;t pull HTML rendered with Javascript. We&amp;rsquo;ll be looking at how to get data rendered with JS in a later article, but since this is the only piece of information that&amp;rsquo;s rendered this way we can manually recreate the text.
To find the JS files they&amp;rsquo;re using, just CTRL&#43;F for &amp;ldquo;.js&amp;rdquo; in the page source and open the files in a new tab to look for that logic.
It turned out the logic was located in the eleventh JS file and they have a function that calculates the text and color with these parameters:
 Range Agreeance   $ratio  3$ absolutely agrees   $2 strongly agrees   $1.5 agrees   $1 somewhat agrees   $ratio = 1$ neutral   $0.67 somewhat disgrees   $0.5 disgrees   $0.33 strongly disagrees   $ratio \leq 0.33$ absolutely disagrees   Let&amp;rsquo;s make a function that replicates this logic:
def get_agreeance_text(ratio): if ratio &amp;gt; 3: return &amp;quot;absolutely agrees&amp;quot; elif 2 &amp;lt; ratio &amp;lt;= 3: return &amp;quot;strongly agrees&amp;quot; elif 1.5 &amp;lt; ratio &amp;lt;= 2: return &amp;quot;agrees&amp;quot; elif 1 &amp;lt; ratio &amp;lt;= 1.5: return &amp;quot;somewhat agrees&amp;quot; elif ratio == 1: return &amp;quot;neutral&amp;quot; elif 0.67 &amp;lt; ratio &amp;lt; 1: return &amp;quot;somewhat disagrees&amp;quot; elif 0.5 &amp;lt; ratio &amp;lt;= 0.67: return &amp;quot;disagrees&amp;quot; elif 0.33 &amp;lt; ratio &amp;lt;= 0.5: return &amp;quot;strongly disagrees&amp;quot; elif ratio &amp;lt;= 0.33: return &amp;quot;absolutely disagrees&amp;quot; else: return None print(get_agreeance_text(2.5))  Now that we have the general logic for a single row and we can generate the agreeance text, let&amp;rsquo;s create a loop that gets data from every row on the first page:
data= [] for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d)  In the loop we can combine any multi-step extractions into one to create the values in the least number of steps.
Our data list now contains a dictionary containing key information for every row.
print(data[0])  Keep in mind that this is still only the first page. The list on AllSides is three pages long as of this writing, so we need to modify this loop to get the other pages.
Requesting and parsing multiple pages Notice that the URLs for each page follow a pattern. The first page has no parameters on the URL, but the next pages do; specifically they attach a ?page=# to the URL where &amp;lsquo;#&amp;rsquo; is the page number.
Right now, the easiest way to get all pages is just to manually make a list of these three pages and loop over them. If we were working on a project with thousands of pages we might build a more automated way of constructing/finding the next URLs, but for now this works.
pages = [ &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=1&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=2&#39; ]  According to AllSides&amp;rsquo; robots.txt we need to make sure we wait ten seconds before each request.
Our loop will: - request a page - parse the page - wait ten seconds - repeat for next page.
Remember, we&amp;rsquo;ve already tested our parsing above on a page that was cached locally so we know it works. You&amp;rsquo;ll want to make sure to do this before making a loop that performs requests to prevent having to reloop if you forgot to parse something.
By combining all the steps we&amp;rsquo;ve done up to this point and adding a loop over pages, here&amp;rsquo;s how it looks:
from time import sleep data= [] for page in pages: r = requests.get(page) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) rows = soup.select(&#39;tbody tr&#39;) for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d) sleep(10)  Now we have a list of dictionaries for each row on all three pages.
To cap it off, we want to get the real URL to the news source, not just the link to their presence on AllSides. To do this, we will need to get the AllSides page and look for the link.
If we go to ABC News&amp;rsquo; page there&amp;rsquo;s a row of external links to Facebook, Twitter, Wikipedia, and the ABC News website. The HTML for that sections looks like this:
&amp;lt;div class=&amp;quot;row-fluid source-links gray-bg-box&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;container&amp;quot;&amp;gt;	&amp;lt;a href=&amp;quot;https://www.facebook.com/ABCNews/&amp;quot; class=&amp;quot;facebook&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-facebook&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;Facebook&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://twitter.com/ABC&amp;quot; class=&amp;quot;twitter&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-twitter&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Twitter&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/ABC_News&amp;quot; class=&amp;quot;wikipedia&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-wikipedia-w&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Wikipedia&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;http://abcnews.go.com/&amp;quot; class=&amp;quot;www&amp;quot;&amp;gt;&amp;lt;i class=&amp;quot;fa fa-globe&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt; &amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;ABC News&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;/contact&amp;quot; class=&amp;quot;improve-this-page&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-line-chart&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Improve this page&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;  Notice the anchor tag (&amp;lt;a&amp;gt;) that contains the link to ABC News has a class of &amp;ldquo;www&amp;rdquo;. Pretty easy to get with what we&amp;rsquo;ve already learned:
website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;]  So let&amp;rsquo;s make another loop to request the AllSides page and get links for each news source. Unfortunately, some pages don&amp;rsquo;t have a link in this grey bar to the news source, which brings up a good point: always account for elements to randomly not exist.
Up until now we&amp;rsquo;ve assumed elements exist in the tables we scraped, but it&amp;rsquo;s always a good idea to program scrapers in way so they don&amp;rsquo;t break when an element goes missing.
Using select_one or select will always return None or an empty list if nothing is found, so in this loop we&amp;rsquo;ll check if we found the website element or not so it doesn&amp;rsquo;t throw an Exception when trying to access the href attribute.
Finally, since there&amp;rsquo;s 265 news source pages and the wait time between pages is 10 seconds, it&amp;rsquo;s going to take ~44 minutes to do this. Instead of blindly not knowing our progress, let&amp;rsquo;s use the tqdm library to give us a nice progress bar:
!pip install tqdm  from tqdm import tqdm_notebook for d in tqdm_notebook(data): r = requests.get(d[&#39;allsides_page&#39;]) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) try: website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;] d[&#39;website&#39;] = website except TypeError: pass sleep(10)  tqdm is a little weird at first, but essentially tqdm_notebook is just wrapping around our data list to produce a progress bar. We are still able to access each dictionary, d, just as we would normally. Note that tqdm_notebook is only for Jupyter notebooks. In regular editors you&amp;rsquo;ll just import tqdm from tqdm and use tqdm instead.
Saving our data So what do we have now? At this moment, data is a list of dictionaries, each of which contains all the data from the tables as well as the websites from each individual news source&amp;rsquo;s page on AllSides.
The first thing we&amp;rsquo;ll want to do now is save that data to a file so we don&amp;rsquo;t have to make those requests again. We&amp;rsquo;ll be storing the data as JSON since it&amp;rsquo;s already in that form anyway:
import json with open(&#39;allsides.json&#39;, &#39;w&#39;) as f: json.dump(data, f)  To load it back in when you need it:
with open(&#39;allsides.json&#39;, &#39;r&#39;) as f: data = json.load(f)  If you&amp;rsquo;re not familiar with JSON, just quickly open allsides.json in an editor and see what it looks like. It should look almost exactly like what data looks like if we print it in Python: a list of dictionaries.
Brief Data Analysis Before ending this article I think it would be worthwhile to actually see what&amp;rsquo;s interesting about this data we just retrieved. So, let&amp;rsquo;s answer a couple of questions.
Which ratings for outlets does the community absolutely agree on?
To find where the community absolutely agrees we can do a simple list comprehension that checks each dict for the agreeance text we want:
abs_agree = [d for d in data if d[&#39;agreeance_text&#39;] == &#39;absolutely agrees&#39;] print(f&amp;quot;{&#39;Outlet&#39;:&amp;lt;20} {&#39;Bias&#39;:&amp;lt;20}&amp;quot;) print(&amp;quot;-&amp;quot; * 30) for d in abs_agree: print(f&amp;quot;{d[&#39;name&#39;]:&amp;lt;20} {d[&#39;bias&#39;]:&amp;lt;20}&amp;quot;)  Using some string formatting we can make it look somewhat tabular. Interestingly, C-SPAN is the only center bias that the community absolutely agrees on. The others for left and right aren&amp;rsquo;t that surprising.
Making analysis easier with Pandas Which ratings for outlets does the community absolutely disagree on?
To make analysis a little easier, we can also load our JSON data into a Pandas DataFrame as well. This is easy with Pandas since they have a simple function for reading JSON into a DataFrame.
As an aside, if you&amp;rsquo;ve never used Pandas, Matplotlib, or any of the other data science libraries, I would definitely recommend checking out [Jose Portilla&amp;rsquo;s data science course]() for a great intro to these tools and many machine learning concepts.
Now to the DataFrame:
import pandas as pd df = pd.read_json(open(&#39;allsides.json&#39;, &#39;r&#39;)) df.set_index(&#39;name&#39;, inplace=True) df.head()  Now filter the DataFrame by &amp;ldquo;agreeance_text&amp;rdquo;:
df[df[&#39;agreeance_text&#39;] == &#39;strongly disagrees&#39;]  It looks like much of the community disagrees strongly with certain outlets being rated with a &amp;ldquo;center&amp;rdquo; bias.
Let&amp;rsquo;s make a quick visualization of agreeance. Since there&amp;rsquo;s too many news sources to plot so let&amp;rsquo;s pull only those with the most votes. To do that, we can make a new column that counts the total votes and then sort by that value:
df[&#39;total_votes&#39;] = df[&#39;agree&#39;] &#43; df[&#39;disagree&#39;] df.sort_values(&#39;total_votes&#39;, ascending=False, inplace=True) df.head(10)  Visualizing the data To make a bar plot we&amp;rsquo;ll use Matplotlib with Seaborn&amp;rsquo;s dark grid style:
import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-darkgrid&#39;)  As mentioned above, we have too many news outlets to plot comfortably, so just make a copy of the top 25 and place it in a new df2 variable:
df2 = df.head(25).copy() df2.head()  With the top 25 news sources by amount of feedback, let&amp;rsquo;s create a stacked bar chart where the number of agrees are stacked on top of the number of disagrees. This makes the total height of the bar the total amount of feedback.
Below, we first create a figure and axes, plot the agree bars, plot the disagree bars on top of the agrees using bottom, then set various text features:
fig, ax = plt.subplots(figsize=(20, 10)) ax.bar(df2.index, df2[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(df2.index, df2[&#39;disagree&#39;], bottom=df2[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) ax.set_ylabel = &#39;Total feedback&#39; plt.yticks(fontsize=&#39;x-large&#39;) plt.xticks(rotation=60, ha=&#39;right&#39;, fontsize=&#39;x-large&#39;, rotation_mode=&#39;anchor&#39;) plt.legend([&#39;Agree&#39;, &#39;Disagree&#39;], fontsize=&#39;xx-large&#39;) plt.title(&#39;AllSides Bias Rating vs. Community Feedback&#39;, fontsize=&#39;xx-large&#39;) plt.show()  For a slightly more complex version, let&amp;rsquo;s make a subplot for each bias and plot the respective news sources.
This time we&amp;rsquo;ll make a new copy of the original DataFrame beforehand since we can plot more news outlets now.
Instead of making one axes, we&amp;rsquo;ll create a new one for each bias to make six total subplots:
df3 = df.copy() fig = plt.figure(figsize=(15,15)) biases = df3[&#39;bias&#39;].unique() for i, bias in enumerate(biases): # Get top 10 news sources for this bias and sort index alphabetically temp_df = df3[df3[&#39;bias&#39;] == bias].iloc[:10] temp_df.sort_index(inplace=True) # Get max votes, i.e. the y value for tallest bar in this temp dataframe max_votes = temp_df[&#39;total_votes&#39;].max() # Add a new subplot in the correct grid position ax = fig.add_subplot(len(biases) / 2, 2, i &#43; 1) # Create the stacked bars ax.bar(temp_df.index, temp_df[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(temp_df.index, temp_df[&#39;disagree&#39;], bottom=temp_df[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) # Place text for the ratio on top of each bar for x, y, ratio in zip(ax.get_xticks(), temp_df[&#39;total_votes&#39;], temp_df[&#39;agree_ratio&#39;]): ax.text(x, y &#43; (0.02 * max_votes), f&amp;quot;{ratio:.2f}&amp;quot;, ha=&#39;center&#39;) ax.set_ylabel(&#39;Total feedback&#39;) ax.set_title(bias.title()) # Make y limit larger to compensate for text on bars ax.set_ylim(0, max_votes &#43; (0.12 * max_votes)) # Rotate tick labels so they don&#39;t overlap plt.setp(ax.get_xticklabels(), rotation=30, ha=&#39;right&#39;) plt.tight_layout(w_pad=3.0, h_pad=1.0) plt.show()  Hopefully the comments help with how these plots were created. We&amp;rsquo;re just looping through each unique bias and adding a subplot to the figure.
When interpreting these plots keep in mind that the y-axis has different scales for each subplot. Overall it&amp;rsquo;s a nice way to see which outlets have a lot of votes and where the most disagreement is. This is what makes scraping so much fun!
Final words We have the tools to make some fairly complex web scrapers now, but there&amp;rsquo;s still the issue with Javascript rendering. This is something that deserves its own article, but for now we can do quite a lot.
There&amp;rsquo;s also some project organization that needs to occur when making this into a more easily runnable program. We need to pull it out of this notebook and code in command-line arguments if we plan to run it often for updates.
These sorts of things will be addressed later when we build more complex scrapers, but feel free to let me know in the comments of anything in particular you&amp;rsquo;re interested in learning about.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_1/web-scraping-request/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_4/web-scraping-request/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup
      </h3>
      <p class="refresh-summary"> After the 2016 election I became much more interested in media bias and the manipulation of individuals through advertising. This series will be a walkthrough of a web scraping project that monitors political news from both left and right wing media outlets and performs an analysis on the rhetoric being used, the ads being displayed, and the sentiment of certain topics.
The first part of the series will we be getting media bias data and focus on only working locally on your computer, but if you wish to learn how to deploy something like this into production, feel free to leave a comment and let me know.
Limit your impact when scraping Every time you load a web page you&amp;rsquo;re making a request to a server, and when you&amp;rsquo;re just a human with a browser there&amp;rsquo;s not a lot of damage you can do. With a Python script that can execute thousands of requests a second if coded incorrectly, you could end up costing the website owner a lot of money and possibly bring down their site (see Denial-of-service attack (DoS)).
With this in mind, we want to be very careful with how we program scrapers to avoid crashing sites and causing damage. Every time we scrape a website we want to attempt to make only one request per page. We don&amp;rsquo;t want to be making a request every time our parsing or other logic doesn&amp;rsquo;t work out, so we need to parse only after we&amp;rsquo;ve saved the page locally.
If I&amp;rsquo;m just doing some quick tests, I&amp;rsquo;ll usually start out in a Jupyter notebook because you can request a web page in one cell and have that web page available to every cell below it without making a new request. Since this article is available as a Jupyter notebook, you will see how it works if you choose that format.
How to save HTML locally After we make a request and retrieve a web page&amp;rsquo;s content, we can store that content locally with Python&amp;rsquo;s open() function. To do so we need to use the argument wb, which stands for &amp;ldquo;write bytes&amp;rdquo;. This let&amp;rsquo;s us avoid any encoding issues when saving.
Below is a function that wraps the open() function to reduce a lot of repetitive coding later on:
def save_html(html, path): with open(path, &#39;wb&#39;) as f: f.write(html) save_html(r.content, &#39;google_com&#39;)  Assume we have captured the HTML from google.com in html, which you&amp;rsquo;ll see later how to do. After running this function we will now have a file in the same directory as this notebook called google_com that contains the HTML.
How to open/read HTML from a local file To retrieve our saved file we&amp;rsquo;ll make another function to wrap reading the HTML back into html. We need to use rb for &amp;ldquo;read bytes&amp;rdquo; in this case.
def open_html(path): with open(path, &#39;rb&#39;) as f: return f.read() html = open_html(&#39;google_com&#39;)  The open function is doing just the opposite: read the HTML from google_com. If our script fails, notebook closes, computer shutsdown, etc., we no longer need to request google.com again, lessening our impact on their servers. While it doesn&amp;rsquo;t matter much with Google since they have a lot of resources, smaller sites with smaller servers will benefit from this.
I save almost every page and parse later when web scraping as a safety precaution.
Follow the rules for scrapers and bots Each site usually has a robots.txt on the root of their domain. This is where the website owner explicitly states what bots are allowed to do on their site. Simply go to example.com/robots.txt and you should find a text file that looks something like this:
User-agent: * Crawl-delay: 10 Allow: /pages/ Disallow: /scripts/ # more stuff  The User-agent field is the name of the bot and the rules that follow are what the bot should follow. Some robots.txt will have many User-agents with different rules. Common bots are googlebot, bingbot, and applebot, all of which you can probably guess the purpose and origin of.
We don&amp;rsquo;t really need to provide a User-agent when scraping, so User-agent: * is what we would follow. A * means that the following rules apply to all bots (that&amp;rsquo;s us).
The Crawl-delay tells us the number of seconds to wait before requests, so in this example we need to wait 10 seconds before making another request.
Allow gives us specific URLs we&amp;rsquo;re allowed to request with bots, and vice versa for Disallow. In this example we&amp;rsquo;re allowed to request anything in the /pages/ subfolder which means anything that starts with example.com/pages/. On the other hand, we are disallowed from scraping anything from the /scripts/ subfolder.
Many times you&amp;rsquo;ll see a * next to Allow or Disallow which means you are either allowed or not allowed to scrape everything on the site.
Sometimes there will be a disallow all pages followed by allowed pages like this:
Disallow: * Allow: /pages/  This means that you&amp;rsquo;re not allowed to scrape anything except the subfolder /pages/. Essentially, you just want to read the rules in order where the next rule overrides the previous rule.
Scraping Project: Getting Media Bias Data This project will primarily be run through a Jupyter notebook, which is done for teaching purposes and is not the usual way scrapers are programmed. After showing you the pieces, we&amp;rsquo;ll put it all together into a Python script that can be run from command line or your IDE of choice.
Making web requests With Python&amp;rsquo;s requests library we&amp;rsquo;re getting a web page by using get() on the URL. The response r contains many things, but using r.content will give us the HTML. Once we have the HTML we can then parse it for the data we&amp;rsquo;re interested in analyzing.
There&amp;rsquo;s an interesting website called AllSides that has a media bias rating table where users can agree or disagree with the rating.
Since there&amp;rsquo;s nothing in their robots.txt that disallows us from scraping this section of the site, I&amp;rsquo;m assuming it&amp;rsquo;s okay to go ahead and extract this data for our project. Let&amp;rsquo;s request the this first page:
!pip install requests  import requests url = &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39; r = requests.get(url) print(r.content[:100])  Since we essentially have a giant string of HTML, we can print a slice of 100 characters to confirm we have the source of the page. Let&amp;rsquo;s start extracting data.
Parsing HTML with BeautifulSoup What does BeautifulSoup do? We used requests to get the page from the AllSides server, but now we need the BeautifulSoup library to parse HTML and XML. When we pass our HTML to the BeautifulSoup constructor we get an object in return that we can then navigate like the original tree structure of the DOM.
This way we can find elements using names of tags, classes, IDs, and through relationships to other elements, like getting the children and siblings of elements.
Creating a new soup object We create a new BeautifulSoup object by passing the constructor our newly acquired HTML content and the type of parser we want to use:
!pip install beautifulsoup4  from bs4 import BeautifulSoup soup = BeautifulSoup(r.content, &#39;html.parser&#39;)  This soup object defines a bunch of methods — many of which can achieve the same result — that we can use to extract data from the HTML. Let&amp;rsquo;s start with finding elements.
Finding elements and data To find elements and data inside our HTML we&amp;rsquo;ll be using select_one, which returns a single element, and select, which returns a list of elements (even if only one item exists). Both of these methods use CSS selectors to find elements, so if you&amp;rsquo;re rusty on how CSS selectors work here&amp;rsquo;s a quick refresher:
A CSS selector refresher 1. To get a tag, such as &amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;, &amp;lt;body&amp;gt;&amp;lt;/body&amp;gt;, use the naked name for the tag. E.g. select_one(&#39;a&#39;) gets an anchor/link element, select_one(&#39;body&#39;) gets the body element 2. .temp gets an element with a class of temp, E.g. to get &amp;lt;a class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp&#39;) 3. #temp gets an element with an id of temp, E.g. to get &amp;lt;a id=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;#temp&#39;) 4. .temp.example gets an element with both classes temp and example, E.g. to get &amp;lt;a class=&amp;quot;temp example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp.example&#39;) 5. .temp a gets an anchor element nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp a&#39;). Note the space between .temp and a. 6. .temp .example gets an element with class example nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a class=&amp;quot;example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp .example&#39;). Again, note the space between .temp and .example. The space tells the selector that the class after the space is a child of the class before the space. 7. ids, such as &amp;lt;a id=one&amp;gt;&amp;lt;/a&amp;gt;, are unique so you can usually use the id selector by itself to get the right element. No need to do nested selectors when using ids.
There&amp;rsquo;s many more selectors for for doing various tasks, like selecting certain child elements, specific links, etc., that you can look up when needed. The selectors above get us pretty close to everything we would need for now.
Tips on figuring out how to select certain elements
Most browsers have a quick way of finding the selector for an element using their developer tools. In Chrome, we can quickly find selectors for elements by 1. Right-click on the the element then select &amp;ldquo;Inspect&amp;rdquo; in the menu. Developer tools opens and and highlights the element we right-clicked 2. Right-click the code element in developer tools, hover over &amp;ldquo;Copy&amp;rdquo; in the menu, then click &amp;ldquo;Copy selector&amp;rdquo;
Sometimes it&amp;rsquo;ll be a little off and we need to scan up a few elements to find the right one. Here&amp;rsquo;s what it looks like to find the selector and Xpath, another type of selector, in Chrome:


Let&amp;rsquo;s start! Getting data out of a table Our data is housed in a table on AllSides, and by inspecting the header element we can find the code that renders the table and rows. What we need to do is select all the rows from the table and then parse out the information from each row.


Simplifying the table&amp;rsquo;s HTML, the structure looks like this (comments &amp;lt;!-- --&amp;gt; added by me):
&amp;lt;table&amp;gt; &amp;lt;thead&amp;gt; &amp;lt;!-- header information --&amp;gt; &amp;lt;/thead&amp;gt; &amp;lt;tbody&amp;gt; &amp;lt;tr class=&amp;quot;odd views-row-first&amp;quot;&amp;gt; &amp;lt;!-- begin table row --&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- outlet name --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- bias data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing-1 what-do-you-think&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree buttons --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;!-- end table row --&amp;gt; &amp;lt;!-- more rows --&amp;gt; &amp;lt;/tbody&amp;gt; &amp;lt;/table&amp;gt;  So to get each row, we just select all &amp;lt;tr&amp;gt; inside &amp;lt;tbody&amp;gt;:
rows = soup.select(&#39;tbody tr&#39;)  tbody tr tells the selector to extract all &amp;lt;tr&amp;gt; (table row) tags that are children of the &amp;lt;tbody&amp;gt; body tag. If there were more than one table on this page we would have to make a more specific selector, but since this is the only table, we&amp;rsquo;re good to go.
Now we have a list of HTML table rows that each contain four cells: - News source name and link - Bias data - Agreement buttons - Community feedback data
Below is a breakdown of how to extract each one.
News source name 

Let&amp;rsquo;s look at the first cell:
&amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/news-source/abc-news-media-bias&amp;quot;&amp;gt;ABC News&amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  The outlet name (ABC News) is the text of an anchor tag that&amp;rsquo;s nested inside a &amp;lt;td&amp;gt; tag, which is a cell — or table data tag.
Getting the outlet name is pretty easy: just get the first row in rows and run a select_one off that object:
row = rows[0] name = row.select_one(&#39;.source-title&#39;).text.strip() print(name)  The only class we needed to use in this case was .source-title since .views-field looks to be just a class each row is given for styling and doesn&amp;rsquo;t provide any uniqueness.
Notice that we didn&amp;rsquo;t need to worry about selecting the anchor tag a that contains the text. When we use .text is gets all text in that element, and since &amp;ldquo;ABC News&amp;rdquo; is the only text, that&amp;rsquo;s all we need to do. Bear in mind that using select or select_one will give you the whole element with the tags included, so we need .text to give us the text between the tags.
.strip() ensures all the whitespace surrounding the name is removed. This is a good thing to always do since many websites use whitespace as a way to visually pad the text inside elements.
You&amp;rsquo;ll notice that we can run BeautifulSoup methods right off one of the rows. That&amp;rsquo;s because the rows become their own BeautifulSoup objects when we make a select from another BeautifulSoup object. On the other hand, our name variable is no longer a BeautifulSoup object because we called .text.
News source page link We also need the link to this news source&amp;rsquo;s page on AllSides. If we look back at the HTML we&amp;rsquo;ll see that in this case we do want to select the anchor in order to get the href that contains the link, so let&amp;rsquo;s do that:
allsides_page = row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] allsides_page = &#39;https://www.allsides.com&#39; &#43; allsides_page print(allsides_page)  It is a relative path in the HTML, so we prepend the site&amp;rsquo;s URL to make it a link we can request later.
Getting the link was a bit different than just selecting an element. We had to access an attribute (href) of the element, which is done using brackets, like how we would access a Python dictionary. This will be the same for other attributes of elements, like src in images and videos.
Bias rating 

We can see that the rating is displayed as an image so how can we get the rating in words? Looking at the HTML notice the link that surrounds the image has the text we need:
&amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/media-bias/left-center&amp;quot;&amp;gt; &amp;lt;img src=&amp;quot;...&amp;quot; width=&amp;quot;144&amp;quot; height=&amp;quot;24&amp;quot; alt=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot; title=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot;&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  We could also pull the alt attribute, but the link looks easier. Let&amp;rsquo;s grab it:
bias = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;] bias = bias.split(&#39;/&#39;)[-1] print(bias)  Here we selected the anchor tag by using the class name and tag together: .views-field-field-bias-image is the class of the &amp;lt;td&amp;gt; and &amp;lt;a&amp;gt; is for the anchor nested inside.
After that we extract the href just like before, but now we only want the last part of the URL for the name of the bias so we split on slashes and get the last element of that split (left-center).
Community feedback data 

The last thing to scrape is the agree/disagree ratio from the community feedback area. The HTML of this cell is pretty convoluted due to the styling, but here&amp;rsquo;s the basic structure:
&amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;getratingval&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;rate-widget-4 rate-widget clear-block rate-average rate-widget-yesno&amp;quot; id=&amp;quot;rate-node-76-4-1&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;item-list&amp;quot;&amp;gt; &amp;lt;ul&amp;gt; &amp;lt;li class=&amp;quot;first&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-3&amp;quot;&amp;gt;agree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;li class=&amp;quot;last&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-4&amp;quot;&amp;gt;disagree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;rate-details&amp;quot;&amp;gt; &amp;lt;span class=&amp;quot;agree&amp;quot;&amp;gt;8241&amp;lt;/span&amp;gt;/&amp;lt;span class=&amp;quot;disagree&amp;quot;&amp;gt;6568&amp;lt;/span&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/td&amp;gt;  The numbers we want are located in two span elements in the last div. Both span elements have classes that are unique in this cell so we can use them to make the selection:
agree = row.select_one(&#39;.agree&#39;).text agree = int(agree) disagree = row.select_one(&#39;.disagree&#39;).text disagree = int(disagree) agree_ratio = agree / disagree print(f&amp;quot;Agree: {agree}, Disagree: {disagree}, Ratio {agree_ratio:.2f}&amp;quot;)  Using .text will return a string, so we need to convert them to integers in order to calculate the ratio.
Side note: If you&amp;rsquo;ve never seen this way of formatting print statements in Python, the f at the front allows us to insert variables right into the string using curly braces. The :.2f is a way to format floats to only show two decimals places.
If you look at the page in your browser you&amp;rsquo;ll notice that they say how much the community is in agreement by using &amp;ldquo;somewhat agree&amp;rdquo;, &amp;ldquo;strongly agree&amp;rdquo;, etc. so how do we get that? If we try to select it:
print(row.select_one(&#39;.community-feedback-rating-page&#39;))  It shows up as None because this element is rendered with Javascript and requests can&amp;rsquo;t pull HTML rendered with Javascript. We&amp;rsquo;ll be looking at how to get data rendered with JS in a later article, but since this is the only piece of information that&amp;rsquo;s rendered this way we can manually recreate the text.
To find the JS files they&amp;rsquo;re using, just CTRL&#43;F for &amp;ldquo;.js&amp;rdquo; in the page source and open the files in a new tab to look for that logic.
It turned out the logic was located in the eleventh JS file and they have a function that calculates the text and color with these parameters:
 Range Agreeance   $ratio  3$ absolutely agrees   $2 strongly agrees   $1.5 agrees   $1 somewhat agrees   $ratio = 1$ neutral   $0.67 somewhat disgrees   $0.5 disgrees   $0.33 strongly disagrees   $ratio \leq 0.33$ absolutely disagrees   Let&amp;rsquo;s make a function that replicates this logic:
def get_agreeance_text(ratio): if ratio &amp;gt; 3: return &amp;quot;absolutely agrees&amp;quot; elif 2 &amp;lt; ratio &amp;lt;= 3: return &amp;quot;strongly agrees&amp;quot; elif 1.5 &amp;lt; ratio &amp;lt;= 2: return &amp;quot;agrees&amp;quot; elif 1 &amp;lt; ratio &amp;lt;= 1.5: return &amp;quot;somewhat agrees&amp;quot; elif ratio == 1: return &amp;quot;neutral&amp;quot; elif 0.67 &amp;lt; ratio &amp;lt; 1: return &amp;quot;somewhat disagrees&amp;quot; elif 0.5 &amp;lt; ratio &amp;lt;= 0.67: return &amp;quot;disagrees&amp;quot; elif 0.33 &amp;lt; ratio &amp;lt;= 0.5: return &amp;quot;strongly disagrees&amp;quot; elif ratio &amp;lt;= 0.33: return &amp;quot;absolutely disagrees&amp;quot; else: return None print(get_agreeance_text(2.5))  Now that we have the general logic for a single row and we can generate the agreeance text, let&amp;rsquo;s create a loop that gets data from every row on the first page:
data= [] for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d)  In the loop we can combine any multi-step extractions into one to create the values in the least number of steps.
Our data list now contains a dictionary containing key information for every row.
print(data[0])  Keep in mind that this is still only the first page. The list on AllSides is three pages long as of this writing, so we need to modify this loop to get the other pages.
Requesting and parsing multiple pages Notice that the URLs for each page follow a pattern. The first page has no parameters on the URL, but the next pages do; specifically they attach a ?page=# to the URL where &amp;lsquo;#&amp;rsquo; is the page number.
Right now, the easiest way to get all pages is just to manually make a list of these three pages and loop over them. If we were working on a project with thousands of pages we might build a more automated way of constructing/finding the next URLs, but for now this works.
pages = [ &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=1&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=2&#39; ]  According to AllSides&amp;rsquo; robots.txt we need to make sure we wait ten seconds before each request.
Our loop will: - request a page - parse the page - wait ten seconds - repeat for next page.
Remember, we&amp;rsquo;ve already tested our parsing above on a page that was cached locally so we know it works. You&amp;rsquo;ll want to make sure to do this before making a loop that performs requests to prevent having to reloop if you forgot to parse something.
By combining all the steps we&amp;rsquo;ve done up to this point and adding a loop over pages, here&amp;rsquo;s how it looks:
from time import sleep data= [] for page in pages: r = requests.get(page) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) rows = soup.select(&#39;tbody tr&#39;) for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d) sleep(10)  Now we have a list of dictionaries for each row on all three pages.
To cap it off, we want to get the real URL to the news source, not just the link to their presence on AllSides. To do this, we will need to get the AllSides page and look for the link.
If we go to ABC News&amp;rsquo; page there&amp;rsquo;s a row of external links to Facebook, Twitter, Wikipedia, and the ABC News website. The HTML for that sections looks like this:
&amp;lt;div class=&amp;quot;row-fluid source-links gray-bg-box&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;container&amp;quot;&amp;gt;	&amp;lt;a href=&amp;quot;https://www.facebook.com/ABCNews/&amp;quot; class=&amp;quot;facebook&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-facebook&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;Facebook&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://twitter.com/ABC&amp;quot; class=&amp;quot;twitter&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-twitter&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Twitter&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/ABC_News&amp;quot; class=&amp;quot;wikipedia&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-wikipedia-w&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Wikipedia&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;http://abcnews.go.com/&amp;quot; class=&amp;quot;www&amp;quot;&amp;gt;&amp;lt;i class=&amp;quot;fa fa-globe&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt; &amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;ABC News&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;/contact&amp;quot; class=&amp;quot;improve-this-page&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-line-chart&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Improve this page&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;  Notice the anchor tag (&amp;lt;a&amp;gt;) that contains the link to ABC News has a class of &amp;ldquo;www&amp;rdquo;. Pretty easy to get with what we&amp;rsquo;ve already learned:
website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;]  So let&amp;rsquo;s make another loop to request the AllSides page and get links for each news source. Unfortunately, some pages don&amp;rsquo;t have a link in this grey bar to the news source, which brings up a good point: always account for elements to randomly not exist.
Up until now we&amp;rsquo;ve assumed elements exist in the tables we scraped, but it&amp;rsquo;s always a good idea to program scrapers in way so they don&amp;rsquo;t break when an element goes missing.
Using select_one or select will always return None or an empty list if nothing is found, so in this loop we&amp;rsquo;ll check if we found the website element or not so it doesn&amp;rsquo;t throw an Exception when trying to access the href attribute.
Finally, since there&amp;rsquo;s 265 news source pages and the wait time between pages is 10 seconds, it&amp;rsquo;s going to take ~44 minutes to do this. Instead of blindly not knowing our progress, let&amp;rsquo;s use the tqdm library to give us a nice progress bar:
!pip install tqdm  from tqdm import tqdm_notebook for d in tqdm_notebook(data): r = requests.get(d[&#39;allsides_page&#39;]) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) try: website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;] d[&#39;website&#39;] = website except TypeError: pass sleep(10)  tqdm is a little weird at first, but essentially tqdm_notebook is just wrapping around our data list to produce a progress bar. We are still able to access each dictionary, d, just as we would normally. Note that tqdm_notebook is only for Jupyter notebooks. In regular editors you&amp;rsquo;ll just import tqdm from tqdm and use tqdm instead.
Saving our data So what do we have now? At this moment, data is a list of dictionaries, each of which contains all the data from the tables as well as the websites from each individual news source&amp;rsquo;s page on AllSides.
The first thing we&amp;rsquo;ll want to do now is save that data to a file so we don&amp;rsquo;t have to make those requests again. We&amp;rsquo;ll be storing the data as JSON since it&amp;rsquo;s already in that form anyway:
import json with open(&#39;allsides.json&#39;, &#39;w&#39;) as f: json.dump(data, f)  To load it back in when you need it:
with open(&#39;allsides.json&#39;, &#39;r&#39;) as f: data = json.load(f)  If you&amp;rsquo;re not familiar with JSON, just quickly open allsides.json in an editor and see what it looks like. It should look almost exactly like what data looks like if we print it in Python: a list of dictionaries.
Brief Data Analysis Before ending this article I think it would be worthwhile to actually see what&amp;rsquo;s interesting about this data we just retrieved. So, let&amp;rsquo;s answer a couple of questions.
Which ratings for outlets does the community absolutely agree on?
To find where the community absolutely agrees we can do a simple list comprehension that checks each dict for the agreeance text we want:
abs_agree = [d for d in data if d[&#39;agreeance_text&#39;] == &#39;absolutely agrees&#39;] print(f&amp;quot;{&#39;Outlet&#39;:&amp;lt;20} {&#39;Bias&#39;:&amp;lt;20}&amp;quot;) print(&amp;quot;-&amp;quot; * 30) for d in abs_agree: print(f&amp;quot;{d[&#39;name&#39;]:&amp;lt;20} {d[&#39;bias&#39;]:&amp;lt;20}&amp;quot;)  Using some string formatting we can make it look somewhat tabular. Interestingly, C-SPAN is the only center bias that the community absolutely agrees on. The others for left and right aren&amp;rsquo;t that surprising.
Making analysis easier with Pandas Which ratings for outlets does the community absolutely disagree on?
To make analysis a little easier, we can also load our JSON data into a Pandas DataFrame as well. This is easy with Pandas since they have a simple function for reading JSON into a DataFrame.
As an aside, if you&amp;rsquo;ve never used Pandas, Matplotlib, or any of the other data science libraries, I would definitely recommend checking out [Jose Portilla&amp;rsquo;s data science course]() for a great intro to these tools and many machine learning concepts.
Now to the DataFrame:
import pandas as pd df = pd.read_json(open(&#39;allsides.json&#39;, &#39;r&#39;)) df.set_index(&#39;name&#39;, inplace=True) df.head()  Now filter the DataFrame by &amp;ldquo;agreeance_text&amp;rdquo;:
df[df[&#39;agreeance_text&#39;] == &#39;strongly disagrees&#39;]  It looks like much of the community disagrees strongly with certain outlets being rated with a &amp;ldquo;center&amp;rdquo; bias.
Let&amp;rsquo;s make a quick visualization of agreeance. Since there&amp;rsquo;s too many news sources to plot so let&amp;rsquo;s pull only those with the most votes. To do that, we can make a new column that counts the total votes and then sort by that value:
df[&#39;total_votes&#39;] = df[&#39;agree&#39;] &#43; df[&#39;disagree&#39;] df.sort_values(&#39;total_votes&#39;, ascending=False, inplace=True) df.head(10)  Visualizing the data To make a bar plot we&amp;rsquo;ll use Matplotlib with Seaborn&amp;rsquo;s dark grid style:
import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-darkgrid&#39;)  As mentioned above, we have too many news outlets to plot comfortably, so just make a copy of the top 25 and place it in a new df2 variable:
df2 = df.head(25).copy() df2.head()  With the top 25 news sources by amount of feedback, let&amp;rsquo;s create a stacked bar chart where the number of agrees are stacked on top of the number of disagrees. This makes the total height of the bar the total amount of feedback.
Below, we first create a figure and axes, plot the agree bars, plot the disagree bars on top of the agrees using bottom, then set various text features:
fig, ax = plt.subplots(figsize=(20, 10)) ax.bar(df2.index, df2[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(df2.index, df2[&#39;disagree&#39;], bottom=df2[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) ax.set_ylabel = &#39;Total feedback&#39; plt.yticks(fontsize=&#39;x-large&#39;) plt.xticks(rotation=60, ha=&#39;right&#39;, fontsize=&#39;x-large&#39;, rotation_mode=&#39;anchor&#39;) plt.legend([&#39;Agree&#39;, &#39;Disagree&#39;], fontsize=&#39;xx-large&#39;) plt.title(&#39;AllSides Bias Rating vs. Community Feedback&#39;, fontsize=&#39;xx-large&#39;) plt.show()  For a slightly more complex version, let&amp;rsquo;s make a subplot for each bias and plot the respective news sources.
This time we&amp;rsquo;ll make a new copy of the original DataFrame beforehand since we can plot more news outlets now.
Instead of making one axes, we&amp;rsquo;ll create a new one for each bias to make six total subplots:
df3 = df.copy() fig = plt.figure(figsize=(15,15)) biases = df3[&#39;bias&#39;].unique() for i, bias in enumerate(biases): # Get top 10 news sources for this bias and sort index alphabetically temp_df = df3[df3[&#39;bias&#39;] == bias].iloc[:10] temp_df.sort_index(inplace=True) # Get max votes, i.e. the y value for tallest bar in this temp dataframe max_votes = temp_df[&#39;total_votes&#39;].max() # Add a new subplot in the correct grid position ax = fig.add_subplot(len(biases) / 2, 2, i &#43; 1) # Create the stacked bars ax.bar(temp_df.index, temp_df[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(temp_df.index, temp_df[&#39;disagree&#39;], bottom=temp_df[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) # Place text for the ratio on top of each bar for x, y, ratio in zip(ax.get_xticks(), temp_df[&#39;total_votes&#39;], temp_df[&#39;agree_ratio&#39;]): ax.text(x, y &#43; (0.02 * max_votes), f&amp;quot;{ratio:.2f}&amp;quot;, ha=&#39;center&#39;) ax.set_ylabel(&#39;Total feedback&#39;) ax.set_title(bias.title()) # Make y limit larger to compensate for text on bars ax.set_ylim(0, max_votes &#43; (0.12 * max_votes)) # Rotate tick labels so they don&#39;t overlap plt.setp(ax.get_xticklabels(), rotation=30, ha=&#39;right&#39;) plt.tight_layout(w_pad=3.0, h_pad=1.0) plt.show()  Hopefully the comments help with how these plots were created. We&amp;rsquo;re just looping through each unique bias and adding a subplot to the figure.
When interpreting these plots keep in mind that the y-axis has different scales for each subplot. Overall it&amp;rsquo;s a nice way to see which outlets have a lot of votes and where the most disagreement is. This is what makes scraping so much fun!
Final words We have the tools to make some fairly complex web scrapers now, but there&amp;rsquo;s still the issue with Javascript rendering. This is something that deserves its own article, but for now we can do quite a lot.
There&amp;rsquo;s also some project organization that needs to occur when making this into a more easily runnable program. We need to pull it out of this notebook and code in command-line arguments if we plan to run it often for updates.
These sorts of things will be addressed later when we build more complex scrapers, but feel free to let me know in the comments of anything in particular you&amp;rsquo;re interested in learning about.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_4/web-scraping-request/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section><section class="section">
                    <div class="container">
                        <div class="columns">
                        <div class="column is-centered-tablet-portrait">
                            <h1 class="title section-title">Web Scraping</h1>
                            <h5 class="subtitle is-5 is-muted"></h5>
                            <div class="divider"></div>        
                            <div class="section">
                            <div class="container">
                                <div class="columns"></div>
                                        <div class="columns"><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_1/web-scraping-request/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup
      </h3>
      <p class="refresh-summary"> After the 2016 election I became much more interested in media bias and the manipulation of individuals through advertising. This series will be a walkthrough of a web scraping project that monitors political news from both left and right wing media outlets and performs an analysis on the rhetoric being used, the ads being displayed, and the sentiment of certain topics.
The first part of the series will we be getting media bias data and focus on only working locally on your computer, but if you wish to learn how to deploy something like this into production, feel free to leave a comment and let me know.
Limit your impact when scraping Every time you load a web page you&amp;rsquo;re making a request to a server, and when you&amp;rsquo;re just a human with a browser there&amp;rsquo;s not a lot of damage you can do. With a Python script that can execute thousands of requests a second if coded incorrectly, you could end up costing the website owner a lot of money and possibly bring down their site (see Denial-of-service attack (DoS)).
With this in mind, we want to be very careful with how we program scrapers to avoid crashing sites and causing damage. Every time we scrape a website we want to attempt to make only one request per page. We don&amp;rsquo;t want to be making a request every time our parsing or other logic doesn&amp;rsquo;t work out, so we need to parse only after we&amp;rsquo;ve saved the page locally.
If I&amp;rsquo;m just doing some quick tests, I&amp;rsquo;ll usually start out in a Jupyter notebook because you can request a web page in one cell and have that web page available to every cell below it without making a new request. Since this article is available as a Jupyter notebook, you will see how it works if you choose that format.
How to save HTML locally After we make a request and retrieve a web page&amp;rsquo;s content, we can store that content locally with Python&amp;rsquo;s open() function. To do so we need to use the argument wb, which stands for &amp;ldquo;write bytes&amp;rdquo;. This let&amp;rsquo;s us avoid any encoding issues when saving.
Below is a function that wraps the open() function to reduce a lot of repetitive coding later on:
def save_html(html, path): with open(path, &#39;wb&#39;) as f: f.write(html) save_html(r.content, &#39;google_com&#39;)  Assume we have captured the HTML from google.com in html, which you&amp;rsquo;ll see later how to do. After running this function we will now have a file in the same directory as this notebook called google_com that contains the HTML.
How to open/read HTML from a local file To retrieve our saved file we&amp;rsquo;ll make another function to wrap reading the HTML back into html. We need to use rb for &amp;ldquo;read bytes&amp;rdquo; in this case.
def open_html(path): with open(path, &#39;rb&#39;) as f: return f.read() html = open_html(&#39;google_com&#39;)  The open function is doing just the opposite: read the HTML from google_com. If our script fails, notebook closes, computer shutsdown, etc., we no longer need to request google.com again, lessening our impact on their servers. While it doesn&amp;rsquo;t matter much with Google since they have a lot of resources, smaller sites with smaller servers will benefit from this.
I save almost every page and parse later when web scraping as a safety precaution.
Follow the rules for scrapers and bots Each site usually has a robots.txt on the root of their domain. This is where the website owner explicitly states what bots are allowed to do on their site. Simply go to example.com/robots.txt and you should find a text file that looks something like this:
User-agent: * Crawl-delay: 10 Allow: /pages/ Disallow: /scripts/ # more stuff  The User-agent field is the name of the bot and the rules that follow are what the bot should follow. Some robots.txt will have many User-agents with different rules. Common bots are googlebot, bingbot, and applebot, all of which you can probably guess the purpose and origin of.
We don&amp;rsquo;t really need to provide a User-agent when scraping, so User-agent: * is what we would follow. A * means that the following rules apply to all bots (that&amp;rsquo;s us).
The Crawl-delay tells us the number of seconds to wait before requests, so in this example we need to wait 10 seconds before making another request.
Allow gives us specific URLs we&amp;rsquo;re allowed to request with bots, and vice versa for Disallow. In this example we&amp;rsquo;re allowed to request anything in the /pages/ subfolder which means anything that starts with example.com/pages/. On the other hand, we are disallowed from scraping anything from the /scripts/ subfolder.
Many times you&amp;rsquo;ll see a * next to Allow or Disallow which means you are either allowed or not allowed to scrape everything on the site.
Sometimes there will be a disallow all pages followed by allowed pages like this:
Disallow: * Allow: /pages/  This means that you&amp;rsquo;re not allowed to scrape anything except the subfolder /pages/. Essentially, you just want to read the rules in order where the next rule overrides the previous rule.
Scraping Project: Getting Media Bias Data This project will primarily be run through a Jupyter notebook, which is done for teaching purposes and is not the usual way scrapers are programmed. After showing you the pieces, we&amp;rsquo;ll put it all together into a Python script that can be run from command line or your IDE of choice.
Making web requests With Python&amp;rsquo;s requests library we&amp;rsquo;re getting a web page by using get() on the URL. The response r contains many things, but using r.content will give us the HTML. Once we have the HTML we can then parse it for the data we&amp;rsquo;re interested in analyzing.
There&amp;rsquo;s an interesting website called AllSides that has a media bias rating table where users can agree or disagree with the rating.
Since there&amp;rsquo;s nothing in their robots.txt that disallows us from scraping this section of the site, I&amp;rsquo;m assuming it&amp;rsquo;s okay to go ahead and extract this data for our project. Let&amp;rsquo;s request the this first page:
!pip install requests  import requests url = &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39; r = requests.get(url) print(r.content[:100])  Since we essentially have a giant string of HTML, we can print a slice of 100 characters to confirm we have the source of the page. Let&amp;rsquo;s start extracting data.
Parsing HTML with BeautifulSoup What does BeautifulSoup do? We used requests to get the page from the AllSides server, but now we need the BeautifulSoup library to parse HTML and XML. When we pass our HTML to the BeautifulSoup constructor we get an object in return that we can then navigate like the original tree structure of the DOM.
This way we can find elements using names of tags, classes, IDs, and through relationships to other elements, like getting the children and siblings of elements.
Creating a new soup object We create a new BeautifulSoup object by passing the constructor our newly acquired HTML content and the type of parser we want to use:
!pip install beautifulsoup4  from bs4 import BeautifulSoup soup = BeautifulSoup(r.content, &#39;html.parser&#39;)  This soup object defines a bunch of methods — many of which can achieve the same result — that we can use to extract data from the HTML. Let&amp;rsquo;s start with finding elements.
Finding elements and data To find elements and data inside our HTML we&amp;rsquo;ll be using select_one, which returns a single element, and select, which returns a list of elements (even if only one item exists). Both of these methods use CSS selectors to find elements, so if you&amp;rsquo;re rusty on how CSS selectors work here&amp;rsquo;s a quick refresher:
A CSS selector refresher 1. To get a tag, such as &amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;, &amp;lt;body&amp;gt;&amp;lt;/body&amp;gt;, use the naked name for the tag. E.g. select_one(&#39;a&#39;) gets an anchor/link element, select_one(&#39;body&#39;) gets the body element 2. .temp gets an element with a class of temp, E.g. to get &amp;lt;a class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp&#39;) 3. #temp gets an element with an id of temp, E.g. to get &amp;lt;a id=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;#temp&#39;) 4. .temp.example gets an element with both classes temp and example, E.g. to get &amp;lt;a class=&amp;quot;temp example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp.example&#39;) 5. .temp a gets an anchor element nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp a&#39;). Note the space between .temp and a. 6. .temp .example gets an element with class example nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a class=&amp;quot;example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp .example&#39;). Again, note the space between .temp and .example. The space tells the selector that the class after the space is a child of the class before the space. 7. ids, such as &amp;lt;a id=one&amp;gt;&amp;lt;/a&amp;gt;, are unique so you can usually use the id selector by itself to get the right element. No need to do nested selectors when using ids.
There&amp;rsquo;s many more selectors for for doing various tasks, like selecting certain child elements, specific links, etc., that you can look up when needed. The selectors above get us pretty close to everything we would need for now.
Tips on figuring out how to select certain elements
Most browsers have a quick way of finding the selector for an element using their developer tools. In Chrome, we can quickly find selectors for elements by 1. Right-click on the the element then select &amp;ldquo;Inspect&amp;rdquo; in the menu. Developer tools opens and and highlights the element we right-clicked 2. Right-click the code element in developer tools, hover over &amp;ldquo;Copy&amp;rdquo; in the menu, then click &amp;ldquo;Copy selector&amp;rdquo;
Sometimes it&amp;rsquo;ll be a little off and we need to scan up a few elements to find the right one. Here&amp;rsquo;s what it looks like to find the selector and Xpath, another type of selector, in Chrome:


Let&amp;rsquo;s start! Getting data out of a table Our data is housed in a table on AllSides, and by inspecting the header element we can find the code that renders the table and rows. What we need to do is select all the rows from the table and then parse out the information from each row.


Simplifying the table&amp;rsquo;s HTML, the structure looks like this (comments &amp;lt;!-- --&amp;gt; added by me):
&amp;lt;table&amp;gt; &amp;lt;thead&amp;gt; &amp;lt;!-- header information --&amp;gt; &amp;lt;/thead&amp;gt; &amp;lt;tbody&amp;gt; &amp;lt;tr class=&amp;quot;odd views-row-first&amp;quot;&amp;gt; &amp;lt;!-- begin table row --&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- outlet name --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- bias data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing-1 what-do-you-think&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree buttons --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;!-- end table row --&amp;gt; &amp;lt;!-- more rows --&amp;gt; &amp;lt;/tbody&amp;gt; &amp;lt;/table&amp;gt;  So to get each row, we just select all &amp;lt;tr&amp;gt; inside &amp;lt;tbody&amp;gt;:
rows = soup.select(&#39;tbody tr&#39;)  tbody tr tells the selector to extract all &amp;lt;tr&amp;gt; (table row) tags that are children of the &amp;lt;tbody&amp;gt; body tag. If there were more than one table on this page we would have to make a more specific selector, but since this is the only table, we&amp;rsquo;re good to go.
Now we have a list of HTML table rows that each contain four cells: - News source name and link - Bias data - Agreement buttons - Community feedback data
Below is a breakdown of how to extract each one.
News source name 

Let&amp;rsquo;s look at the first cell:
&amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/news-source/abc-news-media-bias&amp;quot;&amp;gt;ABC News&amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  The outlet name (ABC News) is the text of an anchor tag that&amp;rsquo;s nested inside a &amp;lt;td&amp;gt; tag, which is a cell — or table data tag.
Getting the outlet name is pretty easy: just get the first row in rows and run a select_one off that object:
row = rows[0] name = row.select_one(&#39;.source-title&#39;).text.strip() print(name)  The only class we needed to use in this case was .source-title since .views-field looks to be just a class each row is given for styling and doesn&amp;rsquo;t provide any uniqueness.
Notice that we didn&amp;rsquo;t need to worry about selecting the anchor tag a that contains the text. When we use .text is gets all text in that element, and since &amp;ldquo;ABC News&amp;rdquo; is the only text, that&amp;rsquo;s all we need to do. Bear in mind that using select or select_one will give you the whole element with the tags included, so we need .text to give us the text between the tags.
.strip() ensures all the whitespace surrounding the name is removed. This is a good thing to always do since many websites use whitespace as a way to visually pad the text inside elements.
You&amp;rsquo;ll notice that we can run BeautifulSoup methods right off one of the rows. That&amp;rsquo;s because the rows become their own BeautifulSoup objects when we make a select from another BeautifulSoup object. On the other hand, our name variable is no longer a BeautifulSoup object because we called .text.
News source page link We also need the link to this news source&amp;rsquo;s page on AllSides. If we look back at the HTML we&amp;rsquo;ll see that in this case we do want to select the anchor in order to get the href that contains the link, so let&amp;rsquo;s do that:
allsides_page = row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] allsides_page = &#39;https://www.allsides.com&#39; &#43; allsides_page print(allsides_page)  It is a relative path in the HTML, so we prepend the site&amp;rsquo;s URL to make it a link we can request later.
Getting the link was a bit different than just selecting an element. We had to access an attribute (href) of the element, which is done using brackets, like how we would access a Python dictionary. This will be the same for other attributes of elements, like src in images and videos.
Bias rating 

We can see that the rating is displayed as an image so how can we get the rating in words? Looking at the HTML notice the link that surrounds the image has the text we need:
&amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/media-bias/left-center&amp;quot;&amp;gt; &amp;lt;img src=&amp;quot;...&amp;quot; width=&amp;quot;144&amp;quot; height=&amp;quot;24&amp;quot; alt=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot; title=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot;&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  We could also pull the alt attribute, but the link looks easier. Let&amp;rsquo;s grab it:
bias = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;] bias = bias.split(&#39;/&#39;)[-1] print(bias)  Here we selected the anchor tag by using the class name and tag together: .views-field-field-bias-image is the class of the &amp;lt;td&amp;gt; and &amp;lt;a&amp;gt; is for the anchor nested inside.
After that we extract the href just like before, but now we only want the last part of the URL for the name of the bias so we split on slashes and get the last element of that split (left-center).
Community feedback data 

The last thing to scrape is the agree/disagree ratio from the community feedback area. The HTML of this cell is pretty convoluted due to the styling, but here&amp;rsquo;s the basic structure:
&amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;getratingval&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;rate-widget-4 rate-widget clear-block rate-average rate-widget-yesno&amp;quot; id=&amp;quot;rate-node-76-4-1&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;item-list&amp;quot;&amp;gt; &amp;lt;ul&amp;gt; &amp;lt;li class=&amp;quot;first&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-3&amp;quot;&amp;gt;agree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;li class=&amp;quot;last&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-4&amp;quot;&amp;gt;disagree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;rate-details&amp;quot;&amp;gt; &amp;lt;span class=&amp;quot;agree&amp;quot;&amp;gt;8241&amp;lt;/span&amp;gt;/&amp;lt;span class=&amp;quot;disagree&amp;quot;&amp;gt;6568&amp;lt;/span&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/td&amp;gt;  The numbers we want are located in two span elements in the last div. Both span elements have classes that are unique in this cell so we can use them to make the selection:
agree = row.select_one(&#39;.agree&#39;).text agree = int(agree) disagree = row.select_one(&#39;.disagree&#39;).text disagree = int(disagree) agree_ratio = agree / disagree print(f&amp;quot;Agree: {agree}, Disagree: {disagree}, Ratio {agree_ratio:.2f}&amp;quot;)  Using .text will return a string, so we need to convert them to integers in order to calculate the ratio.
Side note: If you&amp;rsquo;ve never seen this way of formatting print statements in Python, the f at the front allows us to insert variables right into the string using curly braces. The :.2f is a way to format floats to only show two decimals places.
If you look at the page in your browser you&amp;rsquo;ll notice that they say how much the community is in agreement by using &amp;ldquo;somewhat agree&amp;rdquo;, &amp;ldquo;strongly agree&amp;rdquo;, etc. so how do we get that? If we try to select it:
print(row.select_one(&#39;.community-feedback-rating-page&#39;))  It shows up as None because this element is rendered with Javascript and requests can&amp;rsquo;t pull HTML rendered with Javascript. We&amp;rsquo;ll be looking at how to get data rendered with JS in a later article, but since this is the only piece of information that&amp;rsquo;s rendered this way we can manually recreate the text.
To find the JS files they&amp;rsquo;re using, just CTRL&#43;F for &amp;ldquo;.js&amp;rdquo; in the page source and open the files in a new tab to look for that logic.
It turned out the logic was located in the eleventh JS file and they have a function that calculates the text and color with these parameters:
 Range Agreeance   $ratio  3$ absolutely agrees   $2 strongly agrees   $1.5 agrees   $1 somewhat agrees   $ratio = 1$ neutral   $0.67 somewhat disgrees   $0.5 disgrees   $0.33 strongly disagrees   $ratio \leq 0.33$ absolutely disagrees   Let&amp;rsquo;s make a function that replicates this logic:
def get_agreeance_text(ratio): if ratio &amp;gt; 3: return &amp;quot;absolutely agrees&amp;quot; elif 2 &amp;lt; ratio &amp;lt;= 3: return &amp;quot;strongly agrees&amp;quot; elif 1.5 &amp;lt; ratio &amp;lt;= 2: return &amp;quot;agrees&amp;quot; elif 1 &amp;lt; ratio &amp;lt;= 1.5: return &amp;quot;somewhat agrees&amp;quot; elif ratio == 1: return &amp;quot;neutral&amp;quot; elif 0.67 &amp;lt; ratio &amp;lt; 1: return &amp;quot;somewhat disagrees&amp;quot; elif 0.5 &amp;lt; ratio &amp;lt;= 0.67: return &amp;quot;disagrees&amp;quot; elif 0.33 &amp;lt; ratio &amp;lt;= 0.5: return &amp;quot;strongly disagrees&amp;quot; elif ratio &amp;lt;= 0.33: return &amp;quot;absolutely disagrees&amp;quot; else: return None print(get_agreeance_text(2.5))  Now that we have the general logic for a single row and we can generate the agreeance text, let&amp;rsquo;s create a loop that gets data from every row on the first page:
data= [] for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d)  In the loop we can combine any multi-step extractions into one to create the values in the least number of steps.
Our data list now contains a dictionary containing key information for every row.
print(data[0])  Keep in mind that this is still only the first page. The list on AllSides is three pages long as of this writing, so we need to modify this loop to get the other pages.
Requesting and parsing multiple pages Notice that the URLs for each page follow a pattern. The first page has no parameters on the URL, but the next pages do; specifically they attach a ?page=# to the URL where &amp;lsquo;#&amp;rsquo; is the page number.
Right now, the easiest way to get all pages is just to manually make a list of these three pages and loop over them. If we were working on a project with thousands of pages we might build a more automated way of constructing/finding the next URLs, but for now this works.
pages = [ &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=1&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=2&#39; ]  According to AllSides&amp;rsquo; robots.txt we need to make sure we wait ten seconds before each request.
Our loop will: - request a page - parse the page - wait ten seconds - repeat for next page.
Remember, we&amp;rsquo;ve already tested our parsing above on a page that was cached locally so we know it works. You&amp;rsquo;ll want to make sure to do this before making a loop that performs requests to prevent having to reloop if you forgot to parse something.
By combining all the steps we&amp;rsquo;ve done up to this point and adding a loop over pages, here&amp;rsquo;s how it looks:
from time import sleep data= [] for page in pages: r = requests.get(page) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) rows = soup.select(&#39;tbody tr&#39;) for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d) sleep(10)  Now we have a list of dictionaries for each row on all three pages.
To cap it off, we want to get the real URL to the news source, not just the link to their presence on AllSides. To do this, we will need to get the AllSides page and look for the link.
If we go to ABC News&amp;rsquo; page there&amp;rsquo;s a row of external links to Facebook, Twitter, Wikipedia, and the ABC News website. The HTML for that sections looks like this:
&amp;lt;div class=&amp;quot;row-fluid source-links gray-bg-box&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;container&amp;quot;&amp;gt;	&amp;lt;a href=&amp;quot;https://www.facebook.com/ABCNews/&amp;quot; class=&amp;quot;facebook&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-facebook&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;Facebook&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://twitter.com/ABC&amp;quot; class=&amp;quot;twitter&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-twitter&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Twitter&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/ABC_News&amp;quot; class=&amp;quot;wikipedia&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-wikipedia-w&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Wikipedia&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;http://abcnews.go.com/&amp;quot; class=&amp;quot;www&amp;quot;&amp;gt;&amp;lt;i class=&amp;quot;fa fa-globe&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt; &amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;ABC News&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;/contact&amp;quot; class=&amp;quot;improve-this-page&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-line-chart&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Improve this page&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;  Notice the anchor tag (&amp;lt;a&amp;gt;) that contains the link to ABC News has a class of &amp;ldquo;www&amp;rdquo;. Pretty easy to get with what we&amp;rsquo;ve already learned:
website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;]  So let&amp;rsquo;s make another loop to request the AllSides page and get links for each news source. Unfortunately, some pages don&amp;rsquo;t have a link in this grey bar to the news source, which brings up a good point: always account for elements to randomly not exist.
Up until now we&amp;rsquo;ve assumed elements exist in the tables we scraped, but it&amp;rsquo;s always a good idea to program scrapers in way so they don&amp;rsquo;t break when an element goes missing.
Using select_one or select will always return None or an empty list if nothing is found, so in this loop we&amp;rsquo;ll check if we found the website element or not so it doesn&amp;rsquo;t throw an Exception when trying to access the href attribute.
Finally, since there&amp;rsquo;s 265 news source pages and the wait time between pages is 10 seconds, it&amp;rsquo;s going to take ~44 minutes to do this. Instead of blindly not knowing our progress, let&amp;rsquo;s use the tqdm library to give us a nice progress bar:
!pip install tqdm  from tqdm import tqdm_notebook for d in tqdm_notebook(data): r = requests.get(d[&#39;allsides_page&#39;]) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) try: website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;] d[&#39;website&#39;] = website except TypeError: pass sleep(10)  tqdm is a little weird at first, but essentially tqdm_notebook is just wrapping around our data list to produce a progress bar. We are still able to access each dictionary, d, just as we would normally. Note that tqdm_notebook is only for Jupyter notebooks. In regular editors you&amp;rsquo;ll just import tqdm from tqdm and use tqdm instead.
Saving our data So what do we have now? At this moment, data is a list of dictionaries, each of which contains all the data from the tables as well as the websites from each individual news source&amp;rsquo;s page on AllSides.
The first thing we&amp;rsquo;ll want to do now is save that data to a file so we don&amp;rsquo;t have to make those requests again. We&amp;rsquo;ll be storing the data as JSON since it&amp;rsquo;s already in that form anyway:
import json with open(&#39;allsides.json&#39;, &#39;w&#39;) as f: json.dump(data, f)  To load it back in when you need it:
with open(&#39;allsides.json&#39;, &#39;r&#39;) as f: data = json.load(f)  If you&amp;rsquo;re not familiar with JSON, just quickly open allsides.json in an editor and see what it looks like. It should look almost exactly like what data looks like if we print it in Python: a list of dictionaries.
Brief Data Analysis Before ending this article I think it would be worthwhile to actually see what&amp;rsquo;s interesting about this data we just retrieved. So, let&amp;rsquo;s answer a couple of questions.
Which ratings for outlets does the community absolutely agree on?
To find where the community absolutely agrees we can do a simple list comprehension that checks each dict for the agreeance text we want:
abs_agree = [d for d in data if d[&#39;agreeance_text&#39;] == &#39;absolutely agrees&#39;] print(f&amp;quot;{&#39;Outlet&#39;:&amp;lt;20} {&#39;Bias&#39;:&amp;lt;20}&amp;quot;) print(&amp;quot;-&amp;quot; * 30) for d in abs_agree: print(f&amp;quot;{d[&#39;name&#39;]:&amp;lt;20} {d[&#39;bias&#39;]:&amp;lt;20}&amp;quot;)  Using some string formatting we can make it look somewhat tabular. Interestingly, C-SPAN is the only center bias that the community absolutely agrees on. The others for left and right aren&amp;rsquo;t that surprising.
Making analysis easier with Pandas Which ratings for outlets does the community absolutely disagree on?
To make analysis a little easier, we can also load our JSON data into a Pandas DataFrame as well. This is easy with Pandas since they have a simple function for reading JSON into a DataFrame.
As an aside, if you&amp;rsquo;ve never used Pandas, Matplotlib, or any of the other data science libraries, I would definitely recommend checking out [Jose Portilla&amp;rsquo;s data science course]() for a great intro to these tools and many machine learning concepts.
Now to the DataFrame:
import pandas as pd df = pd.read_json(open(&#39;allsides.json&#39;, &#39;r&#39;)) df.set_index(&#39;name&#39;, inplace=True) df.head()  Now filter the DataFrame by &amp;ldquo;agreeance_text&amp;rdquo;:
df[df[&#39;agreeance_text&#39;] == &#39;strongly disagrees&#39;]  It looks like much of the community disagrees strongly with certain outlets being rated with a &amp;ldquo;center&amp;rdquo; bias.
Let&amp;rsquo;s make a quick visualization of agreeance. Since there&amp;rsquo;s too many news sources to plot so let&amp;rsquo;s pull only those with the most votes. To do that, we can make a new column that counts the total votes and then sort by that value:
df[&#39;total_votes&#39;] = df[&#39;agree&#39;] &#43; df[&#39;disagree&#39;] df.sort_values(&#39;total_votes&#39;, ascending=False, inplace=True) df.head(10)  Visualizing the data To make a bar plot we&amp;rsquo;ll use Matplotlib with Seaborn&amp;rsquo;s dark grid style:
import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-darkgrid&#39;)  As mentioned above, we have too many news outlets to plot comfortably, so just make a copy of the top 25 and place it in a new df2 variable:
df2 = df.head(25).copy() df2.head()  With the top 25 news sources by amount of feedback, let&amp;rsquo;s create a stacked bar chart where the number of agrees are stacked on top of the number of disagrees. This makes the total height of the bar the total amount of feedback.
Below, we first create a figure and axes, plot the agree bars, plot the disagree bars on top of the agrees using bottom, then set various text features:
fig, ax = plt.subplots(figsize=(20, 10)) ax.bar(df2.index, df2[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(df2.index, df2[&#39;disagree&#39;], bottom=df2[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) ax.set_ylabel = &#39;Total feedback&#39; plt.yticks(fontsize=&#39;x-large&#39;) plt.xticks(rotation=60, ha=&#39;right&#39;, fontsize=&#39;x-large&#39;, rotation_mode=&#39;anchor&#39;) plt.legend([&#39;Agree&#39;, &#39;Disagree&#39;], fontsize=&#39;xx-large&#39;) plt.title(&#39;AllSides Bias Rating vs. Community Feedback&#39;, fontsize=&#39;xx-large&#39;) plt.show()  For a slightly more complex version, let&amp;rsquo;s make a subplot for each bias and plot the respective news sources.
This time we&amp;rsquo;ll make a new copy of the original DataFrame beforehand since we can plot more news outlets now.
Instead of making one axes, we&amp;rsquo;ll create a new one for each bias to make six total subplots:
df3 = df.copy() fig = plt.figure(figsize=(15,15)) biases = df3[&#39;bias&#39;].unique() for i, bias in enumerate(biases): # Get top 10 news sources for this bias and sort index alphabetically temp_df = df3[df3[&#39;bias&#39;] == bias].iloc[:10] temp_df.sort_index(inplace=True) # Get max votes, i.e. the y value for tallest bar in this temp dataframe max_votes = temp_df[&#39;total_votes&#39;].max() # Add a new subplot in the correct grid position ax = fig.add_subplot(len(biases) / 2, 2, i &#43; 1) # Create the stacked bars ax.bar(temp_df.index, temp_df[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(temp_df.index, temp_df[&#39;disagree&#39;], bottom=temp_df[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) # Place text for the ratio on top of each bar for x, y, ratio in zip(ax.get_xticks(), temp_df[&#39;total_votes&#39;], temp_df[&#39;agree_ratio&#39;]): ax.text(x, y &#43; (0.02 * max_votes), f&amp;quot;{ratio:.2f}&amp;quot;, ha=&#39;center&#39;) ax.set_ylabel(&#39;Total feedback&#39;) ax.set_title(bias.title()) # Make y limit larger to compensate for text on bars ax.set_ylim(0, max_votes &#43; (0.12 * max_votes)) # Rotate tick labels so they don&#39;t overlap plt.setp(ax.get_xticklabels(), rotation=30, ha=&#39;right&#39;) plt.tight_layout(w_pad=3.0, h_pad=1.0) plt.show()  Hopefully the comments help with how these plots were created. We&amp;rsquo;re just looping through each unique bias and adding a subplot to the figure.
When interpreting these plots keep in mind that the y-axis has different scales for each subplot. Overall it&amp;rsquo;s a nice way to see which outlets have a lot of votes and where the most disagreement is. This is what makes scraping so much fun!
Final words We have the tools to make some fairly complex web scrapers now, but there&amp;rsquo;s still the issue with Javascript rendering. This is something that deserves its own article, but for now we can do quite a lot.
There&amp;rsquo;s also some project organization that needs to occur when making this into a more easily runnable program. We need to pull it out of this notebook and code in command-line arguments if we plan to run it often for updates.
These sorts of things will be addressed later when we build more complex scrapers, but feel free to let me know in the comments of anything in particular you&amp;rsquo;re interested in learning about.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_1/web-scraping-request/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div><div class="column is-4">
    <div class="box">
      <figure class="image is-3by2">
        <a href="/topic_2/subtopic_4/web-scraping-request/"><img src="/images/default_summary_hu2be732aee21a2469ead3fe1f2df8caca_2088687_600x0_resize_q75_box.jpg" alt=""></a>
      </figure>
      <h3 class="title is-5 refresh-summary-title">
        Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup
      </h3>
      <p class="refresh-summary"> After the 2016 election I became much more interested in media bias and the manipulation of individuals through advertising. This series will be a walkthrough of a web scraping project that monitors political news from both left and right wing media outlets and performs an analysis on the rhetoric being used, the ads being displayed, and the sentiment of certain topics.
The first part of the series will we be getting media bias data and focus on only working locally on your computer, but if you wish to learn how to deploy something like this into production, feel free to leave a comment and let me know.
Limit your impact when scraping Every time you load a web page you&amp;rsquo;re making a request to a server, and when you&amp;rsquo;re just a human with a browser there&amp;rsquo;s not a lot of damage you can do. With a Python script that can execute thousands of requests a second if coded incorrectly, you could end up costing the website owner a lot of money and possibly bring down their site (see Denial-of-service attack (DoS)).
With this in mind, we want to be very careful with how we program scrapers to avoid crashing sites and causing damage. Every time we scrape a website we want to attempt to make only one request per page. We don&amp;rsquo;t want to be making a request every time our parsing or other logic doesn&amp;rsquo;t work out, so we need to parse only after we&amp;rsquo;ve saved the page locally.
If I&amp;rsquo;m just doing some quick tests, I&amp;rsquo;ll usually start out in a Jupyter notebook because you can request a web page in one cell and have that web page available to every cell below it without making a new request. Since this article is available as a Jupyter notebook, you will see how it works if you choose that format.
How to save HTML locally After we make a request and retrieve a web page&amp;rsquo;s content, we can store that content locally with Python&amp;rsquo;s open() function. To do so we need to use the argument wb, which stands for &amp;ldquo;write bytes&amp;rdquo;. This let&amp;rsquo;s us avoid any encoding issues when saving.
Below is a function that wraps the open() function to reduce a lot of repetitive coding later on:
def save_html(html, path): with open(path, &#39;wb&#39;) as f: f.write(html) save_html(r.content, &#39;google_com&#39;)  Assume we have captured the HTML from google.com in html, which you&amp;rsquo;ll see later how to do. After running this function we will now have a file in the same directory as this notebook called google_com that contains the HTML.
How to open/read HTML from a local file To retrieve our saved file we&amp;rsquo;ll make another function to wrap reading the HTML back into html. We need to use rb for &amp;ldquo;read bytes&amp;rdquo; in this case.
def open_html(path): with open(path, &#39;rb&#39;) as f: return f.read() html = open_html(&#39;google_com&#39;)  The open function is doing just the opposite: read the HTML from google_com. If our script fails, notebook closes, computer shutsdown, etc., we no longer need to request google.com again, lessening our impact on their servers. While it doesn&amp;rsquo;t matter much with Google since they have a lot of resources, smaller sites with smaller servers will benefit from this.
I save almost every page and parse later when web scraping as a safety precaution.
Follow the rules for scrapers and bots Each site usually has a robots.txt on the root of their domain. This is where the website owner explicitly states what bots are allowed to do on their site. Simply go to example.com/robots.txt and you should find a text file that looks something like this:
User-agent: * Crawl-delay: 10 Allow: /pages/ Disallow: /scripts/ # more stuff  The User-agent field is the name of the bot and the rules that follow are what the bot should follow. Some robots.txt will have many User-agents with different rules. Common bots are googlebot, bingbot, and applebot, all of which you can probably guess the purpose and origin of.
We don&amp;rsquo;t really need to provide a User-agent when scraping, so User-agent: * is what we would follow. A * means that the following rules apply to all bots (that&amp;rsquo;s us).
The Crawl-delay tells us the number of seconds to wait before requests, so in this example we need to wait 10 seconds before making another request.
Allow gives us specific URLs we&amp;rsquo;re allowed to request with bots, and vice versa for Disallow. In this example we&amp;rsquo;re allowed to request anything in the /pages/ subfolder which means anything that starts with example.com/pages/. On the other hand, we are disallowed from scraping anything from the /scripts/ subfolder.
Many times you&amp;rsquo;ll see a * next to Allow or Disallow which means you are either allowed or not allowed to scrape everything on the site.
Sometimes there will be a disallow all pages followed by allowed pages like this:
Disallow: * Allow: /pages/  This means that you&amp;rsquo;re not allowed to scrape anything except the subfolder /pages/. Essentially, you just want to read the rules in order where the next rule overrides the previous rule.
Scraping Project: Getting Media Bias Data This project will primarily be run through a Jupyter notebook, which is done for teaching purposes and is not the usual way scrapers are programmed. After showing you the pieces, we&amp;rsquo;ll put it all together into a Python script that can be run from command line or your IDE of choice.
Making web requests With Python&amp;rsquo;s requests library we&amp;rsquo;re getting a web page by using get() on the URL. The response r contains many things, but using r.content will give us the HTML. Once we have the HTML we can then parse it for the data we&amp;rsquo;re interested in analyzing.
There&amp;rsquo;s an interesting website called AllSides that has a media bias rating table where users can agree or disagree with the rating.
Since there&amp;rsquo;s nothing in their robots.txt that disallows us from scraping this section of the site, I&amp;rsquo;m assuming it&amp;rsquo;s okay to go ahead and extract this data for our project. Let&amp;rsquo;s request the this first page:
!pip install requests  import requests url = &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39; r = requests.get(url) print(r.content[:100])  Since we essentially have a giant string of HTML, we can print a slice of 100 characters to confirm we have the source of the page. Let&amp;rsquo;s start extracting data.
Parsing HTML with BeautifulSoup What does BeautifulSoup do? We used requests to get the page from the AllSides server, but now we need the BeautifulSoup library to parse HTML and XML. When we pass our HTML to the BeautifulSoup constructor we get an object in return that we can then navigate like the original tree structure of the DOM.
This way we can find elements using names of tags, classes, IDs, and through relationships to other elements, like getting the children and siblings of elements.
Creating a new soup object We create a new BeautifulSoup object by passing the constructor our newly acquired HTML content and the type of parser we want to use:
!pip install beautifulsoup4  from bs4 import BeautifulSoup soup = BeautifulSoup(r.content, &#39;html.parser&#39;)  This soup object defines a bunch of methods — many of which can achieve the same result — that we can use to extract data from the HTML. Let&amp;rsquo;s start with finding elements.
Finding elements and data To find elements and data inside our HTML we&amp;rsquo;ll be using select_one, which returns a single element, and select, which returns a list of elements (even if only one item exists). Both of these methods use CSS selectors to find elements, so if you&amp;rsquo;re rusty on how CSS selectors work here&amp;rsquo;s a quick refresher:
A CSS selector refresher 1. To get a tag, such as &amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;, &amp;lt;body&amp;gt;&amp;lt;/body&amp;gt;, use the naked name for the tag. E.g. select_one(&#39;a&#39;) gets an anchor/link element, select_one(&#39;body&#39;) gets the body element 2. .temp gets an element with a class of temp, E.g. to get &amp;lt;a class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp&#39;) 3. #temp gets an element with an id of temp, E.g. to get &amp;lt;a id=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;#temp&#39;) 4. .temp.example gets an element with both classes temp and example, E.g. to get &amp;lt;a class=&amp;quot;temp example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; use select_one(&#39;.temp.example&#39;) 5. .temp a gets an anchor element nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp a&#39;). Note the space between .temp and a. 6. .temp .example gets an element with class example nested inside of a parent element with class temp, E.g. to get &amp;lt;div class=&amp;quot;temp&amp;quot;&amp;gt;&amp;lt;a class=&amp;quot;example&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt; use select_one(&#39;.temp .example&#39;). Again, note the space between .temp and .example. The space tells the selector that the class after the space is a child of the class before the space. 7. ids, such as &amp;lt;a id=one&amp;gt;&amp;lt;/a&amp;gt;, are unique so you can usually use the id selector by itself to get the right element. No need to do nested selectors when using ids.
There&amp;rsquo;s many more selectors for for doing various tasks, like selecting certain child elements, specific links, etc., that you can look up when needed. The selectors above get us pretty close to everything we would need for now.
Tips on figuring out how to select certain elements
Most browsers have a quick way of finding the selector for an element using their developer tools. In Chrome, we can quickly find selectors for elements by 1. Right-click on the the element then select &amp;ldquo;Inspect&amp;rdquo; in the menu. Developer tools opens and and highlights the element we right-clicked 2. Right-click the code element in developer tools, hover over &amp;ldquo;Copy&amp;rdquo; in the menu, then click &amp;ldquo;Copy selector&amp;rdquo;
Sometimes it&amp;rsquo;ll be a little off and we need to scan up a few elements to find the right one. Here&amp;rsquo;s what it looks like to find the selector and Xpath, another type of selector, in Chrome:


Let&amp;rsquo;s start! Getting data out of a table Our data is housed in a table on AllSides, and by inspecting the header element we can find the code that renders the table and rows. What we need to do is select all the rows from the table and then parse out the information from each row.


Simplifying the table&amp;rsquo;s HTML, the structure looks like this (comments &amp;lt;!-- --&amp;gt; added by me):
&amp;lt;table&amp;gt; &amp;lt;thead&amp;gt; &amp;lt;!-- header information --&amp;gt; &amp;lt;/thead&amp;gt; &amp;lt;tbody&amp;gt; &amp;lt;tr class=&amp;quot;odd views-row-first&amp;quot;&amp;gt; &amp;lt;!-- begin table row --&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- outlet name --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- bias data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing-1 what-do-you-think&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree buttons --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;!-- table cell --&amp;gt; &amp;lt;!-- agree / disagree data --&amp;gt; &amp;lt;/td&amp;gt; &amp;lt;/tr&amp;gt; &amp;lt;!-- end table row --&amp;gt; &amp;lt;!-- more rows --&amp;gt; &amp;lt;/tbody&amp;gt; &amp;lt;/table&amp;gt;  So to get each row, we just select all &amp;lt;tr&amp;gt; inside &amp;lt;tbody&amp;gt;:
rows = soup.select(&#39;tbody tr&#39;)  tbody tr tells the selector to extract all &amp;lt;tr&amp;gt; (table row) tags that are children of the &amp;lt;tbody&amp;gt; body tag. If there were more than one table on this page we would have to make a more specific selector, but since this is the only table, we&amp;rsquo;re good to go.
Now we have a list of HTML table rows that each contain four cells: - News source name and link - Bias data - Agreement buttons - Community feedback data
Below is a breakdown of how to extract each one.
News source name 

Let&amp;rsquo;s look at the first cell:
&amp;lt;td class=&amp;quot;views-field views-field-title source-title&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/news-source/abc-news-media-bias&amp;quot;&amp;gt;ABC News&amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  The outlet name (ABC News) is the text of an anchor tag that&amp;rsquo;s nested inside a &amp;lt;td&amp;gt; tag, which is a cell — or table data tag.
Getting the outlet name is pretty easy: just get the first row in rows and run a select_one off that object:
row = rows[0] name = row.select_one(&#39;.source-title&#39;).text.strip() print(name)  The only class we needed to use in this case was .source-title since .views-field looks to be just a class each row is given for styling and doesn&amp;rsquo;t provide any uniqueness.
Notice that we didn&amp;rsquo;t need to worry about selecting the anchor tag a that contains the text. When we use .text is gets all text in that element, and since &amp;ldquo;ABC News&amp;rdquo; is the only text, that&amp;rsquo;s all we need to do. Bear in mind that using select or select_one will give you the whole element with the tags included, so we need .text to give us the text between the tags.
.strip() ensures all the whitespace surrounding the name is removed. This is a good thing to always do since many websites use whitespace as a way to visually pad the text inside elements.
You&amp;rsquo;ll notice that we can run BeautifulSoup methods right off one of the rows. That&amp;rsquo;s because the rows become their own BeautifulSoup objects when we make a select from another BeautifulSoup object. On the other hand, our name variable is no longer a BeautifulSoup object because we called .text.
News source page link We also need the link to this news source&amp;rsquo;s page on AllSides. If we look back at the HTML we&amp;rsquo;ll see that in this case we do want to select the anchor in order to get the href that contains the link, so let&amp;rsquo;s do that:
allsides_page = row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] allsides_page = &#39;https://www.allsides.com&#39; &#43; allsides_page print(allsides_page)  It is a relative path in the HTML, so we prepend the site&amp;rsquo;s URL to make it a link we can request later.
Getting the link was a bit different than just selecting an element. We had to access an attribute (href) of the element, which is done using brackets, like how we would access a Python dictionary. This will be the same for other attributes of elements, like src in images and videos.
Bias rating 

We can see that the rating is displayed as an image so how can we get the rating in words? Looking at the HTML notice the link that surrounds the image has the text we need:
&amp;lt;td class=&amp;quot;views-field views-field-field-bias-image&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;/media-bias/left-center&amp;quot;&amp;gt; &amp;lt;img src=&amp;quot;...&amp;quot; width=&amp;quot;144&amp;quot; height=&amp;quot;24&amp;quot; alt=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot; title=&amp;quot;Political News Media Bias Rating: Lean Left&amp;quot;&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;  We could also pull the alt attribute, but the link looks easier. Let&amp;rsquo;s grab it:
bias = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;] bias = bias.split(&#39;/&#39;)[-1] print(bias)  Here we selected the anchor tag by using the class name and tag together: .views-field-field-bias-image is the class of the &amp;lt;td&amp;gt; and &amp;lt;a&amp;gt; is for the anchor nested inside.
After that we extract the href just like before, but now we only want the last part of the URL for the name of the bias so we split on slashes and get the last element of that split (left-center).
Community feedback data 

The last thing to scrape is the agree/disagree ratio from the community feedback area. The HTML of this cell is pretty convoluted due to the styling, but here&amp;rsquo;s the basic structure:
&amp;lt;td class=&amp;quot;views-field views-field-nothing community-feedback&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;getratingval&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;rate-widget-4 rate-widget clear-block rate-average rate-widget-yesno&amp;quot; id=&amp;quot;rate-node-76-4-1&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;item-list&amp;quot;&amp;gt; &amp;lt;ul&amp;gt; &amp;lt;li class=&amp;quot;first&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-3&amp;quot;&amp;gt;agree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;li class=&amp;quot;last&amp;quot;&amp;gt; &amp;lt;a class=&amp;quot;rate-button rate-btn&amp;quot; href=&amp;quot;...&amp;quot; id=&amp;quot;rate-button-4&amp;quot;&amp;gt;disagree&amp;lt;/a&amp;gt; &amp;lt;/li&amp;gt; &amp;lt;/ul&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;rate-details&amp;quot;&amp;gt; &amp;lt;span class=&amp;quot;agree&amp;quot;&amp;gt;8241&amp;lt;/span&amp;gt;/&amp;lt;span class=&amp;quot;disagree&amp;quot;&amp;gt;6568&amp;lt;/span&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/td&amp;gt;  The numbers we want are located in two span elements in the last div. Both span elements have classes that are unique in this cell so we can use them to make the selection:
agree = row.select_one(&#39;.agree&#39;).text agree = int(agree) disagree = row.select_one(&#39;.disagree&#39;).text disagree = int(disagree) agree_ratio = agree / disagree print(f&amp;quot;Agree: {agree}, Disagree: {disagree}, Ratio {agree_ratio:.2f}&amp;quot;)  Using .text will return a string, so we need to convert them to integers in order to calculate the ratio.
Side note: If you&amp;rsquo;ve never seen this way of formatting print statements in Python, the f at the front allows us to insert variables right into the string using curly braces. The :.2f is a way to format floats to only show two decimals places.
If you look at the page in your browser you&amp;rsquo;ll notice that they say how much the community is in agreement by using &amp;ldquo;somewhat agree&amp;rdquo;, &amp;ldquo;strongly agree&amp;rdquo;, etc. so how do we get that? If we try to select it:
print(row.select_one(&#39;.community-feedback-rating-page&#39;))  It shows up as None because this element is rendered with Javascript and requests can&amp;rsquo;t pull HTML rendered with Javascript. We&amp;rsquo;ll be looking at how to get data rendered with JS in a later article, but since this is the only piece of information that&amp;rsquo;s rendered this way we can manually recreate the text.
To find the JS files they&amp;rsquo;re using, just CTRL&#43;F for &amp;ldquo;.js&amp;rdquo; in the page source and open the files in a new tab to look for that logic.
It turned out the logic was located in the eleventh JS file and they have a function that calculates the text and color with these parameters:
 Range Agreeance   $ratio  3$ absolutely agrees   $2 strongly agrees   $1.5 agrees   $1 somewhat agrees   $ratio = 1$ neutral   $0.67 somewhat disgrees   $0.5 disgrees   $0.33 strongly disagrees   $ratio \leq 0.33$ absolutely disagrees   Let&amp;rsquo;s make a function that replicates this logic:
def get_agreeance_text(ratio): if ratio &amp;gt; 3: return &amp;quot;absolutely agrees&amp;quot; elif 2 &amp;lt; ratio &amp;lt;= 3: return &amp;quot;strongly agrees&amp;quot; elif 1.5 &amp;lt; ratio &amp;lt;= 2: return &amp;quot;agrees&amp;quot; elif 1 &amp;lt; ratio &amp;lt;= 1.5: return &amp;quot;somewhat agrees&amp;quot; elif ratio == 1: return &amp;quot;neutral&amp;quot; elif 0.67 &amp;lt; ratio &amp;lt; 1: return &amp;quot;somewhat disagrees&amp;quot; elif 0.5 &amp;lt; ratio &amp;lt;= 0.67: return &amp;quot;disagrees&amp;quot; elif 0.33 &amp;lt; ratio &amp;lt;= 0.5: return &amp;quot;strongly disagrees&amp;quot; elif ratio &amp;lt;= 0.33: return &amp;quot;absolutely disagrees&amp;quot; else: return None print(get_agreeance_text(2.5))  Now that we have the general logic for a single row and we can generate the agreeance text, let&amp;rsquo;s create a loop that gets data from every row on the first page:
data= [] for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d)  In the loop we can combine any multi-step extractions into one to create the values in the least number of steps.
Our data list now contains a dictionary containing key information for every row.
print(data[0])  Keep in mind that this is still only the first page. The list on AllSides is three pages long as of this writing, so we need to modify this loop to get the other pages.
Requesting and parsing multiple pages Notice that the URLs for each page follow a pattern. The first page has no parameters on the URL, but the next pages do; specifically they attach a ?page=# to the URL where &amp;lsquo;#&amp;rsquo; is the page number.
Right now, the easiest way to get all pages is just to manually make a list of these three pages and loop over them. If we were working on a project with thousands of pages we might build a more automated way of constructing/finding the next URLs, but for now this works.
pages = [ &#39;https://www.allsides.com/media-bias/media-bias-ratings&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=1&#39;, &#39;https://www.allsides.com/media-bias/media-bias-ratings?page=2&#39; ]  According to AllSides&amp;rsquo; robots.txt we need to make sure we wait ten seconds before each request.
Our loop will: - request a page - parse the page - wait ten seconds - repeat for next page.
Remember, we&amp;rsquo;ve already tested our parsing above on a page that was cached locally so we know it works. You&amp;rsquo;ll want to make sure to do this before making a loop that performs requests to prevent having to reloop if you forgot to parse something.
By combining all the steps we&amp;rsquo;ve done up to this point and adding a loop over pages, here&amp;rsquo;s how it looks:
from time import sleep data= [] for page in pages: r = requests.get(page) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) rows = soup.select(&#39;tbody tr&#39;) for row in rows: d = dict() d[&#39;name&#39;] = row.select_one(&#39;.source-title&#39;).text.strip() d[&#39;allsides_page&#39;] = &#39;https://www.allsides.com&#39; &#43; row.select_one(&#39;.source-title a&#39;)[&#39;href&#39;] d[&#39;bias&#39;] = row.select_one(&#39;.views-field-field-bias-image a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1] d[&#39;agree&#39;] = int(row.select_one(&#39;.agree&#39;).text) d[&#39;disagree&#39;] = int(row.select_one(&#39;.disagree&#39;).text) d[&#39;agree_ratio&#39;] = d[&#39;agree&#39;] / d[&#39;disagree&#39;] d[&#39;agreeance_text&#39;] = get_agreeance_text(d[&#39;agree_ratio&#39;]) data.append(d) sleep(10)  Now we have a list of dictionaries for each row on all three pages.
To cap it off, we want to get the real URL to the news source, not just the link to their presence on AllSides. To do this, we will need to get the AllSides page and look for the link.
If we go to ABC News&amp;rsquo; page there&amp;rsquo;s a row of external links to Facebook, Twitter, Wikipedia, and the ABC News website. The HTML for that sections looks like this:
&amp;lt;div class=&amp;quot;row-fluid source-links gray-bg-box&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;container&amp;quot;&amp;gt;	&amp;lt;a href=&amp;quot;https://www.facebook.com/ABCNews/&amp;quot; class=&amp;quot;facebook&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-facebook&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;Facebook&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://twitter.com/ABC&amp;quot; class=&amp;quot;twitter&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-twitter&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Twitter&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/ABC_News&amp;quot; class=&amp;quot;wikipedia&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-wikipedia-w&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Wikipedia&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;http://abcnews.go.com/&amp;quot; class=&amp;quot;www&amp;quot;&amp;gt;&amp;lt;i class=&amp;quot;fa fa-globe&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt; &amp;lt;/i&amp;gt;&amp;lt;span&amp;gt;ABC News&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;/contact&amp;quot; class=&amp;quot;improve-this-page&amp;quot;&amp;gt; &amp;lt;i class=&amp;quot;fa fa-line-chart&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/i&amp;gt; &amp;lt;span&amp;gt;Improve this page&amp;lt;/span&amp;gt; &amp;lt;/a&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;  Notice the anchor tag (&amp;lt;a&amp;gt;) that contains the link to ABC News has a class of &amp;ldquo;www&amp;rdquo;. Pretty easy to get with what we&amp;rsquo;ve already learned:
website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;]  So let&amp;rsquo;s make another loop to request the AllSides page and get links for each news source. Unfortunately, some pages don&amp;rsquo;t have a link in this grey bar to the news source, which brings up a good point: always account for elements to randomly not exist.
Up until now we&amp;rsquo;ve assumed elements exist in the tables we scraped, but it&amp;rsquo;s always a good idea to program scrapers in way so they don&amp;rsquo;t break when an element goes missing.
Using select_one or select will always return None or an empty list if nothing is found, so in this loop we&amp;rsquo;ll check if we found the website element or not so it doesn&amp;rsquo;t throw an Exception when trying to access the href attribute.
Finally, since there&amp;rsquo;s 265 news source pages and the wait time between pages is 10 seconds, it&amp;rsquo;s going to take ~44 minutes to do this. Instead of blindly not knowing our progress, let&amp;rsquo;s use the tqdm library to give us a nice progress bar:
!pip install tqdm  from tqdm import tqdm_notebook for d in tqdm_notebook(data): r = requests.get(d[&#39;allsides_page&#39;]) soup = BeautifulSoup(r.content, &#39;html.parser&#39;) try: website = soup.select_one(&#39;.www&#39;)[&#39;href&#39;] d[&#39;website&#39;] = website except TypeError: pass sleep(10)  tqdm is a little weird at first, but essentially tqdm_notebook is just wrapping around our data list to produce a progress bar. We are still able to access each dictionary, d, just as we would normally. Note that tqdm_notebook is only for Jupyter notebooks. In regular editors you&amp;rsquo;ll just import tqdm from tqdm and use tqdm instead.
Saving our data So what do we have now? At this moment, data is a list of dictionaries, each of which contains all the data from the tables as well as the websites from each individual news source&amp;rsquo;s page on AllSides.
The first thing we&amp;rsquo;ll want to do now is save that data to a file so we don&amp;rsquo;t have to make those requests again. We&amp;rsquo;ll be storing the data as JSON since it&amp;rsquo;s already in that form anyway:
import json with open(&#39;allsides.json&#39;, &#39;w&#39;) as f: json.dump(data, f)  To load it back in when you need it:
with open(&#39;allsides.json&#39;, &#39;r&#39;) as f: data = json.load(f)  If you&amp;rsquo;re not familiar with JSON, just quickly open allsides.json in an editor and see what it looks like. It should look almost exactly like what data looks like if we print it in Python: a list of dictionaries.
Brief Data Analysis Before ending this article I think it would be worthwhile to actually see what&amp;rsquo;s interesting about this data we just retrieved. So, let&amp;rsquo;s answer a couple of questions.
Which ratings for outlets does the community absolutely agree on?
To find where the community absolutely agrees we can do a simple list comprehension that checks each dict for the agreeance text we want:
abs_agree = [d for d in data if d[&#39;agreeance_text&#39;] == &#39;absolutely agrees&#39;] print(f&amp;quot;{&#39;Outlet&#39;:&amp;lt;20} {&#39;Bias&#39;:&amp;lt;20}&amp;quot;) print(&amp;quot;-&amp;quot; * 30) for d in abs_agree: print(f&amp;quot;{d[&#39;name&#39;]:&amp;lt;20} {d[&#39;bias&#39;]:&amp;lt;20}&amp;quot;)  Using some string formatting we can make it look somewhat tabular. Interestingly, C-SPAN is the only center bias that the community absolutely agrees on. The others for left and right aren&amp;rsquo;t that surprising.
Making analysis easier with Pandas Which ratings for outlets does the community absolutely disagree on?
To make analysis a little easier, we can also load our JSON data into a Pandas DataFrame as well. This is easy with Pandas since they have a simple function for reading JSON into a DataFrame.
As an aside, if you&amp;rsquo;ve never used Pandas, Matplotlib, or any of the other data science libraries, I would definitely recommend checking out [Jose Portilla&amp;rsquo;s data science course]() for a great intro to these tools and many machine learning concepts.
Now to the DataFrame:
import pandas as pd df = pd.read_json(open(&#39;allsides.json&#39;, &#39;r&#39;)) df.set_index(&#39;name&#39;, inplace=True) df.head()  Now filter the DataFrame by &amp;ldquo;agreeance_text&amp;rdquo;:
df[df[&#39;agreeance_text&#39;] == &#39;strongly disagrees&#39;]  It looks like much of the community disagrees strongly with certain outlets being rated with a &amp;ldquo;center&amp;rdquo; bias.
Let&amp;rsquo;s make a quick visualization of agreeance. Since there&amp;rsquo;s too many news sources to plot so let&amp;rsquo;s pull only those with the most votes. To do that, we can make a new column that counts the total votes and then sort by that value:
df[&#39;total_votes&#39;] = df[&#39;agree&#39;] &#43; df[&#39;disagree&#39;] df.sort_values(&#39;total_votes&#39;, ascending=False, inplace=True) df.head(10)  Visualizing the data To make a bar plot we&amp;rsquo;ll use Matplotlib with Seaborn&amp;rsquo;s dark grid style:
import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-darkgrid&#39;)  As mentioned above, we have too many news outlets to plot comfortably, so just make a copy of the top 25 and place it in a new df2 variable:
df2 = df.head(25).copy() df2.head()  With the top 25 news sources by amount of feedback, let&amp;rsquo;s create a stacked bar chart where the number of agrees are stacked on top of the number of disagrees. This makes the total height of the bar the total amount of feedback.
Below, we first create a figure and axes, plot the agree bars, plot the disagree bars on top of the agrees using bottom, then set various text features:
fig, ax = plt.subplots(figsize=(20, 10)) ax.bar(df2.index, df2[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(df2.index, df2[&#39;disagree&#39;], bottom=df2[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) ax.set_ylabel = &#39;Total feedback&#39; plt.yticks(fontsize=&#39;x-large&#39;) plt.xticks(rotation=60, ha=&#39;right&#39;, fontsize=&#39;x-large&#39;, rotation_mode=&#39;anchor&#39;) plt.legend([&#39;Agree&#39;, &#39;Disagree&#39;], fontsize=&#39;xx-large&#39;) plt.title(&#39;AllSides Bias Rating vs. Community Feedback&#39;, fontsize=&#39;xx-large&#39;) plt.show()  For a slightly more complex version, let&amp;rsquo;s make a subplot for each bias and plot the respective news sources.
This time we&amp;rsquo;ll make a new copy of the original DataFrame beforehand since we can plot more news outlets now.
Instead of making one axes, we&amp;rsquo;ll create a new one for each bias to make six total subplots:
df3 = df.copy() fig = plt.figure(figsize=(15,15)) biases = df3[&#39;bias&#39;].unique() for i, bias in enumerate(biases): # Get top 10 news sources for this bias and sort index alphabetically temp_df = df3[df3[&#39;bias&#39;] == bias].iloc[:10] temp_df.sort_index(inplace=True) # Get max votes, i.e. the y value for tallest bar in this temp dataframe max_votes = temp_df[&#39;total_votes&#39;].max() # Add a new subplot in the correct grid position ax = fig.add_subplot(len(biases) / 2, 2, i &#43; 1) # Create the stacked bars ax.bar(temp_df.index, temp_df[&#39;agree&#39;], color=&#39;#5DAF83&#39;) ax.bar(temp_df.index, temp_df[&#39;disagree&#39;], bottom=temp_df[&#39;agree&#39;], color=&#39;#AF3B3B&#39;) # Place text for the ratio on top of each bar for x, y, ratio in zip(ax.get_xticks(), temp_df[&#39;total_votes&#39;], temp_df[&#39;agree_ratio&#39;]): ax.text(x, y &#43; (0.02 * max_votes), f&amp;quot;{ratio:.2f}&amp;quot;, ha=&#39;center&#39;) ax.set_ylabel(&#39;Total feedback&#39;) ax.set_title(bias.title()) # Make y limit larger to compensate for text on bars ax.set_ylim(0, max_votes &#43; (0.12 * max_votes)) # Rotate tick labels so they don&#39;t overlap plt.setp(ax.get_xticklabels(), rotation=30, ha=&#39;right&#39;) plt.tight_layout(w_pad=3.0, h_pad=1.0) plt.show()  Hopefully the comments help with how these plots were created. We&amp;rsquo;re just looping through each unique bias and adding a subplot to the figure.
When interpreting these plots keep in mind that the y-axis has different scales for each subplot. Overall it&amp;rsquo;s a nice way to see which outlets have a lot of votes and where the most disagreement is. This is what makes scraping so much fun!
Final words We have the tools to make some fairly complex web scrapers now, but there&amp;rsquo;s still the issue with Javascript rendering. This is something that deserves its own article, but for now we can do quite a lot.
There&amp;rsquo;s also some project organization that needs to occur when making this into a more easily runnable program. We need to pull it out of this notebook and code in command-line arguments if we plan to run it often for updates.
These sorts of things will be addressed later when we build more complex scrapers, but feel free to let me know in the comments of anything in particular you&amp;rsquo;re interested in learning about.
 </p> 
      <div class="action has-text-right">
        <a href="/topic_2/subtopic_4/web-scraping-request/" class="button is-primary">
                Read More
            </a>
      </div>
    </div>
  </div></div>
                            </div>
                            </div>
                        </div>
                        </div>
                    </div>
                </section>

    <footer class="footer footer-dark">
  <div class="container">
    <div class="columns">
      <div class="column">
        <img src="/footer.svg" alt="">
        
      </div>
      
    <div class="column">
        <div class="footer-column">
          <div class="footer-header">
              <h3>Website</h3>
          </div>
          <ul class="link-list">
            <li>
                <a href="/credits">
                  <span class="icon"><i class="fa fa-cube"></i></span>
                  Credits
                </a>
              </li>
            <li>
              <a href="/tags">
                <span class="icon"><i class="fa fa-tag"></i></span> 
                All Tags
              </a>
            </li>
          </ul>
        </div>
      </div>
    
      
      <div class="column">
        <div class="footer-column">
          <div class="footer-header">
              <h3>Contacts</h3>
          </div>
          <ul class="link-list">
            
            <li>
              <a href="https://www.linkedin.com/your_linkedin_profile/" target="_blank">
                <span class="icon"><i class="fa fa-linkedin"></i></span>
                Linkedin
              </a>
            </li>
            
            
            <li>
              <a href="mailto:name.surname@domain.com" target="_blank">
                <span class="icon"><i class="fa fa-envelope"></i></span>
                name.surname@domain.com
              </a>
            </li>
            
                   
                   
                   
          </ul>
        </div>
      </div>
      

      
      <div class="column">
        <div class="footer-column">
          <div class="footer-header">
              <h3>Copyright</h3>
          </div>
          <ul class="link-list">
            <li>
              <a>
                <span class="icon"><i class="fa fa-copyright"></i></span>
                พชรพล วงศ์สง่า - 2019
              </a>
            </li>
          </ul>
        </div>
      </div>
      

    </div>
  </div>
</footer>
    <div id="backtotop"><a href="#"></a></div><div class="sidebar">
  <div class="sidebar-header"><img src="/sidebar.svg" alt="">
    
    <a class="sidebar-close" href="javascript:void(0);">
      <i data-feather="x"></i>
    </a>
  </div>
  <div class="inner">
    <ul class="sidebar-menu">
      <li class="no-children">
          <a href="/tags">
            <div class="columns">
              <table width="100%">  
                <tr>
                  <td class="">
                    <span class="icon"><i class="fa fa-cubes"></i></span>All Tags
                  </td>
                  <td class="has-text-right" >
                      
                  </td>
                </tr>
              </table>
            </div>
          </a>
      <li class="no-children">
          <a href="/tags/custom_summary">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>custom_summary</td>
                    <td class="has-text-right" >
                        <div class="tag-number">5</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/code">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>code</td>
                    <td class="has-text-right" >
                        <div class="tag-number">4</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/leaf_bundle">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>leaf_bundle</td>
                    <td class="has-text-right" >
                        <div class="tag-number">4</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/default_image">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>default_image</td>
                    <td class="has-text-right" >
                        <div class="tag-number">3</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/no_summary">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>no_summary</td>
                    <td class="has-text-right" >
                        <div class="tag-number">3</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/beautifulsoup">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>BeautifulSoup</td>
                    <td class="has-text-right" >
                        <div class="tag-number">2</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/custom_image">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>custom_image</td>
                    <td class="has-text-right" >
                        <div class="tag-number">2</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/requests">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>Requests</td>
                    <td class="has-text-right" >
                        <div class="tag-number">2</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/web-scraping">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>Web Scraping</td>
                    <td class="has-text-right" >
                        <div class="tag-number">2</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/homo_deus">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>homo_deus</td>
                    <td class="has-text-right" >
                        <div class="tag-number">1</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li>
      <li class="no-children">
          <a href="/tags/love">
            <div class="columns">
              
              <table width="100%">  
                  <tr>
                    <td class=""><span class="icon"><i class="fa fa-cube"></i></span>love</td>
                    <td class="has-text-right" >
                        <div class="tag-number">1</div>
                      
                    </td>
                    
                  </tr>
              </table>
              
            </div>
          </a>
      </li></ul>
  </div>
</div>
<script src="/js/jquery-2.2.4.js"></script>
<script src="/js/feather.4.22.0.js"></script>
<script src="/js/modernizr-2.8.3.js"></script>
<script src="/js/refresh.js"></script>
<script src="/js/highlight.9.15.8.pack.js"></script>
<script src="/js/highlightjs-line-numbers.2.7.0.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>hljs.initLineNumbersOnLoad();</script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('.codeinline').forEach((block) => {
      hljs.highlightBlock(block);
    });
  });
</script>



<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>